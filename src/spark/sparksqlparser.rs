// Generated from SparkSql.g4 by ANTLR 4.8
#![allow(dead_code)]
#![allow(non_snake_case)]
#![allow(non_upper_case_globals)]
#![allow(nonstandard_style)]
#![allow(unused_imports)]
#![allow(unused_mut)]
#![allow(unused_braces)]
use crate::PredictionContextCache;
use crate::parser::{Parser, BaseParser, ParserRecog, ParserNodeType};
use crate::token_stream::TokenStream;
use crate::TokenSource;
use crate::parser_atn_simulator::ParserATNSimulator;
use crate::errors::*;
use crate::rule_context::{BaseRuleContext, CustomRuleContext, RuleContext};
use crate::recognizer::{Recognizer,Actions};
use crate::atn_deserializer::ATNDeserializer;
use crate::dfa::DFA;
use crate::atn::{ATN, INVALID_ALT};
use crate::error_strategy::{ErrorStrategy, DefaultErrorStrategy};
use crate::parser_rule_context::{BaseParserRuleContext, ParserRuleContext,cast,cast_mut};
use crate::tree::*;
use crate::token::{TOKEN_EOF,OwningToken,Token};
use crate::int_stream::EOF;
use crate::vocabulary::{Vocabulary,VocabularyImpl};
use crate::token_factory::{CommonTokenFactory,TokenFactory, TokenAware};
use super::sparksqllistener::*;
use crate::lazy_static;
use crate::{TidAble,TidExt};

use std::marker::PhantomData;
use std::sync::Arc;
use std::rc::Rc;
use std::convert::TryFrom;
use std::cell::RefCell;
use std::ops::{DerefMut, Deref};
use std::borrow::{Borrow,BorrowMut};
use std::any::{Any,TypeId};

		pub const T__0:isize=1; 
		pub const T__1:isize=2; 
		pub const T__2:isize=3; 
		pub const T__3:isize=4; 
		pub const T__4:isize=5; 
		pub const T__5:isize=6; 
		pub const T__6:isize=7; 
		pub const T__7:isize=8; 
		pub const T__8:isize=9; 
		pub const T__9:isize=10; 
		pub const ADD:isize=11; 
		pub const AFTER:isize=12; 
		pub const ALL:isize=13; 
		pub const ALTER:isize=14; 
		pub const ANALYZE:isize=15; 
		pub const AND:isize=16; 
		pub const ANTI:isize=17; 
		pub const ANY:isize=18; 
		pub const ARCHIVE:isize=19; 
		pub const ARRAY:isize=20; 
		pub const AS:isize=21; 
		pub const ASC:isize=22; 
		pub const AT:isize=23; 
		pub const AUTHORIZATION:isize=24; 
		pub const BETWEEN:isize=25; 
		pub const BOTH:isize=26; 
		pub const BUCKET:isize=27; 
		pub const BUCKETS:isize=28; 
		pub const BY:isize=29; 
		pub const CACHE:isize=30; 
		pub const CASCADE:isize=31; 
		pub const CASE:isize=32; 
		pub const CAST:isize=33; 
		pub const CHANGE:isize=34; 
		pub const CHECK:isize=35; 
		pub const CLEAR:isize=36; 
		pub const CLUSTER:isize=37; 
		pub const CLUSTERED:isize=38; 
		pub const CODEGEN:isize=39; 
		pub const COLLATE:isize=40; 
		pub const COLLECTION:isize=41; 
		pub const COLUMN:isize=42; 
		pub const COLUMNS:isize=43; 
		pub const COMMENT:isize=44; 
		pub const COMMIT:isize=45; 
		pub const COMPACT:isize=46; 
		pub const COMPACTIONS:isize=47; 
		pub const COMPUTE:isize=48; 
		pub const CONCATENATE:isize=49; 
		pub const CONSTRAINT:isize=50; 
		pub const COST:isize=51; 
		pub const CREATE:isize=52; 
		pub const CROSS:isize=53; 
		pub const CUBE:isize=54; 
		pub const CURRENT:isize=55; 
		pub const CURRENT_DATE:isize=56; 
		pub const CURRENT_TIME:isize=57; 
		pub const CURRENT_TIMESTAMP:isize=58; 
		pub const CURRENT_USER:isize=59; 
		pub const DATA:isize=60; 
		pub const DATABASE:isize=61; 
		pub const DATABASES:isize=62; 
		pub const DBPROPERTIES:isize=63; 
		pub const DEFINED:isize=64; 
		pub const DELETE:isize=65; 
		pub const DELIMITED:isize=66; 
		pub const DESC:isize=67; 
		pub const DESCRIBE:isize=68; 
		pub const DFS:isize=69; 
		pub const DIRECTORIES:isize=70; 
		pub const DIRECTORY:isize=71; 
		pub const DISTINCT:isize=72; 
		pub const DISTRIBUTE:isize=73; 
		pub const DIV:isize=74; 
		pub const DROP:isize=75; 
		pub const ELSE:isize=76; 
		pub const END:isize=77; 
		pub const ESCAPE:isize=78; 
		pub const ESCAPED:isize=79; 
		pub const EXCEPT:isize=80; 
		pub const EXCHANGE:isize=81; 
		pub const EXISTS:isize=82; 
		pub const EXPLAIN:isize=83; 
		pub const EXPORT:isize=84; 
		pub const EXTENDED:isize=85; 
		pub const EXTERNAL:isize=86; 
		pub const EXTRACT:isize=87; 
		pub const FALSE:isize=88; 
		pub const FETCH:isize=89; 
		pub const FIELDS:isize=90; 
		pub const FILTER:isize=91; 
		pub const FILEFORMAT:isize=92; 
		pub const FIRST:isize=93; 
		pub const FOLLOWING:isize=94; 
		pub const FOR:isize=95; 
		pub const FOREIGN:isize=96; 
		pub const FORMAT:isize=97; 
		pub const FORMATTED:isize=98; 
		pub const FROM:isize=99; 
		pub const FULL:isize=100; 
		pub const FUNCTION:isize=101; 
		pub const FUNCTIONS:isize=102; 
		pub const GLOBAL:isize=103; 
		pub const GRANT:isize=104; 
		pub const GROUP:isize=105; 
		pub const GROUPING:isize=106; 
		pub const HAVING:isize=107; 
		pub const IF:isize=108; 
		pub const IGNORE:isize=109; 
		pub const IMPORT:isize=110; 
		pub const IN:isize=111; 
		pub const INDEX:isize=112; 
		pub const INDEXES:isize=113; 
		pub const INNER:isize=114; 
		pub const INPATH:isize=115; 
		pub const INPUTFORMAT:isize=116; 
		pub const INSERT:isize=117; 
		pub const INTERSECT:isize=118; 
		pub const INTERVAL:isize=119; 
		pub const INTO:isize=120; 
		pub const IS:isize=121; 
		pub const ITEMS:isize=122; 
		pub const JOIN:isize=123; 
		pub const KEYS:isize=124; 
		pub const LAST:isize=125; 
		pub const LATERAL:isize=126; 
		pub const LAZY:isize=127; 
		pub const LEADING:isize=128; 
		pub const LEFT:isize=129; 
		pub const LIKE:isize=130; 
		pub const LIMIT:isize=131; 
		pub const LINES:isize=132; 
		pub const LIST:isize=133; 
		pub const LOAD:isize=134; 
		pub const LOCAL:isize=135; 
		pub const LOCATION:isize=136; 
		pub const LOCK:isize=137; 
		pub const LOCKS:isize=138; 
		pub const LOGICAL:isize=139; 
		pub const MACRO:isize=140; 
		pub const MAP:isize=141; 
		pub const MATCHED:isize=142; 
		pub const MERGE:isize=143; 
		pub const MSCK:isize=144; 
		pub const NAMESPACE:isize=145; 
		pub const NAMESPACES:isize=146; 
		pub const NATURAL:isize=147; 
		pub const NO:isize=148; 
		pub const NOT:isize=149; 
		pub const NULL:isize=150; 
		pub const NULLS:isize=151; 
		pub const OF:isize=152; 
		pub const ON:isize=153; 
		pub const ONLY:isize=154; 
		pub const OPTION:isize=155; 
		pub const OPTIONS:isize=156; 
		pub const OR:isize=157; 
		pub const ORDER:isize=158; 
		pub const OUT:isize=159; 
		pub const OUTER:isize=160; 
		pub const OUTPUTFORMAT:isize=161; 
		pub const OVER:isize=162; 
		pub const OVERLAPS:isize=163; 
		pub const OVERLAY:isize=164; 
		pub const OVERWRITE:isize=165; 
		pub const PARTITION:isize=166; 
		pub const PARTITIONED:isize=167; 
		pub const PARTITIONS:isize=168; 
		pub const PERCENTLIT:isize=169; 
		pub const PIVOT:isize=170; 
		pub const PLACING:isize=171; 
		pub const POSITION:isize=172; 
		pub const PRECEDING:isize=173; 
		pub const PRIMARY:isize=174; 
		pub const PRINCIPALS:isize=175; 
		pub const PROPERTIES:isize=176; 
		pub const PURGE:isize=177; 
		pub const QUERY:isize=178; 
		pub const RANGE:isize=179; 
		pub const RECORDREADER:isize=180; 
		pub const RECORDWRITER:isize=181; 
		pub const RECOVER:isize=182; 
		pub const REDUCE:isize=183; 
		pub const REFERENCES:isize=184; 
		pub const REFRESH:isize=185; 
		pub const RENAME:isize=186; 
		pub const REPAIR:isize=187; 
		pub const REPLACE:isize=188; 
		pub const RESET:isize=189; 
		pub const RESTRICT:isize=190; 
		pub const REVOKE:isize=191; 
		pub const RIGHT:isize=192; 
		pub const RLIKE:isize=193; 
		pub const ROLE:isize=194; 
		pub const ROLES:isize=195; 
		pub const ROLLBACK:isize=196; 
		pub const ROLLUP:isize=197; 
		pub const ROW:isize=198; 
		pub const ROWS:isize=199; 
		pub const SCHEMA:isize=200; 
		pub const SELECT:isize=201; 
		pub const SEMI:isize=202; 
		pub const SEPARATED:isize=203; 
		pub const SERDE:isize=204; 
		pub const SERDEPROPERTIES:isize=205; 
		pub const SESSION_USER:isize=206; 
		pub const SET:isize=207; 
		pub const SETMINUS:isize=208; 
		pub const SETS:isize=209; 
		pub const SHOW:isize=210; 
		pub const SKEWED:isize=211; 
		pub const SOME:isize=212; 
		pub const SORT:isize=213; 
		pub const SORTED:isize=214; 
		pub const START:isize=215; 
		pub const STATISTICS:isize=216; 
		pub const STORED:isize=217; 
		pub const STRATIFY:isize=218; 
		pub const STRUCT:isize=219; 
		pub const SUBSTR:isize=220; 
		pub const SUBSTRING:isize=221; 
		pub const TABLE:isize=222; 
		pub const TABLES:isize=223; 
		pub const TABLESAMPLE:isize=224; 
		pub const TBLPROPERTIES:isize=225; 
		pub const TEMPORARY:isize=226; 
		pub const TERMINATED:isize=227; 
		pub const THEN:isize=228; 
		pub const TIME:isize=229; 
		pub const TO:isize=230; 
		pub const TOUCH:isize=231; 
		pub const TRAILING:isize=232; 
		pub const TRANSACTION:isize=233; 
		pub const TRANSACTIONS:isize=234; 
		pub const TRANSFORM:isize=235; 
		pub const TRIM:isize=236; 
		pub const TRUE:isize=237; 
		pub const TRUNCATE:isize=238; 
		pub const TYPE:isize=239; 
		pub const UNARCHIVE:isize=240; 
		pub const UNBOUNDED:isize=241; 
		pub const UNCACHE:isize=242; 
		pub const UNION:isize=243; 
		pub const UNIQUE:isize=244; 
		pub const UNKNOWN:isize=245; 
		pub const UNLOCK:isize=246; 
		pub const UNSET:isize=247; 
		pub const UPDATE:isize=248; 
		pub const USE:isize=249; 
		pub const USER:isize=250; 
		pub const USING:isize=251; 
		pub const VALUES:isize=252; 
		pub const VIEW:isize=253; 
		pub const VIEWS:isize=254; 
		pub const WHEN:isize=255; 
		pub const WHERE:isize=256; 
		pub const WINDOW:isize=257; 
		pub const WITH:isize=258; 
		pub const ZONE:isize=259; 
		pub const EQ:isize=260; 
		pub const NSEQ:isize=261; 
		pub const NEQ:isize=262; 
		pub const NEQJ:isize=263; 
		pub const LT:isize=264; 
		pub const LTE:isize=265; 
		pub const GT:isize=266; 
		pub const GTE:isize=267; 
		pub const PLUS:isize=268; 
		pub const MINUS:isize=269; 
		pub const ASTERISK:isize=270; 
		pub const SLASH:isize=271; 
		pub const PERCENT:isize=272; 
		pub const TILDE:isize=273; 
		pub const AMPERSAND:isize=274; 
		pub const PIPE:isize=275; 
		pub const CONCAT_PIPE:isize=276; 
		pub const HAT:isize=277; 
		pub const SEMICOLON:isize=278; 
		pub const STRING:isize=279; 
		pub const BIGINT_LITERAL:isize=280; 
		pub const SMALLINT_LITERAL:isize=281; 
		pub const TINYINT_LITERAL:isize=282; 
		pub const INTEGER_VALUE:isize=283; 
		pub const EXPONENT_VALUE:isize=284; 
		pub const DECIMAL_VALUE:isize=285; 
		pub const FLOAT_LITERAL:isize=286; 
		pub const DOUBLE_LITERAL:isize=287; 
		pub const BIGDECIMAL_LITERAL:isize=288; 
		pub const IDENTIFIER:isize=289; 
		pub const BACKQUOTED_IDENTIFIER:isize=290; 
		pub const CUSTOM_VARS:isize=291; 
		pub const SIMPLE_COMMENT:isize=292; 
		pub const BRACKETED_COMMENT:isize=293; 
		pub const WS:isize=294; 
		pub const UNRECOGNIZED:isize=295;
	pub const RULE_program:usize = 0; 
	pub const RULE_singleStatement:usize = 1; 
	pub const RULE_emptyStatement:usize = 2; 
	pub const RULE_singleExpression:usize = 3; 
	pub const RULE_singleTableIdentifier:usize = 4; 
	pub const RULE_singleMultipartIdentifier:usize = 5; 
	pub const RULE_singleDataType:usize = 6; 
	pub const RULE_singleTableSchema:usize = 7; 
	pub const RULE_statement:usize = 8; 
	pub const RULE_configKey:usize = 9; 
	pub const RULE_unsupportedHiveNativeCommands:usize = 10; 
	pub const RULE_createTableHeader:usize = 11; 
	pub const RULE_replaceTableHeader:usize = 12; 
	pub const RULE_bucketSpec:usize = 13; 
	pub const RULE_skewSpec:usize = 14; 
	pub const RULE_locationSpec:usize = 15; 
	pub const RULE_commentSpec:usize = 16; 
	pub const RULE_query:usize = 17; 
	pub const RULE_insertInto:usize = 18; 
	pub const RULE_partitionSpecLocation:usize = 19; 
	pub const RULE_partitionSpec:usize = 20; 
	pub const RULE_partitionVal:usize = 21; 
	pub const RULE_namespace:usize = 22; 
	pub const RULE_describeFuncName:usize = 23; 
	pub const RULE_describeColName:usize = 24; 
	pub const RULE_ctes:usize = 25; 
	pub const RULE_namedQuery:usize = 26; 
	pub const RULE_tableProvider:usize = 27; 
	pub const RULE_createTableClauses:usize = 28; 
	pub const RULE_tablePropertyList:usize = 29; 
	pub const RULE_tableProperty:usize = 30; 
	pub const RULE_tablePropertyKey:usize = 31; 
	pub const RULE_tablePropertyValue:usize = 32; 
	pub const RULE_constantList:usize = 33; 
	pub const RULE_nestedConstantList:usize = 34; 
	pub const RULE_createFileFormat:usize = 35; 
	pub const RULE_fileFormat:usize = 36; 
	pub const RULE_storageHandler:usize = 37; 
	pub const RULE_resource:usize = 38; 
	pub const RULE_dmlStatementNoWith:usize = 39; 
	pub const RULE_queryOrganization:usize = 40; 
	pub const RULE_multiInsertQueryBody:usize = 41; 
	pub const RULE_queryTerm:usize = 42; 
	pub const RULE_queryPrimary:usize = 43; 
	pub const RULE_sortItem:usize = 44; 
	pub const RULE_fromStatement:usize = 45; 
	pub const RULE_fromStatementBody:usize = 46; 
	pub const RULE_querySpecification:usize = 47; 
	pub const RULE_transformClause:usize = 48; 
	pub const RULE_selectClause:usize = 49; 
	pub const RULE_setClause:usize = 50; 
	pub const RULE_matchedClause:usize = 51; 
	pub const RULE_notMatchedClause:usize = 52; 
	pub const RULE_matchedAction:usize = 53; 
	pub const RULE_notMatchedAction:usize = 54; 
	pub const RULE_assignmentList:usize = 55; 
	pub const RULE_assignment:usize = 56; 
	pub const RULE_whereClause:usize = 57; 
	pub const RULE_havingClause:usize = 58; 
	pub const RULE_hint:usize = 59; 
	pub const RULE_hintStatement:usize = 60; 
	pub const RULE_fromClause:usize = 61; 
	pub const RULE_aggregationClause:usize = 62; 
	pub const RULE_groupingSet:usize = 63; 
	pub const RULE_pivotClause:usize = 64; 
	pub const RULE_pivotColumn:usize = 65; 
	pub const RULE_pivotValue:usize = 66; 
	pub const RULE_lateralView:usize = 67; 
	pub const RULE_setQuantifier:usize = 68; 
	pub const RULE_relation:usize = 69; 
	pub const RULE_joinRelation:usize = 70; 
	pub const RULE_joinType:usize = 71; 
	pub const RULE_joinCriteria:usize = 72; 
	pub const RULE_sample:usize = 73; 
	pub const RULE_sampleMethod:usize = 74; 
	pub const RULE_identifierList:usize = 75; 
	pub const RULE_identifierSeq:usize = 76; 
	pub const RULE_orderedIdentifierList:usize = 77; 
	pub const RULE_orderedIdentifier:usize = 78; 
	pub const RULE_identifierCommentList:usize = 79; 
	pub const RULE_identifierComment:usize = 80; 
	pub const RULE_relationPrimary:usize = 81; 
	pub const RULE_inlineTable:usize = 82; 
	pub const RULE_functionTable:usize = 83; 
	pub const RULE_tableAlias:usize = 84; 
	pub const RULE_rowFormat:usize = 85; 
	pub const RULE_multipartIdentifierList:usize = 86; 
	pub const RULE_multipartIdentifier:usize = 87; 
	pub const RULE_tableIdentifier:usize = 88; 
	pub const RULE_namedExpression:usize = 89; 
	pub const RULE_namedExpressionSeq:usize = 90; 
	pub const RULE_transformList:usize = 91; 
	pub const RULE_transform:usize = 92; 
	pub const RULE_transformArgument:usize = 93; 
	pub const RULE_expression:usize = 94; 
	pub const RULE_booleanExpression:usize = 95; 
	pub const RULE_predicate:usize = 96; 
	pub const RULE_valueExpression:usize = 97; 
	pub const RULE_primaryExpression:usize = 98; 
	pub const RULE_constant:usize = 99; 
	pub const RULE_comparisonOperator:usize = 100; 
	pub const RULE_arithmeticOperator:usize = 101; 
	pub const RULE_predicateOperator:usize = 102; 
	pub const RULE_booleanValue:usize = 103; 
	pub const RULE_interval:usize = 104; 
	pub const RULE_errorCapturingMultiUnitsInterval:usize = 105; 
	pub const RULE_multiUnitsInterval:usize = 106; 
	pub const RULE_errorCapturingUnitToUnitInterval:usize = 107; 
	pub const RULE_unitToUnitInterval:usize = 108; 
	pub const RULE_intervalValue:usize = 109; 
	pub const RULE_colPosition:usize = 110; 
	pub const RULE_dataType:usize = 111; 
	pub const RULE_qualifiedColTypeWithPositionList:usize = 112; 
	pub const RULE_qualifiedColTypeWithPosition:usize = 113; 
	pub const RULE_colTypeList:usize = 114; 
	pub const RULE_colType:usize = 115; 
	pub const RULE_complexColTypeList:usize = 116; 
	pub const RULE_complexColType:usize = 117; 
	pub const RULE_whenClause:usize = 118; 
	pub const RULE_windowClause:usize = 119; 
	pub const RULE_namedWindow:usize = 120; 
	pub const RULE_windowSpec:usize = 121; 
	pub const RULE_windowFrame:usize = 122; 
	pub const RULE_frameBound:usize = 123; 
	pub const RULE_qualifiedNameList:usize = 124; 
	pub const RULE_functionName:usize = 125; 
	pub const RULE_qualifiedName:usize = 126; 
	pub const RULE_errorCapturingIdentifier:usize = 127; 
	pub const RULE_errorCapturingIdentifierExtra:usize = 128; 
	pub const RULE_identifier:usize = 129; 
	pub const RULE_strictIdentifier:usize = 130; 
	pub const RULE_quotedIdentifier:usize = 131; 
	pub const RULE_number:usize = 132; 
	pub const RULE_alterColumnAction:usize = 133; 
	pub const RULE_ansiNonReserved:usize = 134; 
	pub const RULE_strictNonReserved:usize = 135; 
	pub const RULE_nonReserved:usize = 136;
	pub const ruleNames: [&'static str; 137] =  [
		"program", "singleStatement", "emptyStatement", "singleExpression", "singleTableIdentifier", 
		"singleMultipartIdentifier", "singleDataType", "singleTableSchema", "statement", 
		"configKey", "unsupportedHiveNativeCommands", "createTableHeader", "replaceTableHeader", 
		"bucketSpec", "skewSpec", "locationSpec", "commentSpec", "query", "insertInto", 
		"partitionSpecLocation", "partitionSpec", "partitionVal", "namespace", 
		"describeFuncName", "describeColName", "ctes", "namedQuery", "tableProvider", 
		"createTableClauses", "tablePropertyList", "tableProperty", "tablePropertyKey", 
		"tablePropertyValue", "constantList", "nestedConstantList", "createFileFormat", 
		"fileFormat", "storageHandler", "resource", "dmlStatementNoWith", "queryOrganization", 
		"multiInsertQueryBody", "queryTerm", "queryPrimary", "sortItem", "fromStatement", 
		"fromStatementBody", "querySpecification", "transformClause", "selectClause", 
		"setClause", "matchedClause", "notMatchedClause", "matchedAction", "notMatchedAction", 
		"assignmentList", "assignment", "whereClause", "havingClause", "hint", 
		"hintStatement", "fromClause", "aggregationClause", "groupingSet", "pivotClause", 
		"pivotColumn", "pivotValue", "lateralView", "setQuantifier", "relation", 
		"joinRelation", "joinType", "joinCriteria", "sample", "sampleMethod", 
		"identifierList", "identifierSeq", "orderedIdentifierList", "orderedIdentifier", 
		"identifierCommentList", "identifierComment", "relationPrimary", "inlineTable", 
		"functionTable", "tableAlias", "rowFormat", "multipartIdentifierList", 
		"multipartIdentifier", "tableIdentifier", "namedExpression", "namedExpressionSeq", 
		"transformList", "transform", "transformArgument", "expression", "booleanExpression", 
		"predicate", "valueExpression", "primaryExpression", "constant", "comparisonOperator", 
		"arithmeticOperator", "predicateOperator", "booleanValue", "interval", 
		"errorCapturingMultiUnitsInterval", "multiUnitsInterval", "errorCapturingUnitToUnitInterval", 
		"unitToUnitInterval", "intervalValue", "colPosition", "dataType", "qualifiedColTypeWithPositionList", 
		"qualifiedColTypeWithPosition", "colTypeList", "colType", "complexColTypeList", 
		"complexColType", "whenClause", "windowClause", "namedWindow", "windowSpec", 
		"windowFrame", "frameBound", "qualifiedNameList", "functionName", "qualifiedName", 
		"errorCapturingIdentifier", "errorCapturingIdentifierExtra", "identifier", 
		"strictIdentifier", "quotedIdentifier", "number", "alterColumnAction", 
		"ansiNonReserved", "strictNonReserved", "nonReserved"
	];


	pub const _LITERAL_NAMES: [Option<&'static str>;279] = [
		None, Some("'('"), Some("')'"), Some("','"), Some("'.'"), Some("'/*+'"), 
		Some("'*/'"), Some("'->'"), Some("'['"), Some("']'"), Some("':'"), Some("'ADD'"), 
		Some("'AFTER'"), Some("'ALL'"), Some("'ALTER'"), Some("'ANALYZE'"), Some("'AND'"), 
		Some("'ANTI'"), Some("'ANY'"), Some("'ARCHIVE'"), Some("'ARRAY'"), Some("'AS'"), 
		Some("'ASC'"), Some("'AT'"), Some("'AUTHORIZATION'"), Some("'BETWEEN'"), 
		Some("'BOTH'"), Some("'BUCKET'"), Some("'BUCKETS'"), Some("'BY'"), Some("'CACHE'"), 
		Some("'CASCADE'"), Some("'CASE'"), Some("'CAST'"), Some("'CHANGE'"), Some("'CHECK'"), 
		Some("'CLEAR'"), Some("'CLUSTER'"), Some("'CLUSTERED'"), Some("'CODEGEN'"), 
		Some("'COLLATE'"), Some("'COLLECTION'"), Some("'COLUMN'"), Some("'COLUMNS'"), 
		Some("'COMMENT'"), Some("'COMMIT'"), Some("'COMPACT'"), Some("'COMPACTIONS'"), 
		Some("'COMPUTE'"), Some("'CONCATENATE'"), Some("'CONSTRAINT'"), Some("'COST'"), 
		Some("'CREATE'"), Some("'CROSS'"), Some("'CUBE'"), Some("'CURRENT'"), 
		Some("'CURRENT_DATE'"), Some("'CURRENT_TIME'"), Some("'CURRENT_TIMESTAMP'"), 
		Some("'CURRENT_USER'"), Some("'DATA'"), Some("'DATABASE'"), None, Some("'DBPROPERTIES'"), 
		Some("'DEFINED'"), Some("'DELETE'"), Some("'DELIMITED'"), Some("'DESC'"), 
		Some("'DESCRIBE'"), Some("'DFS'"), Some("'DIRECTORIES'"), Some("'DIRECTORY'"), 
		Some("'DISTINCT'"), Some("'DISTRIBUTE'"), Some("'DIV'"), Some("'DROP'"), 
		Some("'ELSE'"), Some("'END'"), Some("'ESCAPE'"), Some("'ESCAPED'"), Some("'EXCEPT'"), 
		Some("'EXCHANGE'"), Some("'EXISTS'"), Some("'EXPLAIN'"), Some("'EXPORT'"), 
		Some("'EXTENDED'"), Some("'EXTERNAL'"), Some("'EXTRACT'"), Some("'FALSE'"), 
		Some("'FETCH'"), Some("'FIELDS'"), Some("'FILTER'"), Some("'FILEFORMAT'"), 
		Some("'FIRST'"), Some("'FOLLOWING'"), Some("'FOR'"), Some("'FOREIGN'"), 
		Some("'FORMAT'"), Some("'FORMATTED'"), Some("'FROM'"), Some("'FULL'"), 
		Some("'FUNCTION'"), Some("'FUNCTIONS'"), Some("'GLOBAL'"), Some("'GRANT'"), 
		Some("'GROUP'"), Some("'GROUPING'"), Some("'HAVING'"), Some("'IF'"), Some("'IGNORE'"), 
		Some("'IMPORT'"), Some("'IN'"), Some("'INDEX'"), Some("'INDEXES'"), Some("'INNER'"), 
		Some("'INPATH'"), Some("'INPUTFORMAT'"), Some("'INSERT'"), Some("'INTERSECT'"), 
		Some("'INTERVAL'"), Some("'INTO'"), Some("'IS'"), Some("'ITEMS'"), Some("'JOIN'"), 
		Some("'KEYS'"), Some("'LAST'"), Some("'LATERAL'"), Some("'LAZY'"), Some("'LEADING'"), 
		Some("'LEFT'"), Some("'LIKE'"), Some("'LIMIT'"), Some("'LINES'"), Some("'LIST'"), 
		Some("'LOAD'"), Some("'LOCAL'"), Some("'LOCATION'"), Some("'LOCK'"), Some("'LOCKS'"), 
		Some("'LOGICAL'"), Some("'MACRO'"), Some("'MAP'"), Some("'MATCHED'"), 
		Some("'MERGE'"), Some("'MSCK'"), Some("'NAMESPACE'"), Some("'NAMESPACES'"), 
		Some("'NATURAL'"), Some("'NO'"), None, Some("'NULL'"), Some("'NULLS'"), 
		Some("'OF'"), Some("'ON'"), Some("'ONLY'"), Some("'OPTION'"), Some("'OPTIONS'"), 
		Some("'OR'"), Some("'ORDER'"), Some("'OUT'"), Some("'OUTER'"), Some("'OUTPUTFORMAT'"), 
		Some("'OVER'"), Some("'OVERLAPS'"), Some("'OVERLAY'"), Some("'OVERWRITE'"), 
		Some("'PARTITION'"), Some("'PARTITIONED'"), Some("'PARTITIONS'"), Some("'PERCENT'"), 
		Some("'PIVOT'"), Some("'PLACING'"), Some("'POSITION'"), Some("'PRECEDING'"), 
		Some("'PRIMARY'"), Some("'PRINCIPALS'"), Some("'PROPERTIES'"), Some("'PURGE'"), 
		Some("'QUERY'"), Some("'RANGE'"), Some("'RECORDREADER'"), Some("'RECORDWRITER'"), 
		Some("'RECOVER'"), Some("'REDUCE'"), Some("'REFERENCES'"), Some("'REFRESH'"), 
		Some("'RENAME'"), Some("'REPAIR'"), Some("'REPLACE'"), Some("'RESET'"), 
		Some("'RESTRICT'"), Some("'REVOKE'"), Some("'RIGHT'"), None, Some("'ROLE'"), 
		Some("'ROLES'"), Some("'ROLLBACK'"), Some("'ROLLUP'"), Some("'ROW'"), 
		Some("'ROWS'"), Some("'SCHEMA'"), Some("'SELECT'"), Some("'SEMI'"), Some("'SEPARATED'"), 
		Some("'SERDE'"), Some("'SERDEPROPERTIES'"), Some("'SESSION_USER'"), Some("'SET'"), 
		Some("'MINUS'"), Some("'SETS'"), Some("'SHOW'"), Some("'SKEWED'"), Some("'SOME'"), 
		Some("'SORT'"), Some("'SORTED'"), Some("'START'"), Some("'STATISTICS'"), 
		Some("'STORED'"), Some("'STRATIFY'"), Some("'STRUCT'"), Some("'SUBSTR'"), 
		Some("'SUBSTRING'"), Some("'TABLE'"), Some("'TABLES'"), Some("'TABLESAMPLE'"), 
		Some("'TBLPROPERTIES'"), None, Some("'TERMINATED'"), Some("'THEN'"), Some("'TIME'"), 
		Some("'TO'"), Some("'TOUCH'"), Some("'TRAILING'"), Some("'TRANSACTION'"), 
		Some("'TRANSACTIONS'"), Some("'TRANSFORM'"), Some("'TRIM'"), Some("'TRUE'"), 
		Some("'TRUNCATE'"), Some("'TYPE'"), Some("'UNARCHIVE'"), Some("'UNBOUNDED'"), 
		Some("'UNCACHE'"), Some("'UNION'"), Some("'UNIQUE'"), Some("'UNKNOWN'"), 
		Some("'UNLOCK'"), Some("'UNSET'"), Some("'UPDATE'"), Some("'USE'"), Some("'USER'"), 
		Some("'USING'"), Some("'VALUES'"), Some("'VIEW'"), Some("'VIEWS'"), Some("'WHEN'"), 
		Some("'WHERE'"), Some("'WINDOW'"), Some("'WITH'"), Some("'ZONE'"), None, 
		Some("'<=>'"), Some("'<>'"), Some("'!='"), Some("'<'"), None, Some("'>'"), 
		None, Some("'+'"), Some("'-'"), Some("'*'"), Some("'/'"), Some("'%'"), 
		Some("'~'"), Some("'&'"), Some("'|'"), Some("'||'"), Some("'^'"), Some("';'")
	];
	pub const _SYMBOLIC_NAMES: [Option<&'static str>;296]  = [
		None, None, None, None, None, None, None, None, None, None, None, Some("ADD"), 
		Some("AFTER"), Some("ALL"), Some("ALTER"), Some("ANALYZE"), Some("AND"), 
		Some("ANTI"), Some("ANY"), Some("ARCHIVE"), Some("ARRAY"), Some("AS"), 
		Some("ASC"), Some("AT"), Some("AUTHORIZATION"), Some("BETWEEN"), Some("BOTH"), 
		Some("BUCKET"), Some("BUCKETS"), Some("BY"), Some("CACHE"), Some("CASCADE"), 
		Some("CASE"), Some("CAST"), Some("CHANGE"), Some("CHECK"), Some("CLEAR"), 
		Some("CLUSTER"), Some("CLUSTERED"), Some("CODEGEN"), Some("COLLATE"), 
		Some("COLLECTION"), Some("COLUMN"), Some("COLUMNS"), Some("COMMENT"), 
		Some("COMMIT"), Some("COMPACT"), Some("COMPACTIONS"), Some("COMPUTE"), 
		Some("CONCATENATE"), Some("CONSTRAINT"), Some("COST"), Some("CREATE"), 
		Some("CROSS"), Some("CUBE"), Some("CURRENT"), Some("CURRENT_DATE"), Some("CURRENT_TIME"), 
		Some("CURRENT_TIMESTAMP"), Some("CURRENT_USER"), Some("DATA"), Some("DATABASE"), 
		Some("DATABASES"), Some("DBPROPERTIES"), Some("DEFINED"), Some("DELETE"), 
		Some("DELIMITED"), Some("DESC"), Some("DESCRIBE"), Some("DFS"), Some("DIRECTORIES"), 
		Some("DIRECTORY"), Some("DISTINCT"), Some("DISTRIBUTE"), Some("DIV"), 
		Some("DROP"), Some("ELSE"), Some("END"), Some("ESCAPE"), Some("ESCAPED"), 
		Some("EXCEPT"), Some("EXCHANGE"), Some("EXISTS"), Some("EXPLAIN"), Some("EXPORT"), 
		Some("EXTENDED"), Some("EXTERNAL"), Some("EXTRACT"), Some("FALSE"), Some("FETCH"), 
		Some("FIELDS"), Some("FILTER"), Some("FILEFORMAT"), Some("FIRST"), Some("FOLLOWING"), 
		Some("FOR"), Some("FOREIGN"), Some("FORMAT"), Some("FORMATTED"), Some("FROM"), 
		Some("FULL"), Some("FUNCTION"), Some("FUNCTIONS"), Some("GLOBAL"), Some("GRANT"), 
		Some("GROUP"), Some("GROUPING"), Some("HAVING"), Some("IF"), Some("IGNORE"), 
		Some("IMPORT"), Some("IN"), Some("INDEX"), Some("INDEXES"), Some("INNER"), 
		Some("INPATH"), Some("INPUTFORMAT"), Some("INSERT"), Some("INTERSECT"), 
		Some("INTERVAL"), Some("INTO"), Some("IS"), Some("ITEMS"), Some("JOIN"), 
		Some("KEYS"), Some("LAST"), Some("LATERAL"), Some("LAZY"), Some("LEADING"), 
		Some("LEFT"), Some("LIKE"), Some("LIMIT"), Some("LINES"), Some("LIST"), 
		Some("LOAD"), Some("LOCAL"), Some("LOCATION"), Some("LOCK"), Some("LOCKS"), 
		Some("LOGICAL"), Some("MACRO"), Some("MAP"), Some("MATCHED"), Some("MERGE"), 
		Some("MSCK"), Some("NAMESPACE"), Some("NAMESPACES"), Some("NATURAL"), 
		Some("NO"), Some("NOT"), Some("NULL"), Some("NULLS"), Some("OF"), Some("ON"), 
		Some("ONLY"), Some("OPTION"), Some("OPTIONS"), Some("OR"), Some("ORDER"), 
		Some("OUT"), Some("OUTER"), Some("OUTPUTFORMAT"), Some("OVER"), Some("OVERLAPS"), 
		Some("OVERLAY"), Some("OVERWRITE"), Some("PARTITION"), Some("PARTITIONED"), 
		Some("PARTITIONS"), Some("PERCENTLIT"), Some("PIVOT"), Some("PLACING"), 
		Some("POSITION"), Some("PRECEDING"), Some("PRIMARY"), Some("PRINCIPALS"), 
		Some("PROPERTIES"), Some("PURGE"), Some("QUERY"), Some("RANGE"), Some("RECORDREADER"), 
		Some("RECORDWRITER"), Some("RECOVER"), Some("REDUCE"), Some("REFERENCES"), 
		Some("REFRESH"), Some("RENAME"), Some("REPAIR"), Some("REPLACE"), Some("RESET"), 
		Some("RESTRICT"), Some("REVOKE"), Some("RIGHT"), Some("RLIKE"), Some("ROLE"), 
		Some("ROLES"), Some("ROLLBACK"), Some("ROLLUP"), Some("ROW"), Some("ROWS"), 
		Some("SCHEMA"), Some("SELECT"), Some("SEMI"), Some("SEPARATED"), Some("SERDE"), 
		Some("SERDEPROPERTIES"), Some("SESSION_USER"), Some("SET"), Some("SETMINUS"), 
		Some("SETS"), Some("SHOW"), Some("SKEWED"), Some("SOME"), Some("SORT"), 
		Some("SORTED"), Some("START"), Some("STATISTICS"), Some("STORED"), Some("STRATIFY"), 
		Some("STRUCT"), Some("SUBSTR"), Some("SUBSTRING"), Some("TABLE"), Some("TABLES"), 
		Some("TABLESAMPLE"), Some("TBLPROPERTIES"), Some("TEMPORARY"), Some("TERMINATED"), 
		Some("THEN"), Some("TIME"), Some("TO"), Some("TOUCH"), Some("TRAILING"), 
		Some("TRANSACTION"), Some("TRANSACTIONS"), Some("TRANSFORM"), Some("TRIM"), 
		Some("TRUE"), Some("TRUNCATE"), Some("TYPE"), Some("UNARCHIVE"), Some("UNBOUNDED"), 
		Some("UNCACHE"), Some("UNION"), Some("UNIQUE"), Some("UNKNOWN"), Some("UNLOCK"), 
		Some("UNSET"), Some("UPDATE"), Some("USE"), Some("USER"), Some("USING"), 
		Some("VALUES"), Some("VIEW"), Some("VIEWS"), Some("WHEN"), Some("WHERE"), 
		Some("WINDOW"), Some("WITH"), Some("ZONE"), Some("EQ"), Some("NSEQ"), 
		Some("NEQ"), Some("NEQJ"), Some("LT"), Some("LTE"), Some("GT"), Some("GTE"), 
		Some("PLUS"), Some("MINUS"), Some("ASTERISK"), Some("SLASH"), Some("PERCENT"), 
		Some("TILDE"), Some("AMPERSAND"), Some("PIPE"), Some("CONCAT_PIPE"), Some("HAT"), 
		Some("SEMICOLON"), Some("STRING"), Some("BIGINT_LITERAL"), Some("SMALLINT_LITERAL"), 
		Some("TINYINT_LITERAL"), Some("INTEGER_VALUE"), Some("EXPONENT_VALUE"), 
		Some("DECIMAL_VALUE"), Some("FLOAT_LITERAL"), Some("DOUBLE_LITERAL"), 
		Some("BIGDECIMAL_LITERAL"), Some("IDENTIFIER"), Some("BACKQUOTED_IDENTIFIER"), 
		Some("CUSTOM_VARS"), Some("SIMPLE_COMMENT"), Some("BRACKETED_COMMENT"), 
		Some("WS"), Some("UNRECOGNIZED")
	];
	lazy_static!{
	    static ref _shared_context_cache: Arc<PredictionContextCache> = Arc::new(PredictionContextCache::new());
		static ref VOCABULARY: Box<dyn Vocabulary> = Box::new(VocabularyImpl::new(_LITERAL_NAMES.iter(), _SYMBOLIC_NAMES.iter(), None));
	}


type BaseParserType<'input, I> =
	BaseParser<'input,SparkSqlParserExt<'input>, I, SparkSqlParserContextType , dyn SparkSqlListener<'input> + 'input >;

type TokenType<'input> = <LocalTokenFactory<'input> as TokenFactory<'input>>::Tok;
pub type LocalTokenFactory<'input> = CommonTokenFactory;

pub type SparkSqlTreeWalker<'input,'a> =
	ParseTreeWalker<'input, 'a, SparkSqlParserContextType , dyn SparkSqlListener<'input> + 'a>;

/// Parser for SparkSql grammar
pub struct SparkSqlParser<'input,I,H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	base:BaseParserType<'input,I>,
	interpreter:Arc<ParserATNSimulator>,
	_shared_context_cache: Box<PredictionContextCache>,
    pub err_handler: H,
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn get_serialized_atn() -> &'static str { _serializedATN }

    pub fn set_error_strategy(&mut self, strategy: H) {
        self.err_handler = strategy
    }

    pub fn with_strategy(input: I, strategy: H) -> Self {
		crate::recognizer::check_version("0","3");
		let interpreter = Arc::new(ParserATNSimulator::new(
			_ATN.clone(),
			_decision_to_DFA.clone(),
			_shared_context_cache.clone(),
		));
		Self {
			base: BaseParser::new_base_parser(
				input,
				Arc::clone(&interpreter),
				SparkSqlParserExt{
					_pd: Default::default(),
					legacy_setops_precedence_enbled: false,
					legacy_exponent_literal_as_decimal_enabled: false,
					SQL_standard_keyword_behavior: false
				}
			),
			interpreter,
            _shared_context_cache: Box::new(PredictionContextCache::new()),
            err_handler: strategy,
        }
    }

}

type DynStrategy<'input,I> = Box<dyn ErrorStrategy<'input,BaseParserType<'input,I>> + 'input>;

impl<'input, I> SparkSqlParser<'input, I, DynStrategy<'input,I>>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
{
    pub fn with_dyn_strategy(input: I) -> Self{
    	Self::with_strategy(input,Box::new(DefaultErrorStrategy::new()))
    }
}

impl<'input, I> SparkSqlParser<'input, I, DefaultErrorStrategy<'input,SparkSqlParserContextType>>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
{
    pub fn new(input: I) -> Self{
    	Self::with_strategy(input,DefaultErrorStrategy::new())
    }
}

/// Trait for monomorphized trait object that corresponds to the nodes of parse tree generated for SparkSqlParser
pub trait SparkSqlParserContext<'input>:
	for<'x> Listenable<dyn SparkSqlListener<'input> + 'x > + 
	ParserRuleContext<'input, TF=LocalTokenFactory<'input>, Ctx=SparkSqlParserContextType>
{}

crate::coerce_from!{ 'input : SparkSqlParserContext<'input> }

impl<'input> SparkSqlParserContext<'input> for TerminalNode<'input,SparkSqlParserContextType> {}
impl<'input> SparkSqlParserContext<'input> for ErrorNode<'input,SparkSqlParserContextType> {}

crate::tid! { impl<'input> TidAble<'input> for dyn SparkSqlParserContext<'input> + 'input }

crate::tid! { impl<'input> TidAble<'input> for dyn SparkSqlListener<'input> + 'input }

pub struct SparkSqlParserContextType;
crate::tid!{SparkSqlParserContextType}

impl<'input> ParserNodeType<'input> for SparkSqlParserContextType{
	type TF = LocalTokenFactory<'input>;
	type Type = dyn SparkSqlParserContext<'input> + 'input;
}

impl<'input, I, H> Deref for SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
    type Target = BaseParserType<'input,I>;

    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl<'input, I, H> DerefMut for SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}

pub struct SparkSqlParserExt<'input>{
	_pd: PhantomData<&'input str>,
	legacy_setops_precedence_enbled: bool,
	legacy_exponent_literal_as_decimal_enabled: bool,
	SQL_standard_keyword_behavior: bool
}

crate::tid! { SparkSqlParserExt<'a> }

impl<'input> TokenAware<'input> for SparkSqlParserExt<'input>{
	type TF = LocalTokenFactory<'input>;
}

impl<'input,I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>> ParserRecog<'input, BaseParserType<'input,I>> for SparkSqlParserExt<'input>{}

impl<'input,I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>> Actions<'input, BaseParserType<'input,I>> for SparkSqlParserExt<'input>{
	fn get_grammar_file_name(&self) -> &str { "SparkSql.g4"}

   	fn get_rule_names(&self) -> &[& str] {&ruleNames}

   	fn get_vocabulary(&self) -> &dyn Vocabulary { &**VOCABULARY }
	fn sempred(_localctx: Option<&(dyn SparkSqlParserContext<'input> + 'input)>, rule_index: isize, pred_index: isize,
			   recog:&mut BaseParserType<'input,I>
	)->bool{
		match rule_index {
					42 => SparkSqlParser::<'input,I,_>::queryTerm_sempred(_localctx.and_then(|x|x.downcast_ref()), pred_index, recog),
					95 => SparkSqlParser::<'input,I,_>::booleanExpression_sempred(_localctx.and_then(|x|x.downcast_ref()), pred_index, recog),
					97 => SparkSqlParser::<'input,I,_>::valueExpression_sempred(_localctx.and_then(|x|x.downcast_ref()), pred_index, recog),
					98 => SparkSqlParser::<'input,I,_>::primaryExpression_sempred(_localctx.and_then(|x|x.downcast_ref()), pred_index, recog),
					129 => SparkSqlParser::<'input,I,_>::identifier_sempred(_localctx.and_then(|x|x.downcast_ref()), pred_index, recog),
					130 => SparkSqlParser::<'input,I,_>::strictIdentifier_sempred(_localctx.and_then(|x|x.downcast_ref()), pred_index, recog),
					132 => SparkSqlParser::<'input,I,_>::number_sempred(_localctx.and_then(|x|x.downcast_ref()), pred_index, recog),
			_ => true
		}
	}
}

impl<'input, I> SparkSqlParser<'input, I, DefaultErrorStrategy<'input,SparkSqlParserContextType>>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
{
	fn queryTerm_sempred(_localctx: Option<&QueryTermContext<'input>>, pred_index:isize,
						recog:&mut <Self as Deref>::Target
		) -> bool {
		match pred_index {
				0=>{
					recog.precpred(None, 3)
				}
				1=>{
					recog.legacy_setops_precedence_enbled
				}
				2=>{
					recog.precpred(None, 2)
				}
				3=>{
					!recog.legacy_setops_precedence_enbled
				}
				4=>{
					recog.precpred(None, 1)
				}
				5=>{
					!recog.legacy_setops_precedence_enbled
				}
			_ => true
		}
	}
	fn booleanExpression_sempred(_localctx: Option<&BooleanExpressionContext<'input>>, pred_index:isize,
						recog:&mut <Self as Deref>::Target
		) -> bool {
		match pred_index {
				6=>{
					recog.precpred(None, 2)
				}
				7=>{
					recog.precpred(None, 1)
				}
			_ => true
		}
	}
	fn valueExpression_sempred(_localctx: Option<&ValueExpressionContext<'input>>, pred_index:isize,
						recog:&mut <Self as Deref>::Target
		) -> bool {
		match pred_index {
				8=>{
					recog.precpred(None, 6)
				}
				9=>{
					recog.precpred(None, 5)
				}
				10=>{
					recog.precpred(None, 4)
				}
				11=>{
					recog.precpred(None, 3)
				}
				12=>{
					recog.precpred(None, 2)
				}
				13=>{
					recog.precpred(None, 1)
				}
			_ => true
		}
	}
	fn primaryExpression_sempred(_localctx: Option<&PrimaryExpressionContext<'input>>, pred_index:isize,
						recog:&mut <Self as Deref>::Target
		) -> bool {
		match pred_index {
				14=>{
					recog.precpred(None, 8)
				}
				15=>{
					recog.precpred(None, 6)
				}
			_ => true
		}
	}
	fn identifier_sempred(_localctx: Option<&IdentifierContext<'input>>, pred_index:isize,
						recog:&mut <Self as Deref>::Target
		) -> bool {
		match pred_index {
				16=>{
					!recog.SQL_standard_keyword_behavior
				}
			_ => true
		}
	}
	fn strictIdentifier_sempred(_localctx: Option<&StrictIdentifierContext<'input>>, pred_index:isize,
						recog:&mut <Self as Deref>::Target
		) -> bool {
		match pred_index {
				17=>{
					recog.SQL_standard_keyword_behavior
				}
				18=>{
					!recog.SQL_standard_keyword_behavior
				}
			_ => true
		}
	}
	fn number_sempred(_localctx: Option<&NumberContext<'input>>, pred_index:isize,
						recog:&mut <Self as Deref>::Target
		) -> bool {
		match pred_index {
				19=>{
					!recog.legacy_exponent_literal_as_decimal_enabled
				}
				20=>{
					!recog.legacy_exponent_literal_as_decimal_enabled
				}
				21=>{
					recog.legacy_exponent_literal_as_decimal_enabled
				}
			_ => true
		}
	}
}
//------------------- program ----------------
pub type ProgramContextAll<'input> = ProgramContext<'input>;


pub type ProgramContext<'input> = BaseParserRuleContext<'input,ProgramContextExt<'input>>;

#[derive(Clone)]
pub struct ProgramContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ProgramContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ProgramContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_program(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_program(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ProgramContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_program }
	//fn type_rule_index() -> usize where Self: Sized { RULE_program }
}
crate::tid!{ProgramContextExt<'a>}

impl<'input> ProgramContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ProgramContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ProgramContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ProgramContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ProgramContextExt<'input>>{

fn singleStatement(&self) -> Option<Rc<SingleStatementContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EOF
/// Returns `None` if there is no child corresponding to token EOF
fn EOF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EOF, 0)
}

}

impl<'input> ProgramContextAttrs<'input> for ProgramContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn program(&mut self,)
	-> Result<Rc<ProgramContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ProgramContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 0, RULE_program);
        let mut _localctx: Rc<ProgramContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule singleStatement*/
			recog.base.set_state(274);
			recog.singleStatement()?;

			recog.base.set_state(275);
			recog.base.match_token(EOF,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- singleStatement ----------------
pub type SingleStatementContextAll<'input> = SingleStatementContext<'input>;


pub type SingleStatementContext<'input> = BaseParserRuleContext<'input,SingleStatementContextExt<'input>>;

#[derive(Clone)]
pub struct SingleStatementContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SingleStatementContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SingleStatementContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_singleStatement(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_singleStatement(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SingleStatementContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_singleStatement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_singleStatement }
}
crate::tid!{SingleStatementContextExt<'a>}

impl<'input> SingleStatementContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SingleStatementContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SingleStatementContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SingleStatementContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SingleStatementContextExt<'input>>{

fn statement_all(&self) ->  Vec<Rc<StatementContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn statement(&self, i: usize) -> Option<Rc<StatementContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn emptyStatement_all(&self) ->  Vec<Rc<EmptyStatementContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn emptyStatement(&self, i: usize) -> Option<Rc<EmptyStatementContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves all `TerminalNode`s corresponding to token SEMICOLON in current rule
fn SEMICOLON_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token SEMICOLON, starting from 0.
/// Returns `None` if number of children corresponding to token SEMICOLON is less or equal than `i`.
fn SEMICOLON(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SEMICOLON, i)
}

}

impl<'input> SingleStatementContextAttrs<'input> for SingleStatementContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn singleStatement(&mut self,)
	-> Result<Rc<SingleStatementContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SingleStatementContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 2, RULE_singleStatement);
        let mut _localctx: Rc<SingleStatementContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(284);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while (((_la) & !0x3f) == 0 && ((1usize << _la) & ((1usize << T__0) | (1usize << ADD) | (1usize << ALTER) | (1usize << ANALYZE) | (1usize << CACHE))) != 0) || ((((_la - 36)) & !0x3f) == 0 && ((1usize << (_la - 36)) & ((1usize << (CLEAR - 36)) | (1usize << (COMMENT - 36)) | (1usize << (COMMIT - 36)) | (1usize << (CREATE - 36)) | (1usize << (DELETE - 36)) | (1usize << (DESC - 36)))) != 0) || ((((_la - 68)) & !0x3f) == 0 && ((1usize << (_la - 68)) & ((1usize << (DESCRIBE - 68)) | (1usize << (DFS - 68)) | (1usize << (DROP - 68)) | (1usize << (EXPLAIN - 68)) | (1usize << (EXPORT - 68)) | (1usize << (FROM - 68)))) != 0) || ((((_la - 104)) & !0x3f) == 0 && ((1usize << (_la - 104)) & ((1usize << (GRANT - 104)) | (1usize << (IMPORT - 104)) | (1usize << (INSERT - 104)) | (1usize << (LIST - 104)) | (1usize << (LOAD - 104)))) != 0) || ((((_la - 137)) & !0x3f) == 0 && ((1usize << (_la - 137)) & ((1usize << (LOCK - 137)) | (1usize << (MAP - 137)) | (1usize << (MERGE - 137)) | (1usize << (MSCK - 137)))) != 0) || ((((_la - 183)) & !0x3f) == 0 && ((1usize << (_la - 183)) & ((1usize << (REDUCE - 183)) | (1usize << (REFRESH - 183)) | (1usize << (REPLACE - 183)) | (1usize << (RESET - 183)) | (1usize << (REVOKE - 183)) | (1usize << (ROLLBACK - 183)) | (1usize << (SELECT - 183)) | (1usize << (SET - 183)) | (1usize << (SHOW - 183)))) != 0) || ((((_la - 215)) & !0x3f) == 0 && ((1usize << (_la - 215)) & ((1usize << (START - 215)) | (1usize << (TABLE - 215)) | (1usize << (TRUNCATE - 215)) | (1usize << (UNCACHE - 215)) | (1usize << (UNLOCK - 215)))) != 0) || ((((_la - 248)) & !0x3f) == 0 && ((1usize << (_la - 248)) & ((1usize << (UPDATE - 248)) | (1usize << (USE - 248)) | (1usize << (VALUES - 248)) | (1usize << (WITH - 248)) | (1usize << (SEMICOLON - 248)))) != 0) {
				{
				recog.base.set_state(282);
				recog.err_handler.sync(&mut recog.base)?;
				match recog.base.input.la(1) {
				 T__0 | ADD | ALTER | ANALYZE | CACHE | CLEAR | COMMENT | COMMIT | CREATE |
				 DELETE | DESC | DESCRIBE | DFS | DROP | EXPLAIN | EXPORT | FROM | GRANT |
				 IMPORT | INSERT | LIST | LOAD | LOCK | MAP | MERGE | MSCK | REDUCE |
				 REFRESH | REPLACE | RESET | REVOKE | ROLLBACK | SELECT | SET | SHOW |
				 START | TABLE | TRUNCATE | UNCACHE | UNLOCK | UPDATE | USE | VALUES |
				 WITH 
					=> {
						{
						/*InvokeRule statement*/
						recog.base.set_state(277);
						recog.statement()?;

						recog.base.set_state(279);
						recog.err_handler.sync(&mut recog.base)?;
						match  recog.interpreter.adaptive_predict(0,&mut recog.base)? {
							x if x == 1=>{
								{
								recog.base.set_state(278);
								recog.base.match_token(SEMICOLON,&mut recog.err_handler)?;

								}
							}

							_ => {}
						}
						}
					}

				 SEMICOLON 
					=> {
						{
						/*InvokeRule emptyStatement*/
						recog.base.set_state(281);
						recog.emptyStatement()?;

						}
					}

					_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
				}
				}
				recog.base.set_state(286);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- emptyStatement ----------------
pub type EmptyStatementContextAll<'input> = EmptyStatementContext<'input>;


pub type EmptyStatementContext<'input> = BaseParserRuleContext<'input,EmptyStatementContextExt<'input>>;

#[derive(Clone)]
pub struct EmptyStatementContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for EmptyStatementContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for EmptyStatementContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_emptyStatement(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_emptyStatement(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for EmptyStatementContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_emptyStatement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_emptyStatement }
}
crate::tid!{EmptyStatementContextExt<'a>}

impl<'input> EmptyStatementContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<EmptyStatementContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,EmptyStatementContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait EmptyStatementContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<EmptyStatementContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token SEMICOLON
/// Returns `None` if there is no child corresponding to token SEMICOLON
fn SEMICOLON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SEMICOLON, 0)
}

}

impl<'input> EmptyStatementContextAttrs<'input> for EmptyStatementContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn emptyStatement(&mut self,)
	-> Result<Rc<EmptyStatementContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = EmptyStatementContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 4, RULE_emptyStatement);
        let mut _localctx: Rc<EmptyStatementContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(287);
			recog.base.match_token(SEMICOLON,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- singleExpression ----------------
pub type SingleExpressionContextAll<'input> = SingleExpressionContext<'input>;


pub type SingleExpressionContext<'input> = BaseParserRuleContext<'input,SingleExpressionContextExt<'input>>;

#[derive(Clone)]
pub struct SingleExpressionContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SingleExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SingleExpressionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_singleExpression(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_singleExpression(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SingleExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_singleExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_singleExpression }
}
crate::tid!{SingleExpressionContextExt<'a>}

impl<'input> SingleExpressionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SingleExpressionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SingleExpressionContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SingleExpressionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SingleExpressionContextExt<'input>>{

fn namedExpression(&self) -> Option<Rc<NamedExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EOF
/// Returns `None` if there is no child corresponding to token EOF
fn EOF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EOF, 0)
}

}

impl<'input> SingleExpressionContextAttrs<'input> for SingleExpressionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn singleExpression(&mut self,)
	-> Result<Rc<SingleExpressionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SingleExpressionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 6, RULE_singleExpression);
        let mut _localctx: Rc<SingleExpressionContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule namedExpression*/
			recog.base.set_state(289);
			recog.namedExpression()?;

			recog.base.set_state(290);
			recog.base.match_token(EOF,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- singleTableIdentifier ----------------
pub type SingleTableIdentifierContextAll<'input> = SingleTableIdentifierContext<'input>;


pub type SingleTableIdentifierContext<'input> = BaseParserRuleContext<'input,SingleTableIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct SingleTableIdentifierContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SingleTableIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SingleTableIdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_singleTableIdentifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_singleTableIdentifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SingleTableIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_singleTableIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_singleTableIdentifier }
}
crate::tid!{SingleTableIdentifierContextExt<'a>}

impl<'input> SingleTableIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SingleTableIdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SingleTableIdentifierContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SingleTableIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SingleTableIdentifierContextExt<'input>>{

fn tableIdentifier(&self) -> Option<Rc<TableIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EOF
/// Returns `None` if there is no child corresponding to token EOF
fn EOF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EOF, 0)
}

}

impl<'input> SingleTableIdentifierContextAttrs<'input> for SingleTableIdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn singleTableIdentifier(&mut self,)
	-> Result<Rc<SingleTableIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SingleTableIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 8, RULE_singleTableIdentifier);
        let mut _localctx: Rc<SingleTableIdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule tableIdentifier*/
			recog.base.set_state(292);
			recog.tableIdentifier()?;

			recog.base.set_state(293);
			recog.base.match_token(EOF,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- singleMultipartIdentifier ----------------
pub type SingleMultipartIdentifierContextAll<'input> = SingleMultipartIdentifierContext<'input>;


pub type SingleMultipartIdentifierContext<'input> = BaseParserRuleContext<'input,SingleMultipartIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct SingleMultipartIdentifierContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SingleMultipartIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SingleMultipartIdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_singleMultipartIdentifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_singleMultipartIdentifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SingleMultipartIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_singleMultipartIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_singleMultipartIdentifier }
}
crate::tid!{SingleMultipartIdentifierContextExt<'a>}

impl<'input> SingleMultipartIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SingleMultipartIdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SingleMultipartIdentifierContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SingleMultipartIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SingleMultipartIdentifierContextExt<'input>>{

fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EOF
/// Returns `None` if there is no child corresponding to token EOF
fn EOF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EOF, 0)
}

}

impl<'input> SingleMultipartIdentifierContextAttrs<'input> for SingleMultipartIdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn singleMultipartIdentifier(&mut self,)
	-> Result<Rc<SingleMultipartIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SingleMultipartIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 10, RULE_singleMultipartIdentifier);
        let mut _localctx: Rc<SingleMultipartIdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule multipartIdentifier*/
			recog.base.set_state(295);
			recog.multipartIdentifier()?;

			recog.base.set_state(296);
			recog.base.match_token(EOF,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- singleDataType ----------------
pub type SingleDataTypeContextAll<'input> = SingleDataTypeContext<'input>;


pub type SingleDataTypeContext<'input> = BaseParserRuleContext<'input,SingleDataTypeContextExt<'input>>;

#[derive(Clone)]
pub struct SingleDataTypeContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SingleDataTypeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SingleDataTypeContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_singleDataType(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_singleDataType(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SingleDataTypeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_singleDataType }
	//fn type_rule_index() -> usize where Self: Sized { RULE_singleDataType }
}
crate::tid!{SingleDataTypeContextExt<'a>}

impl<'input> SingleDataTypeContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SingleDataTypeContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SingleDataTypeContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SingleDataTypeContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SingleDataTypeContextExt<'input>>{

fn dataType(&self) -> Option<Rc<DataTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EOF
/// Returns `None` if there is no child corresponding to token EOF
fn EOF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EOF, 0)
}

}

impl<'input> SingleDataTypeContextAttrs<'input> for SingleDataTypeContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn singleDataType(&mut self,)
	-> Result<Rc<SingleDataTypeContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SingleDataTypeContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 12, RULE_singleDataType);
        let mut _localctx: Rc<SingleDataTypeContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule dataType*/
			recog.base.set_state(298);
			recog.dataType()?;

			recog.base.set_state(299);
			recog.base.match_token(EOF,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- singleTableSchema ----------------
pub type SingleTableSchemaContextAll<'input> = SingleTableSchemaContext<'input>;


pub type SingleTableSchemaContext<'input> = BaseParserRuleContext<'input,SingleTableSchemaContextExt<'input>>;

#[derive(Clone)]
pub struct SingleTableSchemaContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SingleTableSchemaContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SingleTableSchemaContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_singleTableSchema(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_singleTableSchema(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SingleTableSchemaContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_singleTableSchema }
	//fn type_rule_index() -> usize where Self: Sized { RULE_singleTableSchema }
}
crate::tid!{SingleTableSchemaContextExt<'a>}

impl<'input> SingleTableSchemaContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SingleTableSchemaContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SingleTableSchemaContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SingleTableSchemaContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SingleTableSchemaContextExt<'input>>{

fn colTypeList(&self) -> Option<Rc<ColTypeListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EOF
/// Returns `None` if there is no child corresponding to token EOF
fn EOF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EOF, 0)
}

}

impl<'input> SingleTableSchemaContextAttrs<'input> for SingleTableSchemaContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn singleTableSchema(&mut self,)
	-> Result<Rc<SingleTableSchemaContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SingleTableSchemaContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 14, RULE_singleTableSchema);
        let mut _localctx: Rc<SingleTableSchemaContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule colTypeList*/
			recog.base.set_state(301);
			recog.colTypeList()?;

			recog.base.set_state(302);
			recog.base.match_token(EOF,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- statement ----------------
#[derive(Debug)]
pub enum StatementContextAll<'input>{
	ExplainContext(ExplainContext<'input>),
	ResetConfigurationContext(ResetConfigurationContext<'input>),
	AlterViewQueryContext(AlterViewQueryContext<'input>),
	UseContext(UseContext<'input>),
	DropNamespaceContext(DropNamespaceContext<'input>),
	CreateTempViewUsingContext(CreateTempViewUsingContext<'input>),
	RenameTableContext(RenameTableContext<'input>),
	FailNativeCommandContext(FailNativeCommandContext<'input>),
	ClearCacheContext(ClearCacheContext<'input>),
	DropViewContext(DropViewContext<'input>),
	ShowTablesContext(ShowTablesContext<'input>),
	RecoverPartitionsContext(RecoverPartitionsContext<'input>),
	ShowCurrentNamespaceContext(ShowCurrentNamespaceContext<'input>),
	RenameTablePartitionContext(RenameTablePartitionContext<'input>),
	RepairTableContext(RepairTableContext<'input>),
	RefreshResourceContext(RefreshResourceContext<'input>),
	ShowCreateTableContext(ShowCreateTableContext<'input>),
	ShowNamespacesContext(ShowNamespacesContext<'input>),
	ShowColumnsContext(ShowColumnsContext<'input>),
	ReplaceTableContext(ReplaceTableContext<'input>),
	AddTablePartitionContext(AddTablePartitionContext<'input>),
	SetNamespaceLocationContext(SetNamespaceLocationContext<'input>),
	RefreshTableContext(RefreshTableContext<'input>),
	SetNamespacePropertiesContext(SetNamespacePropertiesContext<'input>),
	ManageResourceContext(ManageResourceContext<'input>),
	SetQuotedConfigurationContext(SetQuotedConfigurationContext<'input>),
	AnalyzeContext(AnalyzeContext<'input>),
	CreateHiveTableContext(CreateHiveTableContext<'input>),
	CreateFunctionContext(CreateFunctionContext<'input>),
	ShowTableContext(ShowTableContext<'input>),
	HiveReplaceColumnsContext(HiveReplaceColumnsContext<'input>),
	CommentNamespaceContext(CommentNamespaceContext<'input>),
	ResetQuotedConfigurationContext(ResetQuotedConfigurationContext<'input>),
	CreateTableContext(CreateTableContext<'input>),
	DmlStatementContext(DmlStatementContext<'input>),
	CreateTableLikeContext(CreateTableLikeContext<'input>),
	UncacheTableContext(UncacheTableContext<'input>),
	DropFunctionContext(DropFunctionContext<'input>),
	DescribeRelationContext(DescribeRelationContext<'input>),
	LoadDataContext(LoadDataContext<'input>),
	ShowPartitionsContext(ShowPartitionsContext<'input>),
	DescribeFunctionContext(DescribeFunctionContext<'input>),
	RenameTableColumnContext(RenameTableColumnContext<'input>),
	StatementDefaultContext(StatementDefaultContext<'input>),
	HiveChangeColumnContext(HiveChangeColumnContext<'input>),
	SetTimeZoneContext(SetTimeZoneContext<'input>),
	DescribeQueryContext(DescribeQueryContext<'input>),
	TruncateTableContext(TruncateTableContext<'input>),
	SetTableSerDeContext(SetTableSerDeContext<'input>),
	CreateViewContext(CreateViewContext<'input>),
	DropTablePartitionsContext(DropTablePartitionsContext<'input>),
	SetConfigurationContext(SetConfigurationContext<'input>),
	DropTableContext(DropTableContext<'input>),
	DescribeNamespaceContext(DescribeNamespaceContext<'input>),
	AlterTableAlterColumnContext(AlterTableAlterColumnContext<'input>),
	RefreshFunctionContext(RefreshFunctionContext<'input>),
	CommentTableContext(CommentTableContext<'input>),
	CreateNamespaceContext(CreateNamespaceContext<'input>),
	ShowTblPropertiesContext(ShowTblPropertiesContext<'input>),
	UnsetTablePropertiesContext(UnsetTablePropertiesContext<'input>),
	SetTableLocationContext(SetTableLocationContext<'input>),
	DropTableColumnsContext(DropTableColumnsContext<'input>),
	ShowViewsContext(ShowViewsContext<'input>),
	ShowFunctionsContext(ShowFunctionsContext<'input>),
	CacheTableContext(CacheTableContext<'input>),
	AddTableColumnsContext(AddTableColumnsContext<'input>),
	SetTablePropertiesContext(SetTablePropertiesContext<'input>),
Error(StatementContext<'input>)
}
crate::tid!{StatementContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for StatementContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for StatementContextAll<'input>{}

impl<'input> Deref for StatementContextAll<'input>{
	type Target = dyn StatementContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use StatementContextAll::*;
		match self{
			ExplainContext(inner) => inner,
			ResetConfigurationContext(inner) => inner,
			AlterViewQueryContext(inner) => inner,
			UseContext(inner) => inner,
			DropNamespaceContext(inner) => inner,
			CreateTempViewUsingContext(inner) => inner,
			RenameTableContext(inner) => inner,
			FailNativeCommandContext(inner) => inner,
			ClearCacheContext(inner) => inner,
			DropViewContext(inner) => inner,
			ShowTablesContext(inner) => inner,
			RecoverPartitionsContext(inner) => inner,
			ShowCurrentNamespaceContext(inner) => inner,
			RenameTablePartitionContext(inner) => inner,
			RepairTableContext(inner) => inner,
			RefreshResourceContext(inner) => inner,
			ShowCreateTableContext(inner) => inner,
			ShowNamespacesContext(inner) => inner,
			ShowColumnsContext(inner) => inner,
			ReplaceTableContext(inner) => inner,
			AddTablePartitionContext(inner) => inner,
			SetNamespaceLocationContext(inner) => inner,
			RefreshTableContext(inner) => inner,
			SetNamespacePropertiesContext(inner) => inner,
			ManageResourceContext(inner) => inner,
			SetQuotedConfigurationContext(inner) => inner,
			AnalyzeContext(inner) => inner,
			CreateHiveTableContext(inner) => inner,
			CreateFunctionContext(inner) => inner,
			ShowTableContext(inner) => inner,
			HiveReplaceColumnsContext(inner) => inner,
			CommentNamespaceContext(inner) => inner,
			ResetQuotedConfigurationContext(inner) => inner,
			CreateTableContext(inner) => inner,
			DmlStatementContext(inner) => inner,
			CreateTableLikeContext(inner) => inner,
			UncacheTableContext(inner) => inner,
			DropFunctionContext(inner) => inner,
			DescribeRelationContext(inner) => inner,
			LoadDataContext(inner) => inner,
			ShowPartitionsContext(inner) => inner,
			DescribeFunctionContext(inner) => inner,
			RenameTableColumnContext(inner) => inner,
			StatementDefaultContext(inner) => inner,
			HiveChangeColumnContext(inner) => inner,
			SetTimeZoneContext(inner) => inner,
			DescribeQueryContext(inner) => inner,
			TruncateTableContext(inner) => inner,
			SetTableSerDeContext(inner) => inner,
			CreateViewContext(inner) => inner,
			DropTablePartitionsContext(inner) => inner,
			SetConfigurationContext(inner) => inner,
			DropTableContext(inner) => inner,
			DescribeNamespaceContext(inner) => inner,
			AlterTableAlterColumnContext(inner) => inner,
			RefreshFunctionContext(inner) => inner,
			CommentTableContext(inner) => inner,
			CreateNamespaceContext(inner) => inner,
			ShowTblPropertiesContext(inner) => inner,
			UnsetTablePropertiesContext(inner) => inner,
			SetTableLocationContext(inner) => inner,
			DropTableColumnsContext(inner) => inner,
			ShowViewsContext(inner) => inner,
			ShowFunctionsContext(inner) => inner,
			CacheTableContext(inner) => inner,
			AddTableColumnsContext(inner) => inner,
			SetTablePropertiesContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StatementContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type StatementContext<'input> = BaseParserRuleContext<'input,StatementContextExt<'input>>;

#[derive(Clone)]
pub struct StatementContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for StatementContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StatementContext<'input>{
}

impl<'input> CustomRuleContext<'input> for StatementContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}
crate::tid!{StatementContextExt<'a>}

impl<'input> StatementContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<StatementContextAll<'input>> {
		Rc::new(
		StatementContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,StatementContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait StatementContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<StatementContextExt<'input>>{


}

impl<'input> StatementContextAttrs<'input> for StatementContext<'input>{}

pub type ExplainContext<'input> = BaseParserRuleContext<'input,ExplainContextExt<'input>>;

pub trait ExplainContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token EXPLAIN
	/// Returns `None` if there is no child corresponding to token EXPLAIN
	fn EXPLAIN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXPLAIN, 0)
	}
	fn statement(&self) -> Option<Rc<StatementContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token LOGICAL
	/// Returns `None` if there is no child corresponding to token LOGICAL
	fn LOGICAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LOGICAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FORMATTED
	/// Returns `None` if there is no child corresponding to token FORMATTED
	fn FORMATTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FORMATTED, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXTENDED
	/// Returns `None` if there is no child corresponding to token EXTENDED
	fn EXTENDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXTENDED, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CODEGEN
	/// Returns `None` if there is no child corresponding to token CODEGEN
	fn CODEGEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CODEGEN, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COST
	/// Returns `None` if there is no child corresponding to token COST
	fn COST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COST, 0)
	}
}

impl<'input> ExplainContextAttrs<'input> for ExplainContext<'input>{}

pub struct ExplainContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ExplainContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ExplainContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ExplainContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_explain(self);
	}
}

impl<'input> CustomRuleContext<'input> for ExplainContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ExplainContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ExplainContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ExplainContext<'input> {}

impl<'input> ExplainContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ExplainContext(
				BaseParserRuleContext::copy_from(ctx,ExplainContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ResetConfigurationContext<'input> = BaseParserRuleContext<'input,ResetConfigurationContextExt<'input>>;

pub trait ResetConfigurationContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token RESET
	/// Returns `None` if there is no child corresponding to token RESET
	fn RESET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(RESET, 0)
	}
}

impl<'input> ResetConfigurationContextAttrs<'input> for ResetConfigurationContext<'input>{}

pub struct ResetConfigurationContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ResetConfigurationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ResetConfigurationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ResetConfigurationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_resetConfiguration(self);
	}
}

impl<'input> CustomRuleContext<'input> for ResetConfigurationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ResetConfigurationContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ResetConfigurationContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ResetConfigurationContext<'input> {}

impl<'input> ResetConfigurationContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ResetConfigurationContext(
				BaseParserRuleContext::copy_from(ctx,ResetConfigurationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type AlterViewQueryContext<'input> = BaseParserRuleContext<'input,AlterViewQueryContextExt<'input>>;

pub trait AlterViewQueryContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
}

impl<'input> AlterViewQueryContextAttrs<'input> for AlterViewQueryContext<'input>{}

pub struct AlterViewQueryContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{AlterViewQueryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for AlterViewQueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AlterViewQueryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_alterViewQuery(self);
	}
}

impl<'input> CustomRuleContext<'input> for AlterViewQueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for AlterViewQueryContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for AlterViewQueryContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for AlterViewQueryContext<'input> {}

impl<'input> AlterViewQueryContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::AlterViewQueryContext(
				BaseParserRuleContext::copy_from(ctx,AlterViewQueryContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type UseContext<'input> = BaseParserRuleContext<'input,UseContextExt<'input>>;

pub trait UseContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token USE
	/// Returns `None` if there is no child corresponding to token USE
	fn USE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(USE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token NAMESPACE
	/// Returns `None` if there is no child corresponding to token NAMESPACE
	fn NAMESPACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NAMESPACE, 0)
	}
}

impl<'input> UseContextAttrs<'input> for UseContext<'input>{}

pub struct UseContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{UseContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for UseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for UseContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_use(self);
	}
}

impl<'input> CustomRuleContext<'input> for UseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for UseContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for UseContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for UseContext<'input> {}

impl<'input> UseContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::UseContext(
				BaseParserRuleContext::copy_from(ctx,UseContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DropNamespaceContext<'input> = BaseParserRuleContext<'input,DropNamespaceContextExt<'input>>;

pub trait DropNamespaceContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token DROP
	/// Returns `None` if there is no child corresponding to token DROP
	fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DROP, 0)
	}
	fn namespace(&self) -> Option<Rc<NamespaceContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token RESTRICT
	/// Returns `None` if there is no child corresponding to token RESTRICT
	fn RESTRICT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(RESTRICT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CASCADE
	/// Returns `None` if there is no child corresponding to token CASCADE
	fn CASCADE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CASCADE, 0)
	}
}

impl<'input> DropNamespaceContextAttrs<'input> for DropNamespaceContext<'input>{}

pub struct DropNamespaceContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DropNamespaceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DropNamespaceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DropNamespaceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dropNamespace(self);
	}
}

impl<'input> CustomRuleContext<'input> for DropNamespaceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DropNamespaceContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DropNamespaceContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DropNamespaceContext<'input> {}

impl<'input> DropNamespaceContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DropNamespaceContext(
				BaseParserRuleContext::copy_from(ctx,DropNamespaceContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CreateTempViewUsingContext<'input> = BaseParserRuleContext<'input,CreateTempViewUsingContextExt<'input>>;

pub trait CreateTempViewUsingContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CREATE
	/// Returns `None` if there is no child corresponding to token CREATE
	fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CREATE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TEMPORARY
	/// Returns `None` if there is no child corresponding to token TEMPORARY
	fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TEMPORARY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	fn tableIdentifier(&self) -> Option<Rc<TableIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableProvider(&self) -> Option<Rc<TableProviderContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token OR
	/// Returns `None` if there is no child corresponding to token OR
	fn OR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OR, 0)
	}
	/// Retrieves first TerminalNode corresponding to token REPLACE
	/// Returns `None` if there is no child corresponding to token REPLACE
	fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REPLACE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token GLOBAL
	/// Returns `None` if there is no child corresponding to token GLOBAL
	fn GLOBAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(GLOBAL, 0)
	}
	fn colTypeList(&self) -> Option<Rc<ColTypeListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token OPTIONS
	/// Returns `None` if there is no child corresponding to token OPTIONS
	fn OPTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OPTIONS, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> CreateTempViewUsingContextAttrs<'input> for CreateTempViewUsingContext<'input>{}

pub struct CreateTempViewUsingContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{CreateTempViewUsingContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CreateTempViewUsingContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateTempViewUsingContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_createTempViewUsing(self);
	}
}

impl<'input> CustomRuleContext<'input> for CreateTempViewUsingContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CreateTempViewUsingContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CreateTempViewUsingContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CreateTempViewUsingContext<'input> {}

impl<'input> CreateTempViewUsingContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CreateTempViewUsingContext(
				BaseParserRuleContext::copy_from(ctx,CreateTempViewUsingContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RenameTableContext<'input> = BaseParserRuleContext<'input,RenameTableContextExt<'input>>;

pub trait RenameTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token RENAME
	/// Returns `None` if there is no child corresponding to token RENAME
	fn RENAME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(RENAME, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TO
	/// Returns `None` if there is no child corresponding to token TO
	fn TO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TO, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	fn multipartIdentifier_all(&self) ->  Vec<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn multipartIdentifier(&self, i: usize) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> RenameTableContextAttrs<'input> for RenameTableContext<'input>{}

pub struct RenameTableContextExt<'input>{
	base:StatementContextExt<'input>,
	pub from: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub to: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{RenameTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RenameTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RenameTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_renameTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for RenameTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RenameTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RenameTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RenameTableContext<'input> {}

impl<'input> RenameTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RenameTableContext(
				BaseParserRuleContext::copy_from(ctx,RenameTableContextExt{
        			from:None, to:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type FailNativeCommandContext<'input> = BaseParserRuleContext<'input,FailNativeCommandContextExt<'input>>;

pub trait FailNativeCommandContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ROLE
	/// Returns `None` if there is no child corresponding to token ROLE
	fn ROLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ROLE, 0)
	}
	fn unsupportedHiveNativeCommands(&self) -> Option<Rc<UnsupportedHiveNativeCommandsContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> FailNativeCommandContextAttrs<'input> for FailNativeCommandContext<'input>{}

pub struct FailNativeCommandContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{FailNativeCommandContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for FailNativeCommandContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FailNativeCommandContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_failNativeCommand(self);
	}
}

impl<'input> CustomRuleContext<'input> for FailNativeCommandContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for FailNativeCommandContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for FailNativeCommandContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for FailNativeCommandContext<'input> {}

impl<'input> FailNativeCommandContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::FailNativeCommandContext(
				BaseParserRuleContext::copy_from(ctx,FailNativeCommandContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ClearCacheContext<'input> = BaseParserRuleContext<'input,ClearCacheContextExt<'input>>;

pub trait ClearCacheContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CLEAR
	/// Returns `None` if there is no child corresponding to token CLEAR
	fn CLEAR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CLEAR, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CACHE
	/// Returns `None` if there is no child corresponding to token CACHE
	fn CACHE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CACHE, 0)
	}
}

impl<'input> ClearCacheContextAttrs<'input> for ClearCacheContext<'input>{}

pub struct ClearCacheContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ClearCacheContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ClearCacheContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ClearCacheContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_clearCache(self);
	}
}

impl<'input> CustomRuleContext<'input> for ClearCacheContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ClearCacheContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ClearCacheContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ClearCacheContext<'input> {}

impl<'input> ClearCacheContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ClearCacheContext(
				BaseParserRuleContext::copy_from(ctx,ClearCacheContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DropViewContext<'input> = BaseParserRuleContext<'input,DropViewContextExt<'input>>;

pub trait DropViewContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token DROP
	/// Returns `None` if there is no child corresponding to token DROP
	fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DROP, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
}

impl<'input> DropViewContextAttrs<'input> for DropViewContext<'input>{}

pub struct DropViewContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DropViewContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DropViewContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DropViewContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dropView(self);
	}
}

impl<'input> CustomRuleContext<'input> for DropViewContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DropViewContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DropViewContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DropViewContext<'input> {}

impl<'input> DropViewContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DropViewContext(
				BaseParserRuleContext::copy_from(ctx,DropViewContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowTablesContext<'input> = BaseParserRuleContext<'input,ShowTablesContextExt<'input>>;

pub trait ShowTablesContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLES
	/// Returns `None` if there is no child corresponding to token TABLES
	fn TABLES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLES, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IN
	/// Returns `None` if there is no child corresponding to token IN
	fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IN, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LIKE
	/// Returns `None` if there is no child corresponding to token LIKE
	fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LIKE, 0)
	}
}

impl<'input> ShowTablesContextAttrs<'input> for ShowTablesContext<'input>{}

pub struct ShowTablesContextExt<'input>{
	base:StatementContextExt<'input>,
	pub pattern: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowTablesContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowTablesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowTablesContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showTables(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowTablesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowTablesContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowTablesContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowTablesContext<'input> {}

impl<'input> ShowTablesContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowTablesContext(
				BaseParserRuleContext::copy_from(ctx,ShowTablesContextExt{
					pattern:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RecoverPartitionsContext<'input> = BaseParserRuleContext<'input,RecoverPartitionsContextExt<'input>>;

pub trait RecoverPartitionsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token RECOVER
	/// Returns `None` if there is no child corresponding to token RECOVER
	fn RECOVER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(RECOVER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PARTITIONS
	/// Returns `None` if there is no child corresponding to token PARTITIONS
	fn PARTITIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PARTITIONS, 0)
	}
}

impl<'input> RecoverPartitionsContextAttrs<'input> for RecoverPartitionsContext<'input>{}

pub struct RecoverPartitionsContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RecoverPartitionsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RecoverPartitionsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RecoverPartitionsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_recoverPartitions(self);
	}
}

impl<'input> CustomRuleContext<'input> for RecoverPartitionsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RecoverPartitionsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RecoverPartitionsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RecoverPartitionsContext<'input> {}

impl<'input> RecoverPartitionsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RecoverPartitionsContext(
				BaseParserRuleContext::copy_from(ctx,RecoverPartitionsContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowCurrentNamespaceContext<'input> = BaseParserRuleContext<'input,ShowCurrentNamespaceContextExt<'input>>;

pub trait ShowCurrentNamespaceContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CURRENT
	/// Returns `None` if there is no child corresponding to token CURRENT
	fn CURRENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CURRENT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NAMESPACE
	/// Returns `None` if there is no child corresponding to token NAMESPACE
	fn NAMESPACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NAMESPACE, 0)
	}
}

impl<'input> ShowCurrentNamespaceContextAttrs<'input> for ShowCurrentNamespaceContext<'input>{}

pub struct ShowCurrentNamespaceContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowCurrentNamespaceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowCurrentNamespaceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowCurrentNamespaceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showCurrentNamespace(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowCurrentNamespaceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowCurrentNamespaceContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowCurrentNamespaceContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowCurrentNamespaceContext<'input> {}

impl<'input> ShowCurrentNamespaceContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowCurrentNamespaceContext(
				BaseParserRuleContext::copy_from(ctx,ShowCurrentNamespaceContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RenameTablePartitionContext<'input> = BaseParserRuleContext<'input,RenameTablePartitionContextExt<'input>>;

pub trait RenameTablePartitionContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token RENAME
	/// Returns `None` if there is no child corresponding to token RENAME
	fn RENAME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(RENAME, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TO
	/// Returns `None` if there is no child corresponding to token TO
	fn TO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TO, 0)
	}
	fn partitionSpec_all(&self) ->  Vec<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn partitionSpec(&self, i: usize) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> RenameTablePartitionContextAttrs<'input> for RenameTablePartitionContext<'input>{}

pub struct RenameTablePartitionContextExt<'input>{
	base:StatementContextExt<'input>,
	pub from: Option<Rc<PartitionSpecContextAll<'input>>>,
	pub to: Option<Rc<PartitionSpecContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{RenameTablePartitionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RenameTablePartitionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RenameTablePartitionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_renameTablePartition(self);
	}
}

impl<'input> CustomRuleContext<'input> for RenameTablePartitionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RenameTablePartitionContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RenameTablePartitionContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RenameTablePartitionContext<'input> {}

impl<'input> RenameTablePartitionContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RenameTablePartitionContext(
				BaseParserRuleContext::copy_from(ctx,RenameTablePartitionContextExt{
        			from:None, to:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RepairTableContext<'input> = BaseParserRuleContext<'input,RepairTableContextExt<'input>>;

pub trait RepairTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token MSCK
	/// Returns `None` if there is no child corresponding to token MSCK
	fn MSCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MSCK, 0)
	}
	/// Retrieves first TerminalNode corresponding to token REPAIR
	/// Returns `None` if there is no child corresponding to token REPAIR
	fn REPAIR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REPAIR, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> RepairTableContextAttrs<'input> for RepairTableContext<'input>{}

pub struct RepairTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RepairTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RepairTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RepairTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_repairTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for RepairTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RepairTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RepairTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RepairTableContext<'input> {}

impl<'input> RepairTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RepairTableContext(
				BaseParserRuleContext::copy_from(ctx,RepairTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RefreshResourceContext<'input> = BaseParserRuleContext<'input,RefreshResourceContextExt<'input>>;

pub trait RefreshResourceContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token REFRESH
	/// Returns `None` if there is no child corresponding to token REFRESH
	fn REFRESH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REFRESH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
}

impl<'input> RefreshResourceContextAttrs<'input> for RefreshResourceContext<'input>{}

pub struct RefreshResourceContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RefreshResourceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RefreshResourceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RefreshResourceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_refreshResource(self);
	}
}

impl<'input> CustomRuleContext<'input> for RefreshResourceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RefreshResourceContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RefreshResourceContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RefreshResourceContext<'input> {}

impl<'input> RefreshResourceContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RefreshResourceContext(
				BaseParserRuleContext::copy_from(ctx,RefreshResourceContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowCreateTableContext<'input> = BaseParserRuleContext<'input,ShowCreateTableContextExt<'input>>;

pub trait ShowCreateTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CREATE
	/// Returns `None` if there is no child corresponding to token CREATE
	fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CREATE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SERDE
	/// Returns `None` if there is no child corresponding to token SERDE
	fn SERDE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SERDE, 0)
	}
}

impl<'input> ShowCreateTableContextAttrs<'input> for ShowCreateTableContext<'input>{}

pub struct ShowCreateTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowCreateTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowCreateTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowCreateTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showCreateTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowCreateTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowCreateTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowCreateTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowCreateTableContext<'input> {}

impl<'input> ShowCreateTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowCreateTableContext(
				BaseParserRuleContext::copy_from(ctx,ShowCreateTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowNamespacesContext<'input> = BaseParserRuleContext<'input,ShowNamespacesContextExt<'input>>;

pub trait ShowNamespacesContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DATABASES
	/// Returns `None` if there is no child corresponding to token DATABASES
	fn DATABASES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DATABASES, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NAMESPACES
	/// Returns `None` if there is no child corresponding to token NAMESPACES
	fn NAMESPACES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NAMESPACES, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IN
	/// Returns `None` if there is no child corresponding to token IN
	fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IN, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LIKE
	/// Returns `None` if there is no child corresponding to token LIKE
	fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LIKE, 0)
	}
}

impl<'input> ShowNamespacesContextAttrs<'input> for ShowNamespacesContext<'input>{}

pub struct ShowNamespacesContextExt<'input>{
	base:StatementContextExt<'input>,
	pub pattern: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowNamespacesContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowNamespacesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowNamespacesContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showNamespaces(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowNamespacesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowNamespacesContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowNamespacesContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowNamespacesContext<'input> {}

impl<'input> ShowNamespacesContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowNamespacesContext(
				BaseParserRuleContext::copy_from(ctx,ShowNamespacesContextExt{
					pattern:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowColumnsContext<'input> = BaseParserRuleContext<'input,ShowColumnsContextExt<'input>>;

pub trait ShowColumnsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMNS
	/// Returns `None` if there is no child corresponding to token COLUMNS
	fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMNS, 0)
	}
	/// Retrieves all `TerminalNode`s corresponding to token FROM in current rule
	fn FROM_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token FROM, starting from 0.
	/// Returns `None` if number of children corresponding to token FROM is less or equal than `i`.
	fn FROM(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token IN in current rule
	fn IN_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token IN, starting from 0.
	/// Returns `None` if number of children corresponding to token IN is less or equal than `i`.
	fn IN(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IN, i)
	}
	fn multipartIdentifier_all(&self) ->  Vec<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn multipartIdentifier(&self, i: usize) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> ShowColumnsContextAttrs<'input> for ShowColumnsContext<'input>{}

pub struct ShowColumnsContextExt<'input>{
	base:StatementContextExt<'input>,
	pub table: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub ns: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowColumnsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowColumnsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowColumnsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showColumns(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowColumnsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowColumnsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowColumnsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowColumnsContext<'input> {}

impl<'input> ShowColumnsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowColumnsContext(
				BaseParserRuleContext::copy_from(ctx,ShowColumnsContextExt{
        			table:None, ns:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ReplaceTableContext<'input> = BaseParserRuleContext<'input,ReplaceTableContextExt<'input>>;

pub trait ReplaceTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn replaceTableHeader(&self) -> Option<Rc<ReplaceTableHeaderContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableProvider(&self) -> Option<Rc<TableProviderContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn createTableClauses(&self) -> Option<Rc<CreateTableClausesContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn colTypeList(&self) -> Option<Rc<ColTypeListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
}

impl<'input> ReplaceTableContextAttrs<'input> for ReplaceTableContext<'input>{}

pub struct ReplaceTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ReplaceTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ReplaceTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ReplaceTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_replaceTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for ReplaceTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ReplaceTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ReplaceTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ReplaceTableContext<'input> {}

impl<'input> ReplaceTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ReplaceTableContext(
				BaseParserRuleContext::copy_from(ctx,ReplaceTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type AddTablePartitionContext<'input> = BaseParserRuleContext<'input,AddTablePartitionContextExt<'input>>;

pub trait AddTablePartitionContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token ADD
	/// Returns `None` if there is no child corresponding to token ADD
	fn ADD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ADD, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	fn partitionSpecLocation_all(&self) ->  Vec<Rc<PartitionSpecLocationContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn partitionSpecLocation(&self, i: usize) -> Option<Rc<PartitionSpecLocationContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> AddTablePartitionContextAttrs<'input> for AddTablePartitionContext<'input>{}

pub struct AddTablePartitionContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{AddTablePartitionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for AddTablePartitionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AddTablePartitionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_addTablePartition(self);
	}
}

impl<'input> CustomRuleContext<'input> for AddTablePartitionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for AddTablePartitionContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for AddTablePartitionContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for AddTablePartitionContext<'input> {}

impl<'input> AddTablePartitionContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::AddTablePartitionContext(
				BaseParserRuleContext::copy_from(ctx,AddTablePartitionContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetNamespaceLocationContext<'input> = BaseParserRuleContext<'input,SetNamespaceLocationContextExt<'input>>;

pub trait SetNamespaceLocationContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	fn namespace(&self) -> Option<Rc<NamespaceContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	fn locationSpec(&self) -> Option<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SetNamespaceLocationContextAttrs<'input> for SetNamespaceLocationContext<'input>{}

pub struct SetNamespaceLocationContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetNamespaceLocationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetNamespaceLocationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetNamespaceLocationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setNamespaceLocation(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetNamespaceLocationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetNamespaceLocationContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetNamespaceLocationContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetNamespaceLocationContext<'input> {}

impl<'input> SetNamespaceLocationContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetNamespaceLocationContext(
				BaseParserRuleContext::copy_from(ctx,SetNamespaceLocationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RefreshTableContext<'input> = BaseParserRuleContext<'input,RefreshTableContextExt<'input>>;

pub trait RefreshTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token REFRESH
	/// Returns `None` if there is no child corresponding to token REFRESH
	fn REFRESH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REFRESH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> RefreshTableContextAttrs<'input> for RefreshTableContext<'input>{}

pub struct RefreshTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RefreshTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RefreshTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RefreshTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_refreshTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for RefreshTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RefreshTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RefreshTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RefreshTableContext<'input> {}

impl<'input> RefreshTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RefreshTableContext(
				BaseParserRuleContext::copy_from(ctx,RefreshTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetNamespacePropertiesContext<'input> = BaseParserRuleContext<'input,SetNamespacePropertiesContextExt<'input>>;

pub trait SetNamespacePropertiesContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	fn namespace(&self) -> Option<Rc<NamespaceContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token DBPROPERTIES
	/// Returns `None` if there is no child corresponding to token DBPROPERTIES
	fn DBPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DBPROPERTIES, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PROPERTIES
	/// Returns `None` if there is no child corresponding to token PROPERTIES
	fn PROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PROPERTIES, 0)
	}
}

impl<'input> SetNamespacePropertiesContextAttrs<'input> for SetNamespacePropertiesContext<'input>{}

pub struct SetNamespacePropertiesContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetNamespacePropertiesContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetNamespacePropertiesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetNamespacePropertiesContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setNamespaceProperties(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetNamespacePropertiesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetNamespacePropertiesContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetNamespacePropertiesContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetNamespacePropertiesContext<'input> {}

impl<'input> SetNamespacePropertiesContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetNamespacePropertiesContext(
				BaseParserRuleContext::copy_from(ctx,SetNamespacePropertiesContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ManageResourceContext<'input> = BaseParserRuleContext<'input,ManageResourceContextExt<'input>>;

pub trait ManageResourceContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token ADD
	/// Returns `None` if there is no child corresponding to token ADD
	fn ADD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ADD, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LIST
	/// Returns `None` if there is no child corresponding to token LIST
	fn LIST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LIST, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
}

impl<'input> ManageResourceContextAttrs<'input> for ManageResourceContext<'input>{}

pub struct ManageResourceContextExt<'input>{
	base:StatementContextExt<'input>,
	pub op: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ManageResourceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ManageResourceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ManageResourceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_manageResource(self);
	}
}

impl<'input> CustomRuleContext<'input> for ManageResourceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ManageResourceContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ManageResourceContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ManageResourceContext<'input> {}

impl<'input> ManageResourceContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ManageResourceContext(
				BaseParserRuleContext::copy_from(ctx,ManageResourceContextExt{
					op:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetQuotedConfigurationContext<'input> = BaseParserRuleContext<'input,SetQuotedConfigurationContextExt<'input>>;

pub trait SetQuotedConfigurationContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	fn configKey(&self) -> Option<Rc<ConfigKeyContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token EQ
	/// Returns `None` if there is no child corresponding to token EQ
	fn EQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EQ, 0)
	}
}

impl<'input> SetQuotedConfigurationContextAttrs<'input> for SetQuotedConfigurationContext<'input>{}

pub struct SetQuotedConfigurationContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetQuotedConfigurationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetQuotedConfigurationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetQuotedConfigurationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setQuotedConfiguration(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetQuotedConfigurationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetQuotedConfigurationContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetQuotedConfigurationContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetQuotedConfigurationContext<'input> {}

impl<'input> SetQuotedConfigurationContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetQuotedConfigurationContext(
				BaseParserRuleContext::copy_from(ctx,SetQuotedConfigurationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type AnalyzeContext<'input> = BaseParserRuleContext<'input,AnalyzeContextExt<'input>>;

pub trait AnalyzeContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ANALYZE
	/// Returns `None` if there is no child corresponding to token ANALYZE
	fn ANALYZE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ANALYZE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token COMPUTE
	/// Returns `None` if there is no child corresponding to token COMPUTE
	fn COMPUTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COMPUTE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STATISTICS
	/// Returns `None` if there is no child corresponding to token STATISTICS
	fn STATISTICS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STATISTICS, 0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token FOR
	/// Returns `None` if there is no child corresponding to token FOR
	fn FOR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FOR, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMNS
	/// Returns `None` if there is no child corresponding to token COLUMNS
	fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMNS, 0)
	}
	fn identifierSeq(&self) -> Option<Rc<IdentifierSeqContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token ALL
	/// Returns `None` if there is no child corresponding to token ALL
	fn ALL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALL, 0)
	}
}

impl<'input> AnalyzeContextAttrs<'input> for AnalyzeContext<'input>{}

pub struct AnalyzeContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{AnalyzeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for AnalyzeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AnalyzeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_analyze(self);
	}
}

impl<'input> CustomRuleContext<'input> for AnalyzeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for AnalyzeContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for AnalyzeContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for AnalyzeContext<'input> {}

impl<'input> AnalyzeContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::AnalyzeContext(
				BaseParserRuleContext::copy_from(ctx,AnalyzeContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CreateHiveTableContext<'input> = BaseParserRuleContext<'input,CreateHiveTableContextExt<'input>>;

pub trait CreateHiveTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn createTableHeader(&self) -> Option<Rc<CreateTableHeaderContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn commentSpec_all(&self) ->  Vec<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn commentSpec(&self, i: usize) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn bucketSpec_all(&self) ->  Vec<Rc<BucketSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn bucketSpec(&self, i: usize) -> Option<Rc<BucketSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn skewSpec_all(&self) ->  Vec<Rc<SkewSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn skewSpec(&self, i: usize) -> Option<Rc<SkewSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn rowFormat_all(&self) ->  Vec<Rc<RowFormatContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn rowFormat(&self, i: usize) -> Option<Rc<RowFormatContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn createFileFormat_all(&self) ->  Vec<Rc<CreateFileFormatContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn createFileFormat(&self, i: usize) -> Option<Rc<CreateFileFormatContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn locationSpec_all(&self) ->  Vec<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn locationSpec(&self, i: usize) -> Option<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn colTypeList_all(&self) ->  Vec<Rc<ColTypeListContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn colTypeList(&self, i: usize) -> Option<Rc<ColTypeListContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token PARTITIONED in current rule
	fn PARTITIONED_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token PARTITIONED, starting from 0.
	/// Returns `None` if number of children corresponding to token PARTITIONED is less or equal than `i`.
	fn PARTITIONED(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PARTITIONED, i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token BY in current rule
	fn BY_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token BY, starting from 0.
	/// Returns `None` if number of children corresponding to token BY is less or equal than `i`.
	fn BY(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(BY, i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token TBLPROPERTIES in current rule
	fn TBLPROPERTIES_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token TBLPROPERTIES, starting from 0.
	/// Returns `None` if number of children corresponding to token TBLPROPERTIES is less or equal than `i`.
	fn TBLPROPERTIES(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TBLPROPERTIES, i)
	}
	fn identifierList_all(&self) ->  Vec<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn identifierList(&self, i: usize) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn tablePropertyList_all(&self) ->  Vec<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn tablePropertyList(&self, i: usize) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
}

impl<'input> CreateHiveTableContextAttrs<'input> for CreateHiveTableContext<'input>{}

pub struct CreateHiveTableContextExt<'input>{
	base:StatementContextExt<'input>,
	pub columns: Option<Rc<ColTypeListContextAll<'input>>>,
	pub partitionColumns: Option<Rc<ColTypeListContextAll<'input>>>,
	pub partitionColumnNames: Option<Rc<IdentifierListContextAll<'input>>>,
	pub tableProps: Option<Rc<TablePropertyListContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{CreateHiveTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CreateHiveTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateHiveTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_createHiveTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for CreateHiveTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CreateHiveTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CreateHiveTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CreateHiveTableContext<'input> {}

impl<'input> CreateHiveTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CreateHiveTableContext(
				BaseParserRuleContext::copy_from(ctx,CreateHiveTableContextExt{
        			columns:None, partitionColumns:None, partitionColumnNames:None, tableProps:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CreateFunctionContext<'input> = BaseParserRuleContext<'input,CreateFunctionContextExt<'input>>;

pub trait CreateFunctionContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CREATE
	/// Returns `None` if there is no child corresponding to token CREATE
	fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CREATE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FUNCTION
	/// Returns `None` if there is no child corresponding to token FUNCTION
	fn FUNCTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FUNCTION, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OR
	/// Returns `None` if there is no child corresponding to token OR
	fn OR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OR, 0)
	}
	/// Retrieves first TerminalNode corresponding to token REPLACE
	/// Returns `None` if there is no child corresponding to token REPLACE
	fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REPLACE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TEMPORARY
	/// Returns `None` if there is no child corresponding to token TEMPORARY
	fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TEMPORARY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token USING
	/// Returns `None` if there is no child corresponding to token USING
	fn USING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(USING, 0)
	}
	fn resource_all(&self) ->  Vec<Rc<ResourceContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn resource(&self, i: usize) -> Option<Rc<ResourceContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> CreateFunctionContextAttrs<'input> for CreateFunctionContext<'input>{}

pub struct CreateFunctionContextExt<'input>{
	base:StatementContextExt<'input>,
	pub className: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{CreateFunctionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CreateFunctionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateFunctionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_createFunction(self);
	}
}

impl<'input> CustomRuleContext<'input> for CreateFunctionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CreateFunctionContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CreateFunctionContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CreateFunctionContext<'input> {}

impl<'input> CreateFunctionContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CreateFunctionContext(
				BaseParserRuleContext::copy_from(ctx,CreateFunctionContextExt{
					className:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowTableContext<'input> = BaseParserRuleContext<'input,ShowTableContextExt<'input>>;

pub trait ShowTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXTENDED
	/// Returns `None` if there is no child corresponding to token EXTENDED
	fn EXTENDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXTENDED, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LIKE
	/// Returns `None` if there is no child corresponding to token LIKE
	fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LIKE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IN
	/// Returns `None` if there is no child corresponding to token IN
	fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IN, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ShowTableContextAttrs<'input> for ShowTableContext<'input>{}

pub struct ShowTableContextExt<'input>{
	base:StatementContextExt<'input>,
	pub ns: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub pattern: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowTableContext<'input> {}

impl<'input> ShowTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowTableContext(
				BaseParserRuleContext::copy_from(ctx,ShowTableContextExt{
					pattern:None, 
        			ns:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type HiveReplaceColumnsContext<'input> = BaseParserRuleContext<'input,HiveReplaceColumnsContextExt<'input>>;

pub trait HiveReplaceColumnsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token REPLACE
	/// Returns `None` if there is no child corresponding to token REPLACE
	fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REPLACE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMNS
	/// Returns `None` if there is no child corresponding to token COLUMNS
	fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMNS, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn qualifiedColTypeWithPositionList(&self) -> Option<Rc<QualifiedColTypeWithPositionListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> HiveReplaceColumnsContextAttrs<'input> for HiveReplaceColumnsContext<'input>{}

pub struct HiveReplaceColumnsContextExt<'input>{
	base:StatementContextExt<'input>,
	pub table: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub columns: Option<Rc<QualifiedColTypeWithPositionListContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{HiveReplaceColumnsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for HiveReplaceColumnsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for HiveReplaceColumnsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_hiveReplaceColumns(self);
	}
}

impl<'input> CustomRuleContext<'input> for HiveReplaceColumnsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for HiveReplaceColumnsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for HiveReplaceColumnsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for HiveReplaceColumnsContext<'input> {}

impl<'input> HiveReplaceColumnsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::HiveReplaceColumnsContext(
				BaseParserRuleContext::copy_from(ctx,HiveReplaceColumnsContextExt{
        			table:None, columns:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CommentNamespaceContext<'input> = BaseParserRuleContext<'input,CommentNamespaceContextExt<'input>>;

pub trait CommentNamespaceContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token COMMENT
	/// Returns `None` if there is no child corresponding to token COMMENT
	fn COMMENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COMMENT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ON
	/// Returns `None` if there is no child corresponding to token ON
	fn ON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ON, 0)
	}
	fn namespace(&self) -> Option<Rc<NamespaceContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IS
	/// Returns `None` if there is no child corresponding to token IS
	fn IS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NULL
	/// Returns `None` if there is no child corresponding to token NULL
	fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NULL, 0)
	}
}

impl<'input> CommentNamespaceContextAttrs<'input> for CommentNamespaceContext<'input>{}

pub struct CommentNamespaceContextExt<'input>{
	base:StatementContextExt<'input>,
	pub comment: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{CommentNamespaceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CommentNamespaceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CommentNamespaceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_commentNamespace(self);
	}
}

impl<'input> CustomRuleContext<'input> for CommentNamespaceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CommentNamespaceContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CommentNamespaceContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CommentNamespaceContext<'input> {}

impl<'input> CommentNamespaceContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CommentNamespaceContext(
				BaseParserRuleContext::copy_from(ctx,CommentNamespaceContextExt{
					comment:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ResetQuotedConfigurationContext<'input> = BaseParserRuleContext<'input,ResetQuotedConfigurationContextExt<'input>>;

pub trait ResetQuotedConfigurationContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token RESET
	/// Returns `None` if there is no child corresponding to token RESET
	fn RESET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(RESET, 0)
	}
	fn configKey(&self) -> Option<Rc<ConfigKeyContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ResetQuotedConfigurationContextAttrs<'input> for ResetQuotedConfigurationContext<'input>{}

pub struct ResetQuotedConfigurationContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ResetQuotedConfigurationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ResetQuotedConfigurationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ResetQuotedConfigurationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_resetQuotedConfiguration(self);
	}
}

impl<'input> CustomRuleContext<'input> for ResetQuotedConfigurationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ResetQuotedConfigurationContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ResetQuotedConfigurationContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ResetQuotedConfigurationContext<'input> {}

impl<'input> ResetQuotedConfigurationContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ResetQuotedConfigurationContext(
				BaseParserRuleContext::copy_from(ctx,ResetQuotedConfigurationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CreateTableContext<'input> = BaseParserRuleContext<'input,CreateTableContextExt<'input>>;

pub trait CreateTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn createTableHeader(&self) -> Option<Rc<CreateTableHeaderContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableProvider(&self) -> Option<Rc<TableProviderContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn createTableClauses(&self) -> Option<Rc<CreateTableClausesContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn colTypeList(&self) -> Option<Rc<ColTypeListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
}

impl<'input> CreateTableContextAttrs<'input> for CreateTableContext<'input>{}

pub struct CreateTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{CreateTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CreateTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_createTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for CreateTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CreateTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CreateTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CreateTableContext<'input> {}

impl<'input> CreateTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CreateTableContext(
				BaseParserRuleContext::copy_from(ctx,CreateTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DmlStatementContext<'input> = BaseParserRuleContext<'input,DmlStatementContextExt<'input>>;

pub trait DmlStatementContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn dmlStatementNoWith(&self) -> Option<Rc<DmlStatementNoWithContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn ctes(&self) -> Option<Rc<CtesContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> DmlStatementContextAttrs<'input> for DmlStatementContext<'input>{}

pub struct DmlStatementContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DmlStatementContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DmlStatementContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DmlStatementContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dmlStatement(self);
	}
}

impl<'input> CustomRuleContext<'input> for DmlStatementContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DmlStatementContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DmlStatementContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DmlStatementContext<'input> {}

impl<'input> DmlStatementContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DmlStatementContext(
				BaseParserRuleContext::copy_from(ctx,DmlStatementContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CreateTableLikeContext<'input> = BaseParserRuleContext<'input,CreateTableLikeContextExt<'input>>;

pub trait CreateTableLikeContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CREATE
	/// Returns `None` if there is no child corresponding to token CREATE
	fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CREATE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LIKE
	/// Returns `None` if there is no child corresponding to token LIKE
	fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LIKE, 0)
	}
	fn tableIdentifier_all(&self) ->  Vec<Rc<TableIdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn tableIdentifier(&self, i: usize) -> Option<Rc<TableIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	fn tableProvider_all(&self) ->  Vec<Rc<TableProviderContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn tableProvider(&self, i: usize) -> Option<Rc<TableProviderContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn rowFormat_all(&self) ->  Vec<Rc<RowFormatContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn rowFormat(&self, i: usize) -> Option<Rc<RowFormatContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn createFileFormat_all(&self) ->  Vec<Rc<CreateFileFormatContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn createFileFormat(&self, i: usize) -> Option<Rc<CreateFileFormatContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn locationSpec_all(&self) ->  Vec<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn locationSpec(&self, i: usize) -> Option<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token TBLPROPERTIES in current rule
	fn TBLPROPERTIES_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token TBLPROPERTIES, starting from 0.
	/// Returns `None` if number of children corresponding to token TBLPROPERTIES is less or equal than `i`.
	fn TBLPROPERTIES(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TBLPROPERTIES, i)
	}
	fn tablePropertyList_all(&self) ->  Vec<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn tablePropertyList(&self, i: usize) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> CreateTableLikeContextAttrs<'input> for CreateTableLikeContext<'input>{}

pub struct CreateTableLikeContextExt<'input>{
	base:StatementContextExt<'input>,
	pub target: Option<Rc<TableIdentifierContextAll<'input>>>,
	pub source: Option<Rc<TableIdentifierContextAll<'input>>>,
	pub tableProps: Option<Rc<TablePropertyListContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{CreateTableLikeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CreateTableLikeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateTableLikeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_createTableLike(self);
	}
}

impl<'input> CustomRuleContext<'input> for CreateTableLikeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CreateTableLikeContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CreateTableLikeContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CreateTableLikeContext<'input> {}

impl<'input> CreateTableLikeContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CreateTableLikeContext(
				BaseParserRuleContext::copy_from(ctx,CreateTableLikeContextExt{
        			target:None, source:None, tableProps:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type UncacheTableContext<'input> = BaseParserRuleContext<'input,UncacheTableContextExt<'input>>;

pub trait UncacheTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token UNCACHE
	/// Returns `None` if there is no child corresponding to token UNCACHE
	fn UNCACHE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(UNCACHE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
}

impl<'input> UncacheTableContextAttrs<'input> for UncacheTableContext<'input>{}

pub struct UncacheTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{UncacheTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for UncacheTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for UncacheTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_uncacheTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for UncacheTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for UncacheTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for UncacheTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for UncacheTableContext<'input> {}

impl<'input> UncacheTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::UncacheTableContext(
				BaseParserRuleContext::copy_from(ctx,UncacheTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DropFunctionContext<'input> = BaseParserRuleContext<'input,DropFunctionContextExt<'input>>;

pub trait DropFunctionContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token DROP
	/// Returns `None` if there is no child corresponding to token DROP
	fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DROP, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FUNCTION
	/// Returns `None` if there is no child corresponding to token FUNCTION
	fn FUNCTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FUNCTION, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token TEMPORARY
	/// Returns `None` if there is no child corresponding to token TEMPORARY
	fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TEMPORARY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
}

impl<'input> DropFunctionContextAttrs<'input> for DropFunctionContext<'input>{}

pub struct DropFunctionContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DropFunctionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DropFunctionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DropFunctionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dropFunction(self);
	}
}

impl<'input> CustomRuleContext<'input> for DropFunctionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DropFunctionContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DropFunctionContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DropFunctionContext<'input> {}

impl<'input> DropFunctionContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DropFunctionContext(
				BaseParserRuleContext::copy_from(ctx,DropFunctionContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DescribeRelationContext<'input> = BaseParserRuleContext<'input,DescribeRelationContextExt<'input>>;

pub trait DescribeRelationContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token DESC
	/// Returns `None` if there is no child corresponding to token DESC
	fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESC, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DESCRIBE
	/// Returns `None` if there is no child corresponding to token DESCRIBE
	fn DESCRIBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESCRIBE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn describeColName(&self) -> Option<Rc<DescribeColNameContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token EXTENDED
	/// Returns `None` if there is no child corresponding to token EXTENDED
	fn EXTENDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXTENDED, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FORMATTED
	/// Returns `None` if there is no child corresponding to token FORMATTED
	fn FORMATTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FORMATTED, 0)
	}
}

impl<'input> DescribeRelationContextAttrs<'input> for DescribeRelationContext<'input>{}

pub struct DescribeRelationContextExt<'input>{
	base:StatementContextExt<'input>,
	pub option: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{DescribeRelationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DescribeRelationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DescribeRelationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_describeRelation(self);
	}
}

impl<'input> CustomRuleContext<'input> for DescribeRelationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DescribeRelationContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DescribeRelationContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DescribeRelationContext<'input> {}

impl<'input> DescribeRelationContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DescribeRelationContext(
				BaseParserRuleContext::copy_from(ctx,DescribeRelationContextExt{
					option:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type LoadDataContext<'input> = BaseParserRuleContext<'input,LoadDataContextExt<'input>>;

pub trait LoadDataContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token LOAD
	/// Returns `None` if there is no child corresponding to token LOAD
	fn LOAD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LOAD, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DATA
	/// Returns `None` if there is no child corresponding to token DATA
	fn DATA(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DATA, 0)
	}
	/// Retrieves first TerminalNode corresponding to token INPATH
	/// Returns `None` if there is no child corresponding to token INPATH
	fn INPATH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INPATH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token INTO
	/// Returns `None` if there is no child corresponding to token INTO
	fn INTO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTO, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LOCAL
	/// Returns `None` if there is no child corresponding to token LOCAL
	fn LOCAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LOCAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OVERWRITE
	/// Returns `None` if there is no child corresponding to token OVERWRITE
	fn OVERWRITE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OVERWRITE, 0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> LoadDataContextAttrs<'input> for LoadDataContext<'input>{}

pub struct LoadDataContextExt<'input>{
	base:StatementContextExt<'input>,
	pub path: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{LoadDataContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for LoadDataContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LoadDataContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_loadData(self);
	}
}

impl<'input> CustomRuleContext<'input> for LoadDataContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for LoadDataContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for LoadDataContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for LoadDataContext<'input> {}

impl<'input> LoadDataContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::LoadDataContext(
				BaseParserRuleContext::copy_from(ctx,LoadDataContextExt{
					path:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowPartitionsContext<'input> = BaseParserRuleContext<'input,ShowPartitionsContextExt<'input>>;

pub trait ShowPartitionsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PARTITIONS
	/// Returns `None` if there is no child corresponding to token PARTITIONS
	fn PARTITIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PARTITIONS, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ShowPartitionsContextAttrs<'input> for ShowPartitionsContext<'input>{}

pub struct ShowPartitionsContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowPartitionsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowPartitionsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowPartitionsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showPartitions(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowPartitionsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowPartitionsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowPartitionsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowPartitionsContext<'input> {}

impl<'input> ShowPartitionsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowPartitionsContext(
				BaseParserRuleContext::copy_from(ctx,ShowPartitionsContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DescribeFunctionContext<'input> = BaseParserRuleContext<'input,DescribeFunctionContextExt<'input>>;

pub trait DescribeFunctionContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token FUNCTION
	/// Returns `None` if there is no child corresponding to token FUNCTION
	fn FUNCTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FUNCTION, 0)
	}
	fn describeFuncName(&self) -> Option<Rc<DescribeFuncNameContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token DESC
	/// Returns `None` if there is no child corresponding to token DESC
	fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESC, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DESCRIBE
	/// Returns `None` if there is no child corresponding to token DESCRIBE
	fn DESCRIBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESCRIBE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXTENDED
	/// Returns `None` if there is no child corresponding to token EXTENDED
	fn EXTENDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXTENDED, 0)
	}
}

impl<'input> DescribeFunctionContextAttrs<'input> for DescribeFunctionContext<'input>{}

pub struct DescribeFunctionContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DescribeFunctionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DescribeFunctionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DescribeFunctionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_describeFunction(self);
	}
}

impl<'input> CustomRuleContext<'input> for DescribeFunctionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DescribeFunctionContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DescribeFunctionContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DescribeFunctionContext<'input> {}

impl<'input> DescribeFunctionContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DescribeFunctionContext(
				BaseParserRuleContext::copy_from(ctx,DescribeFunctionContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RenameTableColumnContext<'input> = BaseParserRuleContext<'input,RenameTableColumnContextExt<'input>>;

pub trait RenameTableColumnContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token RENAME
	/// Returns `None` if there is no child corresponding to token RENAME
	fn RENAME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(RENAME, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMN
	/// Returns `None` if there is no child corresponding to token COLUMN
	fn COLUMN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMN, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TO
	/// Returns `None` if there is no child corresponding to token TO
	fn TO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TO, 0)
	}
	fn multipartIdentifier_all(&self) ->  Vec<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn multipartIdentifier(&self, i: usize) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> RenameTableColumnContextAttrs<'input> for RenameTableColumnContext<'input>{}

pub struct RenameTableColumnContextExt<'input>{
	base:StatementContextExt<'input>,
	pub table: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub from: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub to: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{RenameTableColumnContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RenameTableColumnContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RenameTableColumnContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_renameTableColumn(self);
	}
}

impl<'input> CustomRuleContext<'input> for RenameTableColumnContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RenameTableColumnContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RenameTableColumnContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RenameTableColumnContext<'input> {}

impl<'input> RenameTableColumnContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RenameTableColumnContext(
				BaseParserRuleContext::copy_from(ctx,RenameTableColumnContextExt{
        			table:None, from:None, to:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type StatementDefaultContext<'input> = BaseParserRuleContext<'input,StatementDefaultContextExt<'input>>;

pub trait StatementDefaultContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> StatementDefaultContextAttrs<'input> for StatementDefaultContext<'input>{}

pub struct StatementDefaultContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{StatementDefaultContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for StatementDefaultContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StatementDefaultContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_statementDefault(self);
	}
}

impl<'input> CustomRuleContext<'input> for StatementDefaultContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for StatementDefaultContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for StatementDefaultContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for StatementDefaultContext<'input> {}

impl<'input> StatementDefaultContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::StatementDefaultContext(
				BaseParserRuleContext::copy_from(ctx,StatementDefaultContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type HiveChangeColumnContext<'input> = BaseParserRuleContext<'input,HiveChangeColumnContextExt<'input>>;

pub trait HiveChangeColumnContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CHANGE
	/// Returns `None` if there is no child corresponding to token CHANGE
	fn CHANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CHANGE, 0)
	}
	fn colType(&self) -> Option<Rc<ColTypeContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier_all(&self) ->  Vec<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn multipartIdentifier(&self, i: usize) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMN
	/// Returns `None` if there is no child corresponding to token COLUMN
	fn COLUMN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMN, 0)
	}
	fn colPosition(&self) -> Option<Rc<ColPositionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> HiveChangeColumnContextAttrs<'input> for HiveChangeColumnContext<'input>{}

pub struct HiveChangeColumnContextExt<'input>{
	base:StatementContextExt<'input>,
	pub table: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub colName: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{HiveChangeColumnContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for HiveChangeColumnContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for HiveChangeColumnContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_hiveChangeColumn(self);
	}
}

impl<'input> CustomRuleContext<'input> for HiveChangeColumnContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for HiveChangeColumnContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for HiveChangeColumnContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for HiveChangeColumnContext<'input> {}

impl<'input> HiveChangeColumnContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::HiveChangeColumnContext(
				BaseParserRuleContext::copy_from(ctx,HiveChangeColumnContextExt{
        			table:None, colName:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetTimeZoneContext<'input> = BaseParserRuleContext<'input,SetTimeZoneContextExt<'input>>;

pub trait SetTimeZoneContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TIME
	/// Returns `None` if there is no child corresponding to token TIME
	fn TIME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TIME, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ZONE
	/// Returns `None` if there is no child corresponding to token ZONE
	fn ZONE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ZONE, 0)
	}
	fn interval(&self) -> Option<Rc<IntervalContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LOCAL
	/// Returns `None` if there is no child corresponding to token LOCAL
	fn LOCAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LOCAL, 0)
	}
}

impl<'input> SetTimeZoneContextAttrs<'input> for SetTimeZoneContext<'input>{}

pub struct SetTimeZoneContextExt<'input>{
	base:StatementContextExt<'input>,
	pub timezone: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetTimeZoneContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetTimeZoneContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetTimeZoneContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setTimeZone(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetTimeZoneContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetTimeZoneContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetTimeZoneContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetTimeZoneContext<'input> {}

impl<'input> SetTimeZoneContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetTimeZoneContext(
				BaseParserRuleContext::copy_from(ctx,SetTimeZoneContextExt{
					timezone:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DescribeQueryContext<'input> = BaseParserRuleContext<'input,DescribeQueryContextExt<'input>>;

pub trait DescribeQueryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token DESC
	/// Returns `None` if there is no child corresponding to token DESC
	fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESC, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DESCRIBE
	/// Returns `None` if there is no child corresponding to token DESCRIBE
	fn DESCRIBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESCRIBE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token QUERY
	/// Returns `None` if there is no child corresponding to token QUERY
	fn QUERY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(QUERY, 0)
	}
}

impl<'input> DescribeQueryContextAttrs<'input> for DescribeQueryContext<'input>{}

pub struct DescribeQueryContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DescribeQueryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DescribeQueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DescribeQueryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_describeQuery(self);
	}
}

impl<'input> CustomRuleContext<'input> for DescribeQueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DescribeQueryContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DescribeQueryContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DescribeQueryContext<'input> {}

impl<'input> DescribeQueryContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DescribeQueryContext(
				BaseParserRuleContext::copy_from(ctx,DescribeQueryContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type TruncateTableContext<'input> = BaseParserRuleContext<'input,TruncateTableContextExt<'input>>;

pub trait TruncateTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token TRUNCATE
	/// Returns `None` if there is no child corresponding to token TRUNCATE
	fn TRUNCATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TRUNCATE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> TruncateTableContextAttrs<'input> for TruncateTableContext<'input>{}

pub struct TruncateTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{TruncateTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TruncateTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TruncateTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_truncateTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for TruncateTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for TruncateTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for TruncateTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for TruncateTableContext<'input> {}

impl<'input> TruncateTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::TruncateTableContext(
				BaseParserRuleContext::copy_from(ctx,TruncateTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetTableSerDeContext<'input> = BaseParserRuleContext<'input,SetTableSerDeContextExt<'input>>;

pub trait SetTableSerDeContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SERDE
	/// Returns `None` if there is no child corresponding to token SERDE
	fn SERDE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SERDE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token WITH
	/// Returns `None` if there is no child corresponding to token WITH
	fn WITH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(WITH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SERDEPROPERTIES
	/// Returns `None` if there is no child corresponding to token SERDEPROPERTIES
	fn SERDEPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SERDEPROPERTIES, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SetTableSerDeContextAttrs<'input> for SetTableSerDeContext<'input>{}

pub struct SetTableSerDeContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetTableSerDeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetTableSerDeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetTableSerDeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setTableSerDe(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetTableSerDeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetTableSerDeContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetTableSerDeContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetTableSerDeContext<'input> {}

impl<'input> SetTableSerDeContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetTableSerDeContext(
				BaseParserRuleContext::copy_from(ctx,SetTableSerDeContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CreateViewContext<'input> = BaseParserRuleContext<'input,CreateViewContextExt<'input>>;

pub trait CreateViewContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CREATE
	/// Returns `None` if there is no child corresponding to token CREATE
	fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CREATE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token OR
	/// Returns `None` if there is no child corresponding to token OR
	fn OR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OR, 0)
	}
	/// Retrieves first TerminalNode corresponding to token REPLACE
	/// Returns `None` if there is no child corresponding to token REPLACE
	fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REPLACE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TEMPORARY
	/// Returns `None` if there is no child corresponding to token TEMPORARY
	fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TEMPORARY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	fn identifierCommentList(&self) -> Option<Rc<IdentifierCommentListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn commentSpec_all(&self) ->  Vec<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn commentSpec(&self, i: usize) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token PARTITIONED in current rule
	fn PARTITIONED_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token PARTITIONED, starting from 0.
	/// Returns `None` if number of children corresponding to token PARTITIONED is less or equal than `i`.
	fn PARTITIONED(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PARTITIONED, i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token ON in current rule
	fn ON_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token ON, starting from 0.
	/// Returns `None` if number of children corresponding to token ON is less or equal than `i`.
	fn ON(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ON, i)
	}
	fn identifierList_all(&self) ->  Vec<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn identifierList(&self, i: usize) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token TBLPROPERTIES in current rule
	fn TBLPROPERTIES_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token TBLPROPERTIES, starting from 0.
	/// Returns `None` if number of children corresponding to token TBLPROPERTIES is less or equal than `i`.
	fn TBLPROPERTIES(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TBLPROPERTIES, i)
	}
	fn tablePropertyList_all(&self) ->  Vec<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn tablePropertyList(&self, i: usize) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token GLOBAL
	/// Returns `None` if there is no child corresponding to token GLOBAL
	fn GLOBAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(GLOBAL, 0)
	}
}

impl<'input> CreateViewContextAttrs<'input> for CreateViewContext<'input>{}

pub struct CreateViewContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{CreateViewContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CreateViewContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateViewContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_createView(self);
	}
}

impl<'input> CustomRuleContext<'input> for CreateViewContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CreateViewContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CreateViewContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CreateViewContext<'input> {}

impl<'input> CreateViewContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CreateViewContext(
				BaseParserRuleContext::copy_from(ctx,CreateViewContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DropTablePartitionsContext<'input> = BaseParserRuleContext<'input,DropTablePartitionsContextExt<'input>>;

pub trait DropTablePartitionsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token DROP
	/// Returns `None` if there is no child corresponding to token DROP
	fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DROP, 0)
	}
	fn partitionSpec_all(&self) ->  Vec<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn partitionSpec(&self, i: usize) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PURGE
	/// Returns `None` if there is no child corresponding to token PURGE
	fn PURGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PURGE, 0)
	}
}

impl<'input> DropTablePartitionsContextAttrs<'input> for DropTablePartitionsContext<'input>{}

pub struct DropTablePartitionsContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DropTablePartitionsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DropTablePartitionsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DropTablePartitionsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dropTablePartitions(self);
	}
}

impl<'input> CustomRuleContext<'input> for DropTablePartitionsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DropTablePartitionsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DropTablePartitionsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DropTablePartitionsContext<'input> {}

impl<'input> DropTablePartitionsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DropTablePartitionsContext(
				BaseParserRuleContext::copy_from(ctx,DropTablePartitionsContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetConfigurationContext<'input> = BaseParserRuleContext<'input,SetConfigurationContextExt<'input>>;

pub trait SetConfigurationContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
}

impl<'input> SetConfigurationContextAttrs<'input> for SetConfigurationContext<'input>{}

pub struct SetConfigurationContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetConfigurationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetConfigurationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetConfigurationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setConfiguration(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetConfigurationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetConfigurationContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetConfigurationContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetConfigurationContext<'input> {}

impl<'input> SetConfigurationContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetConfigurationContext(
				BaseParserRuleContext::copy_from(ctx,SetConfigurationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DropTableContext<'input> = BaseParserRuleContext<'input,DropTableContextExt<'input>>;

pub trait DropTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token DROP
	/// Returns `None` if there is no child corresponding to token DROP
	fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DROP, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PURGE
	/// Returns `None` if there is no child corresponding to token PURGE
	fn PURGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PURGE, 0)
	}
}

impl<'input> DropTableContextAttrs<'input> for DropTableContext<'input>{}

pub struct DropTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DropTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DropTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DropTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dropTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for DropTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DropTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DropTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DropTableContext<'input> {}

impl<'input> DropTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DropTableContext(
				BaseParserRuleContext::copy_from(ctx,DropTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DescribeNamespaceContext<'input> = BaseParserRuleContext<'input,DescribeNamespaceContextExt<'input>>;

pub trait DescribeNamespaceContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn namespace(&self) -> Option<Rc<NamespaceContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token DESC
	/// Returns `None` if there is no child corresponding to token DESC
	fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESC, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DESCRIBE
	/// Returns `None` if there is no child corresponding to token DESCRIBE
	fn DESCRIBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DESCRIBE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXTENDED
	/// Returns `None` if there is no child corresponding to token EXTENDED
	fn EXTENDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXTENDED, 0)
	}
}

impl<'input> DescribeNamespaceContextAttrs<'input> for DescribeNamespaceContext<'input>{}

pub struct DescribeNamespaceContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DescribeNamespaceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DescribeNamespaceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DescribeNamespaceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_describeNamespace(self);
	}
}

impl<'input> CustomRuleContext<'input> for DescribeNamespaceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DescribeNamespaceContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DescribeNamespaceContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DescribeNamespaceContext<'input> {}

impl<'input> DescribeNamespaceContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DescribeNamespaceContext(
				BaseParserRuleContext::copy_from(ctx,DescribeNamespaceContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type AlterTableAlterColumnContext<'input> = BaseParserRuleContext<'input,AlterTableAlterColumnContextExt<'input>>;

pub trait AlterTableAlterColumnContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves all `TerminalNode`s corresponding to token ALTER in current rule
	fn ALTER_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token ALTER, starting from 0.
	/// Returns `None` if number of children corresponding to token ALTER is less or equal than `i`.
	fn ALTER(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, i)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier_all(&self) ->  Vec<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn multipartIdentifier(&self, i: usize) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token CHANGE
	/// Returns `None` if there is no child corresponding to token CHANGE
	fn CHANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CHANGE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMN
	/// Returns `None` if there is no child corresponding to token COLUMN
	fn COLUMN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMN, 0)
	}
	fn alterColumnAction(&self) -> Option<Rc<AlterColumnActionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> AlterTableAlterColumnContextAttrs<'input> for AlterTableAlterColumnContext<'input>{}

pub struct AlterTableAlterColumnContextExt<'input>{
	base:StatementContextExt<'input>,
	pub table: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub column: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{AlterTableAlterColumnContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for AlterTableAlterColumnContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AlterTableAlterColumnContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_alterTableAlterColumn(self);
	}
}

impl<'input> CustomRuleContext<'input> for AlterTableAlterColumnContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for AlterTableAlterColumnContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for AlterTableAlterColumnContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for AlterTableAlterColumnContext<'input> {}

impl<'input> AlterTableAlterColumnContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::AlterTableAlterColumnContext(
				BaseParserRuleContext::copy_from(ctx,AlterTableAlterColumnContextExt{
        			table:None, column:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RefreshFunctionContext<'input> = BaseParserRuleContext<'input,RefreshFunctionContextExt<'input>>;

pub trait RefreshFunctionContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token REFRESH
	/// Returns `None` if there is no child corresponding to token REFRESH
	fn REFRESH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(REFRESH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FUNCTION
	/// Returns `None` if there is no child corresponding to token FUNCTION
	fn FUNCTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FUNCTION, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> RefreshFunctionContextAttrs<'input> for RefreshFunctionContext<'input>{}

pub struct RefreshFunctionContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RefreshFunctionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RefreshFunctionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RefreshFunctionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_refreshFunction(self);
	}
}

impl<'input> CustomRuleContext<'input> for RefreshFunctionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for RefreshFunctionContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for RefreshFunctionContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for RefreshFunctionContext<'input> {}

impl<'input> RefreshFunctionContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::RefreshFunctionContext(
				BaseParserRuleContext::copy_from(ctx,RefreshFunctionContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CommentTableContext<'input> = BaseParserRuleContext<'input,CommentTableContextExt<'input>>;

pub trait CommentTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token COMMENT
	/// Returns `None` if there is no child corresponding to token COMMENT
	fn COMMENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COMMENT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ON
	/// Returns `None` if there is no child corresponding to token ON
	fn ON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ON, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IS
	/// Returns `None` if there is no child corresponding to token IS
	fn IS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NULL
	/// Returns `None` if there is no child corresponding to token NULL
	fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NULL, 0)
	}
}

impl<'input> CommentTableContextAttrs<'input> for CommentTableContext<'input>{}

pub struct CommentTableContextExt<'input>{
	base:StatementContextExt<'input>,
	pub comment: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{CommentTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CommentTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CommentTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_commentTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for CommentTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CommentTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CommentTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CommentTableContext<'input> {}

impl<'input> CommentTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CommentTableContext(
				BaseParserRuleContext::copy_from(ctx,CommentTableContextExt{
					comment:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CreateNamespaceContext<'input> = BaseParserRuleContext<'input,CreateNamespaceContextExt<'input>>;

pub trait CreateNamespaceContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CREATE
	/// Returns `None` if there is no child corresponding to token CREATE
	fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CREATE, 0)
	}
	fn namespace(&self) -> Option<Rc<NamespaceContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	fn commentSpec_all(&self) ->  Vec<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn commentSpec(&self, i: usize) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn locationSpec_all(&self) ->  Vec<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn locationSpec(&self, i: usize) -> Option<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token WITH in current rule
	fn WITH_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token WITH, starting from 0.
	/// Returns `None` if number of children corresponding to token WITH is less or equal than `i`.
	fn WITH(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(WITH, i)
	}
	fn tablePropertyList_all(&self) ->  Vec<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn tablePropertyList(&self, i: usize) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token DBPROPERTIES in current rule
	fn DBPROPERTIES_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token DBPROPERTIES, starting from 0.
	/// Returns `None` if number of children corresponding to token DBPROPERTIES is less or equal than `i`.
	fn DBPROPERTIES(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DBPROPERTIES, i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token PROPERTIES in current rule
	fn PROPERTIES_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token PROPERTIES, starting from 0.
	/// Returns `None` if number of children corresponding to token PROPERTIES is less or equal than `i`.
	fn PROPERTIES(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PROPERTIES, i)
	}
}

impl<'input> CreateNamespaceContextAttrs<'input> for CreateNamespaceContext<'input>{}

pub struct CreateNamespaceContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{CreateNamespaceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CreateNamespaceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateNamespaceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_createNamespace(self);
	}
}

impl<'input> CustomRuleContext<'input> for CreateNamespaceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CreateNamespaceContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CreateNamespaceContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CreateNamespaceContext<'input> {}

impl<'input> CreateNamespaceContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CreateNamespaceContext(
				BaseParserRuleContext::copy_from(ctx,CreateNamespaceContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowTblPropertiesContext<'input> = BaseParserRuleContext<'input,ShowTblPropertiesContextExt<'input>>;

pub trait ShowTblPropertiesContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TBLPROPERTIES
	/// Returns `None` if there is no child corresponding to token TBLPROPERTIES
	fn TBLPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TBLPROPERTIES, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tablePropertyKey(&self) -> Option<Rc<TablePropertyKeyContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ShowTblPropertiesContextAttrs<'input> for ShowTblPropertiesContext<'input>{}

pub struct ShowTblPropertiesContextExt<'input>{
	base:StatementContextExt<'input>,
	pub table: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub key: Option<Rc<TablePropertyKeyContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowTblPropertiesContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowTblPropertiesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowTblPropertiesContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showTblProperties(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowTblPropertiesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowTblPropertiesContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowTblPropertiesContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowTblPropertiesContext<'input> {}

impl<'input> ShowTblPropertiesContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowTblPropertiesContext(
				BaseParserRuleContext::copy_from(ctx,ShowTblPropertiesContextExt{
        			table:None, key:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type UnsetTablePropertiesContext<'input> = BaseParserRuleContext<'input,UnsetTablePropertiesContextExt<'input>>;

pub trait UnsetTablePropertiesContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token UNSET
	/// Returns `None` if there is no child corresponding to token UNSET
	fn UNSET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(UNSET, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TBLPROPERTIES
	/// Returns `None` if there is no child corresponding to token TBLPROPERTIES
	fn TBLPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TBLPROPERTIES, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
}

impl<'input> UnsetTablePropertiesContextAttrs<'input> for UnsetTablePropertiesContext<'input>{}

pub struct UnsetTablePropertiesContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{UnsetTablePropertiesContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for UnsetTablePropertiesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for UnsetTablePropertiesContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_unsetTableProperties(self);
	}
}

impl<'input> CustomRuleContext<'input> for UnsetTablePropertiesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for UnsetTablePropertiesContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for UnsetTablePropertiesContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for UnsetTablePropertiesContext<'input> {}

impl<'input> UnsetTablePropertiesContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::UnsetTablePropertiesContext(
				BaseParserRuleContext::copy_from(ctx,UnsetTablePropertiesContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetTableLocationContext<'input> = BaseParserRuleContext<'input,SetTableLocationContextExt<'input>>;

pub trait SetTableLocationContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	fn locationSpec(&self) -> Option<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SetTableLocationContextAttrs<'input> for SetTableLocationContext<'input>{}

pub struct SetTableLocationContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetTableLocationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetTableLocationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetTableLocationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setTableLocation(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetTableLocationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetTableLocationContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetTableLocationContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetTableLocationContext<'input> {}

impl<'input> SetTableLocationContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetTableLocationContext(
				BaseParserRuleContext::copy_from(ctx,SetTableLocationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DropTableColumnsContext<'input> = BaseParserRuleContext<'input,DropTableColumnsContextExt<'input>>;

pub trait DropTableColumnsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token DROP
	/// Returns `None` if there is no child corresponding to token DROP
	fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DROP, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMN
	/// Returns `None` if there is no child corresponding to token COLUMN
	fn COLUMN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMN, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMNS
	/// Returns `None` if there is no child corresponding to token COLUMNS
	fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMNS, 0)
	}
	fn multipartIdentifierList(&self) -> Option<Rc<MultipartIdentifierListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> DropTableColumnsContextAttrs<'input> for DropTableColumnsContext<'input>{}

pub struct DropTableColumnsContextExt<'input>{
	base:StatementContextExt<'input>,
	pub columns: Option<Rc<MultipartIdentifierListContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{DropTableColumnsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DropTableColumnsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DropTableColumnsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dropTableColumns(self);
	}
}

impl<'input> CustomRuleContext<'input> for DropTableColumnsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for DropTableColumnsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for DropTableColumnsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for DropTableColumnsContext<'input> {}

impl<'input> DropTableColumnsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::DropTableColumnsContext(
				BaseParserRuleContext::copy_from(ctx,DropTableColumnsContextExt{
        			columns:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowViewsContext<'input> = BaseParserRuleContext<'input,ShowViewsContextExt<'input>>;

pub trait ShowViewsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEWS
	/// Returns `None` if there is no child corresponding to token VIEWS
	fn VIEWS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEWS, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IN
	/// Returns `None` if there is no child corresponding to token IN
	fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IN, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LIKE
	/// Returns `None` if there is no child corresponding to token LIKE
	fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LIKE, 0)
	}
}

impl<'input> ShowViewsContextAttrs<'input> for ShowViewsContext<'input>{}

pub struct ShowViewsContextExt<'input>{
	base:StatementContextExt<'input>,
	pub pattern: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowViewsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowViewsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowViewsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showViews(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowViewsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowViewsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowViewsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowViewsContext<'input> {}

impl<'input> ShowViewsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowViewsContext(
				BaseParserRuleContext::copy_from(ctx,ShowViewsContextExt{
					pattern:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ShowFunctionsContext<'input> = BaseParserRuleContext<'input,ShowFunctionsContextExt<'input>>;

pub trait ShowFunctionsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SHOW
	/// Returns `None` if there is no child corresponding to token SHOW
	fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SHOW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FUNCTIONS
	/// Returns `None` if there is no child corresponding to token FUNCTIONS
	fn FUNCTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FUNCTIONS, 0)
	}
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token LIKE
	/// Returns `None` if there is no child corresponding to token LIKE
	fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LIKE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
}

impl<'input> ShowFunctionsContextAttrs<'input> for ShowFunctionsContext<'input>{}

pub struct ShowFunctionsContextExt<'input>{
	base:StatementContextExt<'input>,
	pub pattern: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ShowFunctionsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ShowFunctionsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ShowFunctionsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_showFunctions(self);
	}
}

impl<'input> CustomRuleContext<'input> for ShowFunctionsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for ShowFunctionsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for ShowFunctionsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for ShowFunctionsContext<'input> {}

impl<'input> ShowFunctionsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::ShowFunctionsContext(
				BaseParserRuleContext::copy_from(ctx,ShowFunctionsContextExt{
					pattern:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CacheTableContext<'input> = BaseParserRuleContext<'input,CacheTableContextExt<'input>>;

pub trait CacheTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CACHE
	/// Returns `None` if there is no child corresponding to token CACHE
	fn CACHE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CACHE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token LAZY
	/// Returns `None` if there is no child corresponding to token LAZY
	fn LAZY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LAZY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OPTIONS
	/// Returns `None` if there is no child corresponding to token OPTIONS
	fn OPTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OPTIONS, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
}

impl<'input> CacheTableContextAttrs<'input> for CacheTableContext<'input>{}

pub struct CacheTableContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{CacheTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CacheTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CacheTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_cacheTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for CacheTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for CacheTableContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for CacheTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for CacheTableContext<'input> {}

impl<'input> CacheTableContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::CacheTableContext(
				BaseParserRuleContext::copy_from(ctx,CacheTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type AddTableColumnsContext<'input> = BaseParserRuleContext<'input,AddTableColumnsContextExt<'input>>;

pub trait AddTableColumnsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token ADD
	/// Returns `None` if there is no child corresponding to token ADD
	fn ADD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ADD, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMN
	/// Returns `None` if there is no child corresponding to token COLUMN
	fn COLUMN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMN, 0)
	}
	/// Retrieves first TerminalNode corresponding to token COLUMNS
	/// Returns `None` if there is no child corresponding to token COLUMNS
	fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLUMNS, 0)
	}
	fn qualifiedColTypeWithPositionList(&self) -> Option<Rc<QualifiedColTypeWithPositionListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> AddTableColumnsContextAttrs<'input> for AddTableColumnsContext<'input>{}

pub struct AddTableColumnsContextExt<'input>{
	base:StatementContextExt<'input>,
	pub columns: Option<Rc<QualifiedColTypeWithPositionListContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{AddTableColumnsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for AddTableColumnsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AddTableColumnsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_addTableColumns(self);
	}
}

impl<'input> CustomRuleContext<'input> for AddTableColumnsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for AddTableColumnsContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for AddTableColumnsContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for AddTableColumnsContext<'input> {}

impl<'input> AddTableColumnsContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::AddTableColumnsContext(
				BaseParserRuleContext::copy_from(ctx,AddTableColumnsContextExt{
        			columns:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetTablePropertiesContext<'input> = BaseParserRuleContext<'input,SetTablePropertiesContextExt<'input>>;

pub trait SetTablePropertiesContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ALTER
	/// Returns `None` if there is no child corresponding to token ALTER
	fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ALTER, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token SET
	/// Returns `None` if there is no child corresponding to token SET
	fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SET, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TBLPROPERTIES
	/// Returns `None` if there is no child corresponding to token TBLPROPERTIES
	fn TBLPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TBLPROPERTIES, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token VIEW
	/// Returns `None` if there is no child corresponding to token VIEW
	fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(VIEW, 0)
	}
}

impl<'input> SetTablePropertiesContextAttrs<'input> for SetTablePropertiesContext<'input>{}

pub struct SetTablePropertiesContextExt<'input>{
	base:StatementContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetTablePropertiesContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetTablePropertiesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetTablePropertiesContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setTableProperties(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetTablePropertiesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_statement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_statement }
}

impl<'input> Borrow<StatementContextExt<'input>> for SetTablePropertiesContext<'input>{
	fn borrow(&self) -> &StatementContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StatementContextExt<'input>> for SetTablePropertiesContext<'input>{
	fn borrow_mut(&mut self) -> &mut StatementContextExt<'input> { &mut self.base }
}

impl<'input> StatementContextAttrs<'input> for SetTablePropertiesContext<'input> {}

impl<'input> SetTablePropertiesContextExt<'input>{
	fn new(ctx: &dyn StatementContextAttrs<'input>) -> Rc<StatementContextAll<'input>>  {
		Rc::new(
			StatementContextAll::SetTablePropertiesContext(
				BaseParserRuleContext::copy_from(ctx,SetTablePropertiesContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn statement(&mut self,)
	-> Result<Rc<StatementContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = StatementContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 16, RULE_statement);
        let mut _localctx: Rc<StatementContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			recog.base.set_state(1048);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(112,&mut recog.base)? {
				1 =>{
					let tmp = StatementDefaultContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					/*InvokeRule query*/
					recog.base.set_state(304);
					recog.query()?;

					}
				}
			,
				2 =>{
					let tmp = DmlStatementContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					recog.base.set_state(306);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==WITH {
						{
						/*InvokeRule ctes*/
						recog.base.set_state(305);
						recog.ctes()?;

						}
					}

					/*InvokeRule dmlStatementNoWith*/
					recog.base.set_state(308);
					recog.dmlStatementNoWith()?;

					}
				}
			,
				3 =>{
					let tmp = UseContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(309);
					recog.base.match_token(USE,&mut recog.err_handler)?;

					recog.base.set_state(311);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(4,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(310);
							recog.base.match_token(NAMESPACE,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(313);
					recog.multipartIdentifier()?;

					}
				}
			,
				4 =>{
					let tmp = CreateNamespaceContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					recog.base.set_state(314);
					recog.base.match_token(CREATE,&mut recog.err_handler)?;

					/*InvokeRule namespace*/
					recog.base.set_state(315);
					recog.namespace()?;

					recog.base.set_state(319);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(5,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(316);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(317);
							recog.base.match_token(NOT,&mut recog.err_handler)?;

							recog.base.set_state(318);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(321);
					recog.multipartIdentifier()?;

					recog.base.set_state(329);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(7,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							recog.base.set_state(327);
							recog.err_handler.sync(&mut recog.base)?;
							match recog.base.input.la(1) {
							 COMMENT 
								=> {
									{
									/*InvokeRule commentSpec*/
									recog.base.set_state(322);
									recog.commentSpec()?;

									}
								}

							 LOCATION 
								=> {
									{
									/*InvokeRule locationSpec*/
									recog.base.set_state(323);
									recog.locationSpec()?;

									}
								}

							 WITH 
								=> {
									{
									{
									recog.base.set_state(324);
									recog.base.match_token(WITH,&mut recog.err_handler)?;

									recog.base.set_state(325);
									_la = recog.base.input.la(1);
									if { !(_la==DBPROPERTIES || _la==PROPERTIES) } {
										recog.err_handler.recover_inline(&mut recog.base)?;

									}
									else {
										if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
										recog.err_handler.report_match(&mut recog.base);
										recog.base.consume(&mut recog.err_handler);
									}
									/*InvokeRule tablePropertyList*/
									recog.base.set_state(326);
									recog.tablePropertyList()?;

									}
									}
								}

								_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
							}
							} 
						}
						recog.base.set_state(331);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(7,&mut recog.base)?;
					}
					}
				}
			,
				5 =>{
					let tmp = SetNamespacePropertiesContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 5);
					_localctx = tmp;
					{
					recog.base.set_state(332);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					/*InvokeRule namespace*/
					recog.base.set_state(333);
					recog.namespace()?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(334);
					recog.multipartIdentifier()?;

					recog.base.set_state(335);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(336);
					_la = recog.base.input.la(1);
					if { !(_la==DBPROPERTIES || _la==PROPERTIES) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule tablePropertyList*/
					recog.base.set_state(337);
					recog.tablePropertyList()?;

					}
				}
			,
				6 =>{
					let tmp = SetNamespaceLocationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 6);
					_localctx = tmp;
					{
					recog.base.set_state(339);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					/*InvokeRule namespace*/
					recog.base.set_state(340);
					recog.namespace()?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(341);
					recog.multipartIdentifier()?;

					recog.base.set_state(342);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					/*InvokeRule locationSpec*/
					recog.base.set_state(343);
					recog.locationSpec()?;

					}
				}
			,
				7 =>{
					let tmp = DropNamespaceContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 7);
					_localctx = tmp;
					{
					recog.base.set_state(345);
					recog.base.match_token(DROP,&mut recog.err_handler)?;

					/*InvokeRule namespace*/
					recog.base.set_state(346);
					recog.namespace()?;

					recog.base.set_state(349);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(8,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(347);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(348);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(351);
					recog.multipartIdentifier()?;

					recog.base.set_state(353);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==CASCADE || _la==RESTRICT {
						{
						recog.base.set_state(352);
						_la = recog.base.input.la(1);
						if { !(_la==CASCADE || _la==RESTRICT) } {
							recog.err_handler.recover_inline(&mut recog.base)?;

						}
						else {
							if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
							recog.err_handler.report_match(&mut recog.base);
							recog.base.consume(&mut recog.err_handler);
						}
						}
					}

					}
				}
			,
				8 =>{
					let tmp = ShowNamespacesContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 8);
					_localctx = tmp;
					{
					recog.base.set_state(355);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(356);
					_la = recog.base.input.la(1);
					if { !(_la==DATABASES || _la==NAMESPACES) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(359);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(10,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(357);
							_la = recog.base.input.la(1);
							if { !(_la==FROM || _la==IN) } {
								recog.err_handler.recover_inline(&mut recog.base)?;

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							/*InvokeRule multipartIdentifier*/
							recog.base.set_state(358);
							recog.multipartIdentifier()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(365);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LIKE || _la==STRING {
						{
						recog.base.set_state(362);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if _la==LIKE {
							{
							recog.base.set_state(361);
							recog.base.match_token(LIKE,&mut recog.err_handler)?;

							}
						}

						recog.base.set_state(364);
						let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
						if let StatementContextAll::ShowNamespacesContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.pattern = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					}
				}
			,
				9 =>{
					let tmp = CreateTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 9);
					_localctx = tmp;
					{
					/*InvokeRule createTableHeader*/
					recog.base.set_state(367);
					recog.createTableHeader()?;

					recog.base.set_state(372);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==T__0 {
						{
						recog.base.set_state(368);
						recog.base.match_token(T__0,&mut recog.err_handler)?;

						/*InvokeRule colTypeList*/
						recog.base.set_state(369);
						recog.colTypeList()?;

						recog.base.set_state(370);
						recog.base.match_token(T__1,&mut recog.err_handler)?;

						}
					}

					/*InvokeRule tableProvider*/
					recog.base.set_state(374);
					recog.tableProvider()?;

					/*InvokeRule createTableClauses*/
					recog.base.set_state(375);
					recog.createTableClauses()?;

					recog.base.set_state(380);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(15,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(377);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==AS {
								{
								recog.base.set_state(376);
								recog.base.match_token(AS,&mut recog.err_handler)?;

								}
							}

							/*InvokeRule query*/
							recog.base.set_state(379);
							recog.query()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				10 =>{
					let tmp = CreateHiveTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 10);
					_localctx = tmp;
					{
					/*InvokeRule createTableHeader*/
					recog.base.set_state(382);
					recog.createTableHeader()?;

					recog.base.set_state(387);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(16,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(383);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							/*InvokeRule colTypeList*/
							recog.base.set_state(384);
							let tmp = recog.colTypeList()?;
							if let StatementContextAll::CreateHiveTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
							ctx.columns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							recog.base.set_state(385);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					recog.base.set_state(410);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(19,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							recog.base.set_state(408);
							recog.err_handler.sync(&mut recog.base)?;
							match recog.base.input.la(1) {
							 COMMENT 
								=> {
									{
									/*InvokeRule commentSpec*/
									recog.base.set_state(389);
									recog.commentSpec()?;

									}
								}

							 PARTITIONED 
								=> {
									{
									recog.base.set_state(399);
									recog.err_handler.sync(&mut recog.base)?;
									match  recog.interpreter.adaptive_predict(17,&mut recog.base)? {
										1 =>{
											{
											recog.base.set_state(390);
											recog.base.match_token(PARTITIONED,&mut recog.err_handler)?;

											recog.base.set_state(391);
											recog.base.match_token(BY,&mut recog.err_handler)?;

											recog.base.set_state(392);
											recog.base.match_token(T__0,&mut recog.err_handler)?;

											/*InvokeRule colTypeList*/
											recog.base.set_state(393);
											let tmp = recog.colTypeList()?;
											if let StatementContextAll::CreateHiveTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
											ctx.partitionColumns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

											recog.base.set_state(394);
											recog.base.match_token(T__1,&mut recog.err_handler)?;

											}
										}
									,
										2 =>{
											{
											recog.base.set_state(396);
											recog.base.match_token(PARTITIONED,&mut recog.err_handler)?;

											recog.base.set_state(397);
											recog.base.match_token(BY,&mut recog.err_handler)?;

											/*InvokeRule identifierList*/
											recog.base.set_state(398);
											let tmp = recog.identifierList()?;
											if let StatementContextAll::CreateHiveTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
											ctx.partitionColumnNames = Some(tmp.clone()); } else {unreachable!("cant cast");}  

											}
										}

										_ => {}
									}
									}
								}

							 CLUSTERED 
								=> {
									{
									/*InvokeRule bucketSpec*/
									recog.base.set_state(401);
									recog.bucketSpec()?;

									}
								}

							 SKEWED 
								=> {
									{
									/*InvokeRule skewSpec*/
									recog.base.set_state(402);
									recog.skewSpec()?;

									}
								}

							 ROW 
								=> {
									{
									/*InvokeRule rowFormat*/
									recog.base.set_state(403);
									recog.rowFormat()?;

									}
								}

							 STORED 
								=> {
									{
									/*InvokeRule createFileFormat*/
									recog.base.set_state(404);
									recog.createFileFormat()?;

									}
								}

							 LOCATION 
								=> {
									{
									/*InvokeRule locationSpec*/
									recog.base.set_state(405);
									recog.locationSpec()?;

									}
								}

							 TBLPROPERTIES 
								=> {
									{
									{
									recog.base.set_state(406);
									recog.base.match_token(TBLPROPERTIES,&mut recog.err_handler)?;

									/*InvokeRule tablePropertyList*/
									recog.base.set_state(407);
									let tmp = recog.tablePropertyList()?;
									if let StatementContextAll::CreateHiveTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
									ctx.tableProps = Some(tmp.clone()); } else {unreachable!("cant cast");}  

									}
									}
								}

								_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
							}
							} 
						}
						recog.base.set_state(412);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(19,&mut recog.base)?;
					}
					recog.base.set_state(417);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(21,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(414);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==AS {
								{
								recog.base.set_state(413);
								recog.base.match_token(AS,&mut recog.err_handler)?;

								}
							}

							/*InvokeRule query*/
							recog.base.set_state(416);
							recog.query()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				11 =>{
					let tmp = CreateTableLikeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 11);
					_localctx = tmp;
					{
					recog.base.set_state(419);
					recog.base.match_token(CREATE,&mut recog.err_handler)?;

					recog.base.set_state(420);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					recog.base.set_state(424);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(22,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(421);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(422);
							recog.base.match_token(NOT,&mut recog.err_handler)?;

							recog.base.set_state(423);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule tableIdentifier*/
					recog.base.set_state(426);
					let tmp = recog.tableIdentifier()?;
					if let StatementContextAll::CreateTableLikeContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.target = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(427);
					recog.base.match_token(LIKE,&mut recog.err_handler)?;

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(428);
					let tmp = recog.tableIdentifier()?;
					if let StatementContextAll::CreateTableLikeContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.source = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(437);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==LOCATION || ((((_la - 198)) & !0x3f) == 0 && ((1usize << (_la - 198)) & ((1usize << (ROW - 198)) | (1usize << (STORED - 198)) | (1usize << (TBLPROPERTIES - 198)))) != 0) || _la==USING {
						{
						recog.base.set_state(435);
						recog.err_handler.sync(&mut recog.base)?;
						match recog.base.input.la(1) {
						 USING 
							=> {
								{
								/*InvokeRule tableProvider*/
								recog.base.set_state(429);
								recog.tableProvider()?;

								}
							}

						 ROW 
							=> {
								{
								/*InvokeRule rowFormat*/
								recog.base.set_state(430);
								recog.rowFormat()?;

								}
							}

						 STORED 
							=> {
								{
								/*InvokeRule createFileFormat*/
								recog.base.set_state(431);
								recog.createFileFormat()?;

								}
							}

						 LOCATION 
							=> {
								{
								/*InvokeRule locationSpec*/
								recog.base.set_state(432);
								recog.locationSpec()?;

								}
							}

						 TBLPROPERTIES 
							=> {
								{
								{
								recog.base.set_state(433);
								recog.base.match_token(TBLPROPERTIES,&mut recog.err_handler)?;

								/*InvokeRule tablePropertyList*/
								recog.base.set_state(434);
								let tmp = recog.tablePropertyList()?;
								if let StatementContextAll::CreateTableLikeContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
								ctx.tableProps = Some(tmp.clone()); } else {unreachable!("cant cast");}  

								}
								}
							}

							_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
						}
						}
						recog.base.set_state(439);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					}
				}
			,
				12 =>{
					let tmp = ReplaceTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 12);
					_localctx = tmp;
					{
					/*InvokeRule replaceTableHeader*/
					recog.base.set_state(440);
					recog.replaceTableHeader()?;

					recog.base.set_state(445);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==T__0 {
						{
						recog.base.set_state(441);
						recog.base.match_token(T__0,&mut recog.err_handler)?;

						/*InvokeRule colTypeList*/
						recog.base.set_state(442);
						recog.colTypeList()?;

						recog.base.set_state(443);
						recog.base.match_token(T__1,&mut recog.err_handler)?;

						}
					}

					/*InvokeRule tableProvider*/
					recog.base.set_state(447);
					recog.tableProvider()?;

					/*InvokeRule createTableClauses*/
					recog.base.set_state(448);
					recog.createTableClauses()?;

					recog.base.set_state(453);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(27,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(450);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==AS {
								{
								recog.base.set_state(449);
								recog.base.match_token(AS,&mut recog.err_handler)?;

								}
							}

							/*InvokeRule query*/
							recog.base.set_state(452);
							recog.query()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				13 =>{
					let tmp = AnalyzeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 13);
					_localctx = tmp;
					{
					recog.base.set_state(455);
					recog.base.match_token(ANALYZE,&mut recog.err_handler)?;

					recog.base.set_state(456);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(457);
					recog.multipartIdentifier()?;

					recog.base.set_state(459);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(458);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(461);
					recog.base.match_token(COMPUTE,&mut recog.err_handler)?;

					recog.base.set_state(462);
					recog.base.match_token(STATISTICS,&mut recog.err_handler)?;

					recog.base.set_state(470);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(29,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule identifier*/
							recog.base.set_state(463);
							recog.identifier()?;

							}
						}

						x if x == 2=>{
							{
							recog.base.set_state(464);
							recog.base.match_token(FOR,&mut recog.err_handler)?;

							recog.base.set_state(465);
							recog.base.match_token(COLUMNS,&mut recog.err_handler)?;

							/*InvokeRule identifierSeq*/
							recog.base.set_state(466);
							recog.identifierSeq()?;

							}
						}

						x if x == 3=>{
							{
							recog.base.set_state(467);
							recog.base.match_token(FOR,&mut recog.err_handler)?;

							recog.base.set_state(468);
							recog.base.match_token(ALL,&mut recog.err_handler)?;

							recog.base.set_state(469);
							recog.base.match_token(COLUMNS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					}
				}
			,
				14 =>{
					let tmp = AddTableColumnsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 14);
					_localctx = tmp;
					{
					recog.base.set_state(472);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(473);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(474);
					recog.multipartIdentifier()?;

					recog.base.set_state(475);
					recog.base.match_token(ADD,&mut recog.err_handler)?;

					recog.base.set_state(476);
					_la = recog.base.input.la(1);
					if { !(_la==COLUMN || _la==COLUMNS) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule qualifiedColTypeWithPositionList*/
					recog.base.set_state(477);
					let tmp = recog.qualifiedColTypeWithPositionList()?;
					if let StatementContextAll::AddTableColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.columns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}
			,
				15 =>{
					let tmp = AddTableColumnsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 15);
					_localctx = tmp;
					{
					recog.base.set_state(479);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(480);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(481);
					recog.multipartIdentifier()?;

					recog.base.set_state(482);
					recog.base.match_token(ADD,&mut recog.err_handler)?;

					recog.base.set_state(483);
					_la = recog.base.input.la(1);
					if { !(_la==COLUMN || _la==COLUMNS) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(484);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule qualifiedColTypeWithPositionList*/
					recog.base.set_state(485);
					let tmp = recog.qualifiedColTypeWithPositionList()?;
					if let StatementContextAll::AddTableColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.columns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(486);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				16 =>{
					let tmp = RenameTableColumnContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 16);
					_localctx = tmp;
					{
					recog.base.set_state(488);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(489);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(490);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::RenameTableColumnContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.table = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(491);
					recog.base.match_token(RENAME,&mut recog.err_handler)?;

					recog.base.set_state(492);
					recog.base.match_token(COLUMN,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(493);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::RenameTableColumnContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.from = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(494);
					recog.base.match_token(TO,&mut recog.err_handler)?;

					/*InvokeRule errorCapturingIdentifier*/
					recog.base.set_state(495);
					let tmp = recog.errorCapturingIdentifier()?;
					if let StatementContextAll::RenameTableColumnContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.to = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}
			,
				17 =>{
					let tmp = DropTableColumnsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 17);
					_localctx = tmp;
					{
					recog.base.set_state(497);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(498);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(499);
					recog.multipartIdentifier()?;

					recog.base.set_state(500);
					recog.base.match_token(DROP,&mut recog.err_handler)?;

					recog.base.set_state(501);
					_la = recog.base.input.la(1);
					if { !(_la==COLUMN || _la==COLUMNS) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(502);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifierList*/
					recog.base.set_state(503);
					let tmp = recog.multipartIdentifierList()?;
					if let StatementContextAll::DropTableColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.columns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(504);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				18 =>{
					let tmp = DropTableColumnsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 18);
					_localctx = tmp;
					{
					recog.base.set_state(506);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(507);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(508);
					recog.multipartIdentifier()?;

					recog.base.set_state(509);
					recog.base.match_token(DROP,&mut recog.err_handler)?;

					recog.base.set_state(510);
					_la = recog.base.input.la(1);
					if { !(_la==COLUMN || _la==COLUMNS) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule multipartIdentifierList*/
					recog.base.set_state(511);
					let tmp = recog.multipartIdentifierList()?;
					if let StatementContextAll::DropTableColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.columns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}
			,
				19 =>{
					let tmp = RenameTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 19);
					_localctx = tmp;
					{
					recog.base.set_state(513);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(514);
					_la = recog.base.input.la(1);
					if { !(_la==TABLE || _la==VIEW) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(515);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::RenameTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.from = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(516);
					recog.base.match_token(RENAME,&mut recog.err_handler)?;

					recog.base.set_state(517);
					recog.base.match_token(TO,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(518);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::RenameTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.to = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}
			,
				20 =>{
					let tmp = SetTablePropertiesContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 20);
					_localctx = tmp;
					{
					recog.base.set_state(520);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(521);
					_la = recog.base.input.la(1);
					if { !(_la==TABLE || _la==VIEW) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(522);
					recog.multipartIdentifier()?;

					recog.base.set_state(523);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(524);
					recog.base.match_token(TBLPROPERTIES,&mut recog.err_handler)?;

					/*InvokeRule tablePropertyList*/
					recog.base.set_state(525);
					recog.tablePropertyList()?;

					}
				}
			,
				21 =>{
					let tmp = UnsetTablePropertiesContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 21);
					_localctx = tmp;
					{
					recog.base.set_state(527);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(528);
					_la = recog.base.input.la(1);
					if { !(_la==TABLE || _la==VIEW) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(529);
					recog.multipartIdentifier()?;

					recog.base.set_state(530);
					recog.base.match_token(UNSET,&mut recog.err_handler)?;

					recog.base.set_state(531);
					recog.base.match_token(TBLPROPERTIES,&mut recog.err_handler)?;

					recog.base.set_state(534);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==IF {
						{
						recog.base.set_state(532);
						recog.base.match_token(IF,&mut recog.err_handler)?;

						recog.base.set_state(533);
						recog.base.match_token(EXISTS,&mut recog.err_handler)?;

						}
					}

					/*InvokeRule tablePropertyList*/
					recog.base.set_state(536);
					recog.tablePropertyList()?;

					}
				}
			,
				22 =>{
					let tmp = AlterTableAlterColumnContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 22);
					_localctx = tmp;
					{
					recog.base.set_state(538);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(539);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(540);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::AlterTableAlterColumnContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.table = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(541);
					_la = recog.base.input.la(1);
					if { !(_la==ALTER || _la==CHANGE) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(543);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(31,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(542);
							recog.base.match_token(COLUMN,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(545);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::AlterTableAlterColumnContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.column = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(547);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(32,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule alterColumnAction*/
							recog.base.set_state(546);
							recog.alterColumnAction()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				23 =>{
					let tmp = HiveChangeColumnContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 23);
					_localctx = tmp;
					{
					recog.base.set_state(549);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(550);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(551);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::HiveChangeColumnContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.table = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(553);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(552);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(555);
					recog.base.match_token(CHANGE,&mut recog.err_handler)?;

					recog.base.set_state(557);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(34,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(556);
							recog.base.match_token(COLUMN,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(559);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::HiveChangeColumnContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.colName = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					/*InvokeRule colType*/
					recog.base.set_state(560);
					recog.colType()?;

					recog.base.set_state(562);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==AFTER || _la==FIRST {
						{
						/*InvokeRule colPosition*/
						recog.base.set_state(561);
						recog.colPosition()?;

						}
					}

					}
				}
			,
				24 =>{
					let tmp = HiveReplaceColumnsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 24);
					_localctx = tmp;
					{
					recog.base.set_state(564);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(565);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(566);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::HiveReplaceColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.table = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(568);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(567);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(570);
					recog.base.match_token(REPLACE,&mut recog.err_handler)?;

					recog.base.set_state(571);
					recog.base.match_token(COLUMNS,&mut recog.err_handler)?;

					recog.base.set_state(572);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule qualifiedColTypeWithPositionList*/
					recog.base.set_state(573);
					let tmp = recog.qualifiedColTypeWithPositionList()?;
					if let StatementContextAll::HiveReplaceColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.columns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(574);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				25 =>{
					let tmp = SetTableSerDeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 25);
					_localctx = tmp;
					{
					recog.base.set_state(576);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(577);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(578);
					recog.multipartIdentifier()?;

					recog.base.set_state(580);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(579);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(582);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(583);
					recog.base.match_token(SERDE,&mut recog.err_handler)?;

					recog.base.set_state(584);
					recog.base.match_token(STRING,&mut recog.err_handler)?;

					recog.base.set_state(588);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(38,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(585);
							recog.base.match_token(WITH,&mut recog.err_handler)?;

							recog.base.set_state(586);
							recog.base.match_token(SERDEPROPERTIES,&mut recog.err_handler)?;

							/*InvokeRule tablePropertyList*/
							recog.base.set_state(587);
							recog.tablePropertyList()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				26 =>{
					let tmp = SetTableSerDeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 26);
					_localctx = tmp;
					{
					recog.base.set_state(590);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(591);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(592);
					recog.multipartIdentifier()?;

					recog.base.set_state(594);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(593);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(596);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(597);
					recog.base.match_token(SERDEPROPERTIES,&mut recog.err_handler)?;

					/*InvokeRule tablePropertyList*/
					recog.base.set_state(598);
					recog.tablePropertyList()?;

					}
				}
			,
				27 =>{
					let tmp = AddTablePartitionContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 27);
					_localctx = tmp;
					{
					recog.base.set_state(600);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(601);
					_la = recog.base.input.la(1);
					if { !(_la==TABLE || _la==VIEW) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(602);
					recog.multipartIdentifier()?;

					recog.base.set_state(603);
					recog.base.match_token(ADD,&mut recog.err_handler)?;

					recog.base.set_state(607);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==IF {
						{
						recog.base.set_state(604);
						recog.base.match_token(IF,&mut recog.err_handler)?;

						recog.base.set_state(605);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						recog.base.set_state(606);
						recog.base.match_token(EXISTS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(610); 
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					loop {
						{
						{
						/*InvokeRule partitionSpecLocation*/
						recog.base.set_state(609);
						recog.partitionSpecLocation()?;

						}
						}
						recog.base.set_state(612); 
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if !(_la==PARTITION) {break}
					}
					}
				}
			,
				28 =>{
					let tmp = RenameTablePartitionContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 28);
					_localctx = tmp;
					{
					recog.base.set_state(614);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(615);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(616);
					recog.multipartIdentifier()?;

					/*InvokeRule partitionSpec*/
					recog.base.set_state(617);
					let tmp = recog.partitionSpec()?;
					if let StatementContextAll::RenameTablePartitionContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.from = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(618);
					recog.base.match_token(RENAME,&mut recog.err_handler)?;

					recog.base.set_state(619);
					recog.base.match_token(TO,&mut recog.err_handler)?;

					/*InvokeRule partitionSpec*/
					recog.base.set_state(620);
					let tmp = recog.partitionSpec()?;
					if let StatementContextAll::RenameTablePartitionContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.to = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}
			,
				29 =>{
					let tmp = DropTablePartitionsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 29);
					_localctx = tmp;
					{
					recog.base.set_state(622);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(623);
					_la = recog.base.input.la(1);
					if { !(_la==TABLE || _la==VIEW) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(624);
					recog.multipartIdentifier()?;

					recog.base.set_state(625);
					recog.base.match_token(DROP,&mut recog.err_handler)?;

					recog.base.set_state(628);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==IF {
						{
						recog.base.set_state(626);
						recog.base.match_token(IF,&mut recog.err_handler)?;

						recog.base.set_state(627);
						recog.base.match_token(EXISTS,&mut recog.err_handler)?;

						}
					}

					/*InvokeRule partitionSpec*/
					recog.base.set_state(630);
					recog.partitionSpec()?;

					recog.base.set_state(635);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(631);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule partitionSpec*/
						recog.base.set_state(632);
						recog.partitionSpec()?;

						}
						}
						recog.base.set_state(637);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(639);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PURGE {
						{
						recog.base.set_state(638);
						recog.base.match_token(PURGE,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				30 =>{
					let tmp = SetTableLocationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 30);
					_localctx = tmp;
					{
					recog.base.set_state(641);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(642);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(643);
					recog.multipartIdentifier()?;

					recog.base.set_state(645);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(644);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(647);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					/*InvokeRule locationSpec*/
					recog.base.set_state(648);
					recog.locationSpec()?;

					}
				}
			,
				31 =>{
					let tmp = RecoverPartitionsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 31);
					_localctx = tmp;
					{
					recog.base.set_state(650);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(651);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(652);
					recog.multipartIdentifier()?;

					recog.base.set_state(653);
					recog.base.match_token(RECOVER,&mut recog.err_handler)?;

					recog.base.set_state(654);
					recog.base.match_token(PARTITIONS,&mut recog.err_handler)?;

					}
				}
			,
				32 =>{
					let tmp = DropTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 32);
					_localctx = tmp;
					{
					recog.base.set_state(656);
					recog.base.match_token(DROP,&mut recog.err_handler)?;

					recog.base.set_state(657);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					recog.base.set_state(660);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(46,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(658);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(659);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(662);
					recog.multipartIdentifier()?;

					recog.base.set_state(664);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PURGE {
						{
						recog.base.set_state(663);
						recog.base.match_token(PURGE,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				33 =>{
					let tmp = DropViewContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 33);
					_localctx = tmp;
					{
					recog.base.set_state(666);
					recog.base.match_token(DROP,&mut recog.err_handler)?;

					recog.base.set_state(667);
					recog.base.match_token(VIEW,&mut recog.err_handler)?;

					recog.base.set_state(670);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(48,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(668);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(669);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(672);
					recog.multipartIdentifier()?;

					}
				}
			,
				34 =>{
					let tmp = CreateViewContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 34);
					_localctx = tmp;
					{
					recog.base.set_state(673);
					recog.base.match_token(CREATE,&mut recog.err_handler)?;

					recog.base.set_state(676);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OR {
						{
						recog.base.set_state(674);
						recog.base.match_token(OR,&mut recog.err_handler)?;

						recog.base.set_state(675);
						recog.base.match_token(REPLACE,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(682);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==GLOBAL || _la==TEMPORARY {
						{
						recog.base.set_state(679);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if _la==GLOBAL {
							{
							recog.base.set_state(678);
							recog.base.match_token(GLOBAL,&mut recog.err_handler)?;

							}
						}

						recog.base.set_state(681);
						recog.base.match_token(TEMPORARY,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(684);
					recog.base.match_token(VIEW,&mut recog.err_handler)?;

					recog.base.set_state(688);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(52,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(685);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(686);
							recog.base.match_token(NOT,&mut recog.err_handler)?;

							recog.base.set_state(687);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(690);
					recog.multipartIdentifier()?;

					recog.base.set_state(692);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==T__0 {
						{
						/*InvokeRule identifierCommentList*/
						recog.base.set_state(691);
						recog.identifierCommentList()?;

						}
					}

					recog.base.set_state(702);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==COMMENT || _la==PARTITIONED || _la==TBLPROPERTIES {
						{
						recog.base.set_state(700);
						recog.err_handler.sync(&mut recog.base)?;
						match recog.base.input.la(1) {
						 COMMENT 
							=> {
								{
								/*InvokeRule commentSpec*/
								recog.base.set_state(694);
								recog.commentSpec()?;

								}
							}

						 PARTITIONED 
							=> {
								{
								{
								recog.base.set_state(695);
								recog.base.match_token(PARTITIONED,&mut recog.err_handler)?;

								recog.base.set_state(696);
								recog.base.match_token(ON,&mut recog.err_handler)?;

								/*InvokeRule identifierList*/
								recog.base.set_state(697);
								recog.identifierList()?;

								}
								}
							}

						 TBLPROPERTIES 
							=> {
								{
								{
								recog.base.set_state(698);
								recog.base.match_token(TBLPROPERTIES,&mut recog.err_handler)?;

								/*InvokeRule tablePropertyList*/
								recog.base.set_state(699);
								recog.tablePropertyList()?;

								}
								}
							}

							_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
						}
						}
						recog.base.set_state(704);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(705);
					recog.base.match_token(AS,&mut recog.err_handler)?;

					/*InvokeRule query*/
					recog.base.set_state(706);
					recog.query()?;

					}
				}
			,
				35 =>{
					let tmp = CreateTempViewUsingContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 35);
					_localctx = tmp;
					{
					recog.base.set_state(708);
					recog.base.match_token(CREATE,&mut recog.err_handler)?;

					recog.base.set_state(711);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OR {
						{
						recog.base.set_state(709);
						recog.base.match_token(OR,&mut recog.err_handler)?;

						recog.base.set_state(710);
						recog.base.match_token(REPLACE,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(714);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==GLOBAL {
						{
						recog.base.set_state(713);
						recog.base.match_token(GLOBAL,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(716);
					recog.base.match_token(TEMPORARY,&mut recog.err_handler)?;

					recog.base.set_state(717);
					recog.base.match_token(VIEW,&mut recog.err_handler)?;

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(718);
					recog.tableIdentifier()?;

					recog.base.set_state(723);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==T__0 {
						{
						recog.base.set_state(719);
						recog.base.match_token(T__0,&mut recog.err_handler)?;

						/*InvokeRule colTypeList*/
						recog.base.set_state(720);
						recog.colTypeList()?;

						recog.base.set_state(721);
						recog.base.match_token(T__1,&mut recog.err_handler)?;

						}
					}

					/*InvokeRule tableProvider*/
					recog.base.set_state(725);
					recog.tableProvider()?;

					recog.base.set_state(728);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OPTIONS {
						{
						recog.base.set_state(726);
						recog.base.match_token(OPTIONS,&mut recog.err_handler)?;

						/*InvokeRule tablePropertyList*/
						recog.base.set_state(727);
						recog.tablePropertyList()?;

						}
					}

					}
				}
			,
				36 =>{
					let tmp = AlterViewQueryContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 36);
					_localctx = tmp;
					{
					recog.base.set_state(730);
					recog.base.match_token(ALTER,&mut recog.err_handler)?;

					recog.base.set_state(731);
					recog.base.match_token(VIEW,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(732);
					recog.multipartIdentifier()?;

					recog.base.set_state(734);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==AS {
						{
						recog.base.set_state(733);
						recog.base.match_token(AS,&mut recog.err_handler)?;

						}
					}

					/*InvokeRule query*/
					recog.base.set_state(736);
					recog.query()?;

					}
				}
			,
				37 =>{
					let tmp = CreateFunctionContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 37);
					_localctx = tmp;
					{
					recog.base.set_state(738);
					recog.base.match_token(CREATE,&mut recog.err_handler)?;

					recog.base.set_state(741);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OR {
						{
						recog.base.set_state(739);
						recog.base.match_token(OR,&mut recog.err_handler)?;

						recog.base.set_state(740);
						recog.base.match_token(REPLACE,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(744);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==TEMPORARY {
						{
						recog.base.set_state(743);
						recog.base.match_token(TEMPORARY,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(746);
					recog.base.match_token(FUNCTION,&mut recog.err_handler)?;

					recog.base.set_state(750);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(63,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(747);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(748);
							recog.base.match_token(NOT,&mut recog.err_handler)?;

							recog.base.set_state(749);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(752);
					recog.multipartIdentifier()?;

					recog.base.set_state(753);
					recog.base.match_token(AS,&mut recog.err_handler)?;

					recog.base.set_state(754);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					if let StatementContextAll::CreateFunctionContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.className = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(764);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==USING {
						{
						recog.base.set_state(755);
						recog.base.match_token(USING,&mut recog.err_handler)?;

						/*InvokeRule resource*/
						recog.base.set_state(756);
						recog.resource()?;

						recog.base.set_state(761);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						while _la==T__2 {
							{
							{
							recog.base.set_state(757);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							/*InvokeRule resource*/
							recog.base.set_state(758);
							recog.resource()?;

							}
							}
							recog.base.set_state(763);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
						}
						}
					}

					}
				}
			,
				38 =>{
					let tmp = DropFunctionContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 38);
					_localctx = tmp;
					{
					recog.base.set_state(766);
					recog.base.match_token(DROP,&mut recog.err_handler)?;

					recog.base.set_state(768);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==TEMPORARY {
						{
						recog.base.set_state(767);
						recog.base.match_token(TEMPORARY,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(770);
					recog.base.match_token(FUNCTION,&mut recog.err_handler)?;

					recog.base.set_state(773);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(67,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(771);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(772);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(775);
					recog.multipartIdentifier()?;

					}
				}
			,
				39 =>{
					let tmp = ExplainContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 39);
					_localctx = tmp;
					{
					recog.base.set_state(776);
					recog.base.match_token(EXPLAIN,&mut recog.err_handler)?;

					recog.base.set_state(778);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==CODEGEN || _la==COST || _la==EXTENDED || _la==FORMATTED || _la==LOGICAL {
						{
						recog.base.set_state(777);
						_la = recog.base.input.la(1);
						if { !(_la==CODEGEN || _la==COST || _la==EXTENDED || _la==FORMATTED || _la==LOGICAL) } {
							recog.err_handler.recover_inline(&mut recog.base)?;

						}
						else {
							if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
							recog.err_handler.report_match(&mut recog.base);
							recog.base.consume(&mut recog.err_handler);
						}
						}
					}

					/*InvokeRule statement*/
					recog.base.set_state(780);
					recog.statement()?;

					}
				}
			,
				40 =>{
					let tmp = ShowTablesContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 40);
					_localctx = tmp;
					{
					recog.base.set_state(781);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(782);
					recog.base.match_token(TABLES,&mut recog.err_handler)?;

					recog.base.set_state(785);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(69,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(783);
							_la = recog.base.input.la(1);
							if { !(_la==FROM || _la==IN) } {
								recog.err_handler.recover_inline(&mut recog.base)?;

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							/*InvokeRule multipartIdentifier*/
							recog.base.set_state(784);
							recog.multipartIdentifier()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(791);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LIKE || _la==STRING {
						{
						recog.base.set_state(788);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if _la==LIKE {
							{
							recog.base.set_state(787);
							recog.base.match_token(LIKE,&mut recog.err_handler)?;

							}
						}

						recog.base.set_state(790);
						let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
						if let StatementContextAll::ShowTablesContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.pattern = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					}
				}
			,
				41 =>{
					let tmp = ShowTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 41);
					_localctx = tmp;
					{
					recog.base.set_state(793);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(794);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					recog.base.set_state(795);
					recog.base.match_token(EXTENDED,&mut recog.err_handler)?;

					recog.base.set_state(798);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==FROM || _la==IN {
						{
						recog.base.set_state(796);
						_la = recog.base.input.la(1);
						if { !(_la==FROM || _la==IN) } {
							recog.err_handler.recover_inline(&mut recog.base)?;

						}
						else {
							if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
							recog.err_handler.report_match(&mut recog.base);
							recog.base.consume(&mut recog.err_handler);
						}
						/*InvokeRule multipartIdentifier*/
						recog.base.set_state(797);
						let tmp = recog.multipartIdentifier()?;
						if let StatementContextAll::ShowTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.ns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					recog.base.set_state(800);
					recog.base.match_token(LIKE,&mut recog.err_handler)?;

					recog.base.set_state(801);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					if let StatementContextAll::ShowTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.pattern = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(803);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(802);
						recog.partitionSpec()?;

						}
					}

					}
				}
			,
				42 =>{
					let tmp = ShowTblPropertiesContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 42);
					_localctx = tmp;
					{
					recog.base.set_state(805);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(806);
					recog.base.match_token(TBLPROPERTIES,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(807);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::ShowTblPropertiesContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.table = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(812);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(74,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(808);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							/*InvokeRule tablePropertyKey*/
							recog.base.set_state(809);
							let tmp = recog.tablePropertyKey()?;
							if let StatementContextAll::ShowTblPropertiesContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
							ctx.key = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							recog.base.set_state(810);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					}
				}
			,
				43 =>{
					let tmp = ShowColumnsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 43);
					_localctx = tmp;
					{
					recog.base.set_state(814);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(815);
					recog.base.match_token(COLUMNS,&mut recog.err_handler)?;

					recog.base.set_state(816);
					_la = recog.base.input.la(1);
					if { !(_la==FROM || _la==IN) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(817);
					let tmp = recog.multipartIdentifier()?;
					if let StatementContextAll::ShowColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.table = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(820);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(75,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(818);
							_la = recog.base.input.la(1);
							if { !(_la==FROM || _la==IN) } {
								recog.err_handler.recover_inline(&mut recog.base)?;

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							/*InvokeRule multipartIdentifier*/
							recog.base.set_state(819);
							let tmp = recog.multipartIdentifier()?;
							if let StatementContextAll::ShowColumnsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
							ctx.ns = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					}
				}
			,
				44 =>{
					let tmp = ShowViewsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 44);
					_localctx = tmp;
					{
					recog.base.set_state(822);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(823);
					recog.base.match_token(VIEWS,&mut recog.err_handler)?;

					recog.base.set_state(826);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(76,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(824);
							_la = recog.base.input.la(1);
							if { !(_la==FROM || _la==IN) } {
								recog.err_handler.recover_inline(&mut recog.base)?;

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							/*InvokeRule multipartIdentifier*/
							recog.base.set_state(825);
							recog.multipartIdentifier()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(832);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LIKE || _la==STRING {
						{
						recog.base.set_state(829);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if _la==LIKE {
							{
							recog.base.set_state(828);
							recog.base.match_token(LIKE,&mut recog.err_handler)?;

							}
						}

						recog.base.set_state(831);
						let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
						if let StatementContextAll::ShowViewsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.pattern = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					}
				}
			,
				45 =>{
					let tmp = ShowPartitionsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 45);
					_localctx = tmp;
					{
					recog.base.set_state(834);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(835);
					recog.base.match_token(PARTITIONS,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(836);
					recog.multipartIdentifier()?;

					recog.base.set_state(838);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(837);
						recog.partitionSpec()?;

						}
					}

					}
				}
			,
				46 =>{
					let tmp = ShowFunctionsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 46);
					_localctx = tmp;
					{
					recog.base.set_state(840);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(842);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(80,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule identifier*/
							recog.base.set_state(841);
							recog.identifier()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(844);
					recog.base.match_token(FUNCTIONS,&mut recog.err_handler)?;

					recog.base.set_state(852);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(83,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(846);
							recog.err_handler.sync(&mut recog.base)?;
							match  recog.interpreter.adaptive_predict(81,&mut recog.base)? {
								x if x == 1=>{
									{
									recog.base.set_state(845);
									recog.base.match_token(LIKE,&mut recog.err_handler)?;

									}
								}

								_ => {}
							}
							recog.base.set_state(850);
							recog.err_handler.sync(&mut recog.base)?;
							match  recog.interpreter.adaptive_predict(82,&mut recog.base)? {
								1 =>{
									{
									/*InvokeRule multipartIdentifier*/
									recog.base.set_state(848);
									recog.multipartIdentifier()?;

									}
								}
							,
								2 =>{
									{
									recog.base.set_state(849);
									let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
									if let StatementContextAll::ShowFunctionsContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
									ctx.pattern = Some(tmp.clone()); } else {unreachable!("cant cast");}  

									}
								}

								_ => {}
							}
							}
						}

						_ => {}
					}
					}
				}
			,
				47 =>{
					let tmp = ShowCreateTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 47);
					_localctx = tmp;
					{
					recog.base.set_state(854);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(855);
					recog.base.match_token(CREATE,&mut recog.err_handler)?;

					recog.base.set_state(856);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(857);
					recog.multipartIdentifier()?;

					recog.base.set_state(860);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==AS {
						{
						recog.base.set_state(858);
						recog.base.match_token(AS,&mut recog.err_handler)?;

						recog.base.set_state(859);
						recog.base.match_token(SERDE,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				48 =>{
					let tmp = ShowCurrentNamespaceContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 48);
					_localctx = tmp;
					{
					recog.base.set_state(862);
					recog.base.match_token(SHOW,&mut recog.err_handler)?;

					recog.base.set_state(863);
					recog.base.match_token(CURRENT,&mut recog.err_handler)?;

					recog.base.set_state(864);
					recog.base.match_token(NAMESPACE,&mut recog.err_handler)?;

					}
				}
			,
				49 =>{
					let tmp = DescribeFunctionContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 49);
					_localctx = tmp;
					{
					recog.base.set_state(865);
					_la = recog.base.input.la(1);
					if { !(_la==DESC || _la==DESCRIBE) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(866);
					recog.base.match_token(FUNCTION,&mut recog.err_handler)?;

					recog.base.set_state(868);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(85,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(867);
							recog.base.match_token(EXTENDED,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule describeFuncName*/
					recog.base.set_state(870);
					recog.describeFuncName()?;

					}
				}
			,
				50 =>{
					let tmp = DescribeNamespaceContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 50);
					_localctx = tmp;
					{
					recog.base.set_state(871);
					_la = recog.base.input.la(1);
					if { !(_la==DESC || _la==DESCRIBE) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule namespace*/
					recog.base.set_state(872);
					recog.namespace()?;

					recog.base.set_state(874);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(86,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(873);
							recog.base.match_token(EXTENDED,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(876);
					recog.multipartIdentifier()?;

					}
				}
			,
				51 =>{
					let tmp = DescribeRelationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 51);
					_localctx = tmp;
					{
					recog.base.set_state(878);
					_la = recog.base.input.la(1);
					if { !(_la==DESC || _la==DESCRIBE) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(880);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(87,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(879);
							recog.base.match_token(TABLE,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					recog.base.set_state(883);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(88,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(882);
							if let StatementContextAll::DescribeRelationContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
							ctx.option = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
							_la = recog.base.input.la(1);
							if { !(_la==EXTENDED || _la==FORMATTED) } {
								let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
								if let StatementContextAll::DescribeRelationContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
								ctx.option = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(885);
					recog.multipartIdentifier()?;

					recog.base.set_state(887);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(89,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule partitionSpec*/
							recog.base.set_state(886);
							recog.partitionSpec()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(890);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(90,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule describeColName*/
							recog.base.set_state(889);
							recog.describeColName()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				52 =>{
					let tmp = DescribeQueryContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 52);
					_localctx = tmp;
					{
					recog.base.set_state(892);
					_la = recog.base.input.la(1);
					if { !(_la==DESC || _la==DESCRIBE) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(894);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==QUERY {
						{
						recog.base.set_state(893);
						recog.base.match_token(QUERY,&mut recog.err_handler)?;

						}
					}

					/*InvokeRule query*/
					recog.base.set_state(896);
					recog.query()?;

					}
				}
			,
				53 =>{
					let tmp = CommentNamespaceContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 53);
					_localctx = tmp;
					{
					recog.base.set_state(897);
					recog.base.match_token(COMMENT,&mut recog.err_handler)?;

					recog.base.set_state(898);
					recog.base.match_token(ON,&mut recog.err_handler)?;

					/*InvokeRule namespace*/
					recog.base.set_state(899);
					recog.namespace()?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(900);
					recog.multipartIdentifier()?;

					recog.base.set_state(901);
					recog.base.match_token(IS,&mut recog.err_handler)?;

					recog.base.set_state(902);
					if let StatementContextAll::CommentNamespaceContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.comment = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
					_la = recog.base.input.la(1);
					if { !(_la==NULL || _la==STRING) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						if let StatementContextAll::CommentNamespaceContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.comment = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}
			,
				54 =>{
					let tmp = CommentTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 54);
					_localctx = tmp;
					{
					recog.base.set_state(904);
					recog.base.match_token(COMMENT,&mut recog.err_handler)?;

					recog.base.set_state(905);
					recog.base.match_token(ON,&mut recog.err_handler)?;

					recog.base.set_state(906);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(907);
					recog.multipartIdentifier()?;

					recog.base.set_state(908);
					recog.base.match_token(IS,&mut recog.err_handler)?;

					recog.base.set_state(909);
					if let StatementContextAll::CommentTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.comment = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
					_la = recog.base.input.la(1);
					if { !(_la==NULL || _la==STRING) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						if let StatementContextAll::CommentTableContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.comment = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}
			,
				55 =>{
					let tmp = RefreshTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 55);
					_localctx = tmp;
					{
					recog.base.set_state(911);
					recog.base.match_token(REFRESH,&mut recog.err_handler)?;

					recog.base.set_state(912);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(913);
					recog.multipartIdentifier()?;

					}
				}
			,
				56 =>{
					let tmp = RefreshFunctionContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 56);
					_localctx = tmp;
					{
					recog.base.set_state(914);
					recog.base.match_token(REFRESH,&mut recog.err_handler)?;

					recog.base.set_state(915);
					recog.base.match_token(FUNCTION,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(916);
					recog.multipartIdentifier()?;

					}
				}
			,
				57 =>{
					let tmp = RefreshResourceContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 57);
					_localctx = tmp;
					{
					recog.base.set_state(917);
					recog.base.match_token(REFRESH,&mut recog.err_handler)?;

					recog.base.set_state(925);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(93,&mut recog.base)? {
						1 =>{
							{
							recog.base.set_state(918);
							recog.base.match_token(STRING,&mut recog.err_handler)?;

							}
						}
					,
						2 =>{
							{
							recog.base.set_state(922);
							recog.err_handler.sync(&mut recog.base)?;
							_alt = recog.interpreter.adaptive_predict(92,&mut recog.base)?;
							while { _alt!=1 && _alt!=INVALID_ALT } {
								if _alt==1+1 {
									{
									{
									recog.base.set_state(919);
									recog.base.match_wildcard(&mut recog.err_handler)?;

									}
									} 
								}
								recog.base.set_state(924);
								recog.err_handler.sync(&mut recog.base)?;
								_alt = recog.interpreter.adaptive_predict(92,&mut recog.base)?;
							}
							}
						}

						_ => {}
					}
					}
				}
			,
				58 =>{
					let tmp = CacheTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 58);
					_localctx = tmp;
					{
					recog.base.set_state(927);
					recog.base.match_token(CACHE,&mut recog.err_handler)?;

					recog.base.set_state(929);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LAZY {
						{
						recog.base.set_state(928);
						recog.base.match_token(LAZY,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(931);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(932);
					recog.multipartIdentifier()?;

					recog.base.set_state(935);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OPTIONS {
						{
						recog.base.set_state(933);
						recog.base.match_token(OPTIONS,&mut recog.err_handler)?;

						/*InvokeRule tablePropertyList*/
						recog.base.set_state(934);
						recog.tablePropertyList()?;

						}
					}

					recog.base.set_state(941);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(97,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(938);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==AS {
								{
								recog.base.set_state(937);
								recog.base.match_token(AS,&mut recog.err_handler)?;

								}
							}

							/*InvokeRule query*/
							recog.base.set_state(940);
							recog.query()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				59 =>{
					let tmp = UncacheTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 59);
					_localctx = tmp;
					{
					recog.base.set_state(943);
					recog.base.match_token(UNCACHE,&mut recog.err_handler)?;

					recog.base.set_state(944);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					recog.base.set_state(947);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(98,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(945);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(946);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(949);
					recog.multipartIdentifier()?;

					}
				}
			,
				60 =>{
					let tmp = ClearCacheContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 60);
					_localctx = tmp;
					{
					recog.base.set_state(950);
					recog.base.match_token(CLEAR,&mut recog.err_handler)?;

					recog.base.set_state(951);
					recog.base.match_token(CACHE,&mut recog.err_handler)?;

					}
				}
			,
				61 =>{
					let tmp = LoadDataContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 61);
					_localctx = tmp;
					{
					recog.base.set_state(952);
					recog.base.match_token(LOAD,&mut recog.err_handler)?;

					recog.base.set_state(953);
					recog.base.match_token(DATA,&mut recog.err_handler)?;

					recog.base.set_state(955);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LOCAL {
						{
						recog.base.set_state(954);
						recog.base.match_token(LOCAL,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(957);
					recog.base.match_token(INPATH,&mut recog.err_handler)?;

					recog.base.set_state(958);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					if let StatementContextAll::LoadDataContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.path = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(960);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OVERWRITE {
						{
						recog.base.set_state(959);
						recog.base.match_token(OVERWRITE,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(962);
					recog.base.match_token(INTO,&mut recog.err_handler)?;

					recog.base.set_state(963);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(964);
					recog.multipartIdentifier()?;

					recog.base.set_state(966);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(965);
						recog.partitionSpec()?;

						}
					}

					}
				}
			,
				62 =>{
					let tmp = TruncateTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 62);
					_localctx = tmp;
					{
					recog.base.set_state(968);
					recog.base.match_token(TRUNCATE,&mut recog.err_handler)?;

					recog.base.set_state(969);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(970);
					recog.multipartIdentifier()?;

					recog.base.set_state(972);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(971);
						recog.partitionSpec()?;

						}
					}

					}
				}
			,
				63 =>{
					let tmp = RepairTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 63);
					_localctx = tmp;
					{
					recog.base.set_state(974);
					recog.base.match_token(MSCK,&mut recog.err_handler)?;

					recog.base.set_state(975);
					recog.base.match_token(REPAIR,&mut recog.err_handler)?;

					recog.base.set_state(976);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(977);
					recog.multipartIdentifier()?;

					}
				}
			,
				64 =>{
					let tmp = ManageResourceContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 64);
					_localctx = tmp;
					{
					recog.base.set_state(978);
					if let StatementContextAll::ManageResourceContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.op = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
					_la = recog.base.input.la(1);
					if { !(_la==ADD || _la==LIST) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						if let StatementContextAll::ManageResourceContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.op = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule identifier*/
					recog.base.set_state(979);
					recog.identifier()?;

					recog.base.set_state(987);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(104,&mut recog.base)? {
						1 =>{
							{
							recog.base.set_state(980);
							recog.base.match_token(STRING,&mut recog.err_handler)?;

							}
						}
					,
						2 =>{
							{
							recog.base.set_state(984);
							recog.err_handler.sync(&mut recog.base)?;
							_alt = recog.interpreter.adaptive_predict(103,&mut recog.base)?;
							while { _alt!=1 && _alt!=INVALID_ALT } {
								if _alt==1+1 {
									{
									{
									recog.base.set_state(981);
									recog.base.match_wildcard(&mut recog.err_handler)?;

									}
									} 
								}
								recog.base.set_state(986);
								recog.err_handler.sync(&mut recog.base)?;
								_alt = recog.interpreter.adaptive_predict(103,&mut recog.base)?;
							}
							}
						}

						_ => {}
					}
					}
				}
			,
				65 =>{
					let tmp = FailNativeCommandContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 65);
					_localctx = tmp;
					{
					recog.base.set_state(989);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(990);
					recog.base.match_token(ROLE,&mut recog.err_handler)?;

					recog.base.set_state(994);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(105,&mut recog.base)?;
					while { _alt!=1 && _alt!=INVALID_ALT } {
						if _alt==1+1 {
							{
							{
							recog.base.set_state(991);
							recog.base.match_wildcard(&mut recog.err_handler)?;

							}
							} 
						}
						recog.base.set_state(996);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(105,&mut recog.base)?;
					}
					}
				}
			,
				66 =>{
					let tmp = SetTimeZoneContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 66);
					_localctx = tmp;
					{
					recog.base.set_state(997);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(998);
					recog.base.match_token(TIME,&mut recog.err_handler)?;

					recog.base.set_state(999);
					recog.base.match_token(ZONE,&mut recog.err_handler)?;

					/*InvokeRule interval*/
					recog.base.set_state(1000);
					recog.interval()?;

					}
				}
			,
				67 =>{
					let tmp = SetTimeZoneContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 67);
					_localctx = tmp;
					{
					recog.base.set_state(1001);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(1002);
					recog.base.match_token(TIME,&mut recog.err_handler)?;

					recog.base.set_state(1003);
					recog.base.match_token(ZONE,&mut recog.err_handler)?;

					recog.base.set_state(1004);
					if let StatementContextAll::SetTimeZoneContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
					ctx.timezone = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
					_la = recog.base.input.la(1);
					if { !(_la==LOCAL || _la==STRING) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						if let StatementContextAll::SetTimeZoneContext(ctx) = cast_mut::<_,StatementContextAll >(&mut _localctx){
						ctx.timezone = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}
			,
				68 =>{
					let tmp = SetTimeZoneContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 68);
					_localctx = tmp;
					{
					recog.base.set_state(1005);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(1006);
					recog.base.match_token(TIME,&mut recog.err_handler)?;

					recog.base.set_state(1007);
					recog.base.match_token(ZONE,&mut recog.err_handler)?;

					recog.base.set_state(1011);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(106,&mut recog.base)?;
					while { _alt!=1 && _alt!=INVALID_ALT } {
						if _alt==1+1 {
							{
							{
							recog.base.set_state(1008);
							recog.base.match_wildcard(&mut recog.err_handler)?;

							}
							} 
						}
						recog.base.set_state(1013);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(106,&mut recog.base)?;
					}
					}
				}
			,
				69 =>{
					let tmp = SetQuotedConfigurationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 69);
					_localctx = tmp;
					{
					recog.base.set_state(1014);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					/*InvokeRule configKey*/
					recog.base.set_state(1015);
					recog.configKey()?;

					recog.base.set_state(1023);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==EQ {
						{
						recog.base.set_state(1016);
						recog.base.match_token(EQ,&mut recog.err_handler)?;

						recog.base.set_state(1020);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(107,&mut recog.base)?;
						while { _alt!=1 && _alt!=INVALID_ALT } {
							if _alt==1+1 {
								{
								{
								recog.base.set_state(1017);
								recog.base.match_wildcard(&mut recog.err_handler)?;

								}
								} 
							}
							recog.base.set_state(1022);
							recog.err_handler.sync(&mut recog.base)?;
							_alt = recog.interpreter.adaptive_predict(107,&mut recog.base)?;
						}
						}
					}

					}
				}
			,
				70 =>{
					let tmp = SetConfigurationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 70);
					_localctx = tmp;
					{
					recog.base.set_state(1025);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(1029);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(109,&mut recog.base)?;
					while { _alt!=1 && _alt!=INVALID_ALT } {
						if _alt==1+1 {
							{
							{
							recog.base.set_state(1026);
							recog.base.match_wildcard(&mut recog.err_handler)?;

							}
							} 
						}
						recog.base.set_state(1031);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(109,&mut recog.base)?;
					}
					}
				}
			,
				71 =>{
					let tmp = ResetQuotedConfigurationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 71);
					_localctx = tmp;
					{
					recog.base.set_state(1032);
					recog.base.match_token(RESET,&mut recog.err_handler)?;

					/*InvokeRule configKey*/
					recog.base.set_state(1033);
					recog.configKey()?;

					}
				}
			,
				72 =>{
					let tmp = ResetConfigurationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 72);
					_localctx = tmp;
					{
					recog.base.set_state(1034);
					recog.base.match_token(RESET,&mut recog.err_handler)?;

					recog.base.set_state(1038);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(110,&mut recog.base)?;
					while { _alt!=1 && _alt!=INVALID_ALT } {
						if _alt==1+1 {
							{
							{
							recog.base.set_state(1035);
							recog.base.match_wildcard(&mut recog.err_handler)?;

							}
							} 
						}
						recog.base.set_state(1040);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(110,&mut recog.base)?;
					}
					}
				}
			,
				73 =>{
					let tmp = FailNativeCommandContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 73);
					_localctx = tmp;
					{
					/*InvokeRule unsupportedHiveNativeCommands*/
					recog.base.set_state(1041);
					recog.unsupportedHiveNativeCommands()?;

					recog.base.set_state(1045);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(111,&mut recog.base)?;
					while { _alt!=1 && _alt!=INVALID_ALT } {
						if _alt==1+1 {
							{
							{
							recog.base.set_state(1042);
							recog.base.match_wildcard(&mut recog.err_handler)?;

							}
							} 
						}
						recog.base.set_state(1047);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(111,&mut recog.base)?;
					}
					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- configKey ----------------
pub type ConfigKeyContextAll<'input> = ConfigKeyContext<'input>;


pub type ConfigKeyContext<'input> = BaseParserRuleContext<'input,ConfigKeyContextExt<'input>>;

#[derive(Clone)]
pub struct ConfigKeyContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ConfigKeyContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ConfigKeyContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_configKey(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_configKey(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ConfigKeyContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_configKey }
	//fn type_rule_index() -> usize where Self: Sized { RULE_configKey }
}
crate::tid!{ConfigKeyContextExt<'a>}

impl<'input> ConfigKeyContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ConfigKeyContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ConfigKeyContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ConfigKeyContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ConfigKeyContextExt<'input>>{

fn quotedIdentifier(&self) -> Option<Rc<QuotedIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ConfigKeyContextAttrs<'input> for ConfigKeyContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn configKey(&mut self,)
	-> Result<Rc<ConfigKeyContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ConfigKeyContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 18, RULE_configKey);
        let mut _localctx: Rc<ConfigKeyContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule quotedIdentifier*/
			recog.base.set_state(1050);
			recog.quotedIdentifier()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- unsupportedHiveNativeCommands ----------------
pub type UnsupportedHiveNativeCommandsContextAll<'input> = UnsupportedHiveNativeCommandsContext<'input>;


pub type UnsupportedHiveNativeCommandsContext<'input> = BaseParserRuleContext<'input,UnsupportedHiveNativeCommandsContextExt<'input>>;

#[derive(Clone)]
pub struct UnsupportedHiveNativeCommandsContextExt<'input>{
	pub kw1: Option<TokenType<'input>>,
	pub kw2: Option<TokenType<'input>>,
	pub kw3: Option<TokenType<'input>>,
	pub kw4: Option<TokenType<'input>>,
	pub kw5: Option<TokenType<'input>>,
	pub kw6: Option<TokenType<'input>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for UnsupportedHiveNativeCommandsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for UnsupportedHiveNativeCommandsContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_unsupportedHiveNativeCommands(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_unsupportedHiveNativeCommands(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for UnsupportedHiveNativeCommandsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_unsupportedHiveNativeCommands }
	//fn type_rule_index() -> usize where Self: Sized { RULE_unsupportedHiveNativeCommands }
}
crate::tid!{UnsupportedHiveNativeCommandsContextExt<'a>}

impl<'input> UnsupportedHiveNativeCommandsContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<UnsupportedHiveNativeCommandsContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,UnsupportedHiveNativeCommandsContextExt{
				kw1: None, kw2: None, kw3: None, kw4: None, kw5: None, kw6: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait UnsupportedHiveNativeCommandsContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<UnsupportedHiveNativeCommandsContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token CREATE
/// Returns `None` if there is no child corresponding to token CREATE
fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CREATE, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLE
/// Returns `None` if there is no child corresponding to token ROLE
fn ROLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLE, 0)
}
/// Retrieves first TerminalNode corresponding to token DROP
/// Returns `None` if there is no child corresponding to token DROP
fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DROP, 0)
}
/// Retrieves first TerminalNode corresponding to token GRANT
/// Returns `None` if there is no child corresponding to token GRANT
fn GRANT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GRANT, 0)
}
/// Retrieves first TerminalNode corresponding to token REVOKE
/// Returns `None` if there is no child corresponding to token REVOKE
fn REVOKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REVOKE, 0)
}
/// Retrieves first TerminalNode corresponding to token SHOW
/// Returns `None` if there is no child corresponding to token SHOW
fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SHOW, 0)
}
/// Retrieves first TerminalNode corresponding to token PRINCIPALS
/// Returns `None` if there is no child corresponding to token PRINCIPALS
fn PRINCIPALS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PRINCIPALS, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLES
/// Returns `None` if there is no child corresponding to token ROLES
fn ROLES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLES, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT
/// Returns `None` if there is no child corresponding to token CURRENT
fn CURRENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT, 0)
}
/// Retrieves first TerminalNode corresponding to token EXPORT
/// Returns `None` if there is no child corresponding to token EXPORT
fn EXPORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXPORT, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLE
/// Returns `None` if there is no child corresponding to token TABLE
fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLE, 0)
}
/// Retrieves first TerminalNode corresponding to token IMPORT
/// Returns `None` if there is no child corresponding to token IMPORT
fn IMPORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IMPORT, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPACTIONS
/// Returns `None` if there is no child corresponding to token COMPACTIONS
fn COMPACTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPACTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSACTIONS
/// Returns `None` if there is no child corresponding to token TRANSACTIONS
fn TRANSACTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSACTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token INDEXES
/// Returns `None` if there is no child corresponding to token INDEXES
fn INDEXES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INDEXES, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCKS
/// Returns `None` if there is no child corresponding to token LOCKS
fn LOCKS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCKS, 0)
}
/// Retrieves first TerminalNode corresponding to token INDEX
/// Returns `None` if there is no child corresponding to token INDEX
fn INDEX(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INDEX, 0)
}
/// Retrieves first TerminalNode corresponding to token ALTER
/// Returns `None` if there is no child corresponding to token ALTER
fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ALTER, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCK
/// Returns `None` if there is no child corresponding to token LOCK
fn LOCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCK, 0)
}
/// Retrieves first TerminalNode corresponding to token DATABASE
/// Returns `None` if there is no child corresponding to token DATABASE
fn DATABASE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATABASE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNLOCK
/// Returns `None` if there is no child corresponding to token UNLOCK
fn UNLOCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNLOCK, 0)
}
/// Retrieves first TerminalNode corresponding to token TEMPORARY
/// Returns `None` if there is no child corresponding to token TEMPORARY
fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TEMPORARY, 0)
}
/// Retrieves first TerminalNode corresponding to token MACRO
/// Returns `None` if there is no child corresponding to token MACRO
fn MACRO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MACRO, 0)
}
fn tableIdentifier(&self) -> Option<Rc<TableIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token CLUSTERED
/// Returns `None` if there is no child corresponding to token CLUSTERED
fn CLUSTERED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLUSTERED, 0)
}
/// Retrieves first TerminalNode corresponding to token BY
/// Returns `None` if there is no child corresponding to token BY
fn BY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, 0)
}
/// Retrieves first TerminalNode corresponding to token SORTED
/// Returns `None` if there is no child corresponding to token SORTED
fn SORTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SORTED, 0)
}
/// Retrieves first TerminalNode corresponding to token SKEWED
/// Returns `None` if there is no child corresponding to token SKEWED
fn SKEWED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SKEWED, 0)
}
/// Retrieves first TerminalNode corresponding to token STORED
/// Returns `None` if there is no child corresponding to token STORED
fn STORED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STORED, 0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
/// Retrieves first TerminalNode corresponding to token DIRECTORIES
/// Returns `None` if there is no child corresponding to token DIRECTORIES
fn DIRECTORIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIRECTORIES, 0)
}
/// Retrieves first TerminalNode corresponding to token SET
/// Returns `None` if there is no child corresponding to token SET
fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SET, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCATION
/// Returns `None` if there is no child corresponding to token LOCATION
fn LOCATION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCATION, 0)
}
/// Retrieves first TerminalNode corresponding to token EXCHANGE
/// Returns `None` if there is no child corresponding to token EXCHANGE
fn EXCHANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXCHANGE, 0)
}
/// Retrieves first TerminalNode corresponding to token PARTITION
/// Returns `None` if there is no child corresponding to token PARTITION
fn PARTITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITION, 0)
}
/// Retrieves first TerminalNode corresponding to token ARCHIVE
/// Returns `None` if there is no child corresponding to token ARCHIVE
fn ARCHIVE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ARCHIVE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNARCHIVE
/// Returns `None` if there is no child corresponding to token UNARCHIVE
fn UNARCHIVE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNARCHIVE, 0)
}
/// Retrieves first TerminalNode corresponding to token TOUCH
/// Returns `None` if there is no child corresponding to token TOUCH
fn TOUCH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TOUCH, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPACT
/// Returns `None` if there is no child corresponding to token COMPACT
fn COMPACT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPACT, 0)
}
fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token CONCATENATE
/// Returns `None` if there is no child corresponding to token CONCATENATE
fn CONCATENATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CONCATENATE, 0)
}
/// Retrieves first TerminalNode corresponding to token FILEFORMAT
/// Returns `None` if there is no child corresponding to token FILEFORMAT
fn FILEFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FILEFORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token REPLACE
/// Returns `None` if there is no child corresponding to token REPLACE
fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REPLACE, 0)
}
/// Retrieves first TerminalNode corresponding to token COLUMNS
/// Returns `None` if there is no child corresponding to token COLUMNS
fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COLUMNS, 0)
}
/// Retrieves first TerminalNode corresponding to token START
/// Returns `None` if there is no child corresponding to token START
fn START(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(START, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSACTION
/// Returns `None` if there is no child corresponding to token TRANSACTION
fn TRANSACTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSACTION, 0)
}
/// Retrieves first TerminalNode corresponding to token COMMIT
/// Returns `None` if there is no child corresponding to token COMMIT
fn COMMIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMMIT, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLLBACK
/// Returns `None` if there is no child corresponding to token ROLLBACK
fn ROLLBACK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLLBACK, 0)
}
/// Retrieves first TerminalNode corresponding to token DFS
/// Returns `None` if there is no child corresponding to token DFS
fn DFS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DFS, 0)
}

}

impl<'input> UnsupportedHiveNativeCommandsContextAttrs<'input> for UnsupportedHiveNativeCommandsContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn unsupportedHiveNativeCommands(&mut self,)
	-> Result<Rc<UnsupportedHiveNativeCommandsContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = UnsupportedHiveNativeCommandsContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 20, RULE_unsupportedHiveNativeCommands);
        let mut _localctx: Rc<UnsupportedHiveNativeCommandsContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1220);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(120,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(1052);
					let tmp = recog.base.match_token(CREATE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1053);
					let tmp = recog.base.match_token(ROLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1054);
					let tmp = recog.base.match_token(DROP,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1055);
					let tmp = recog.base.match_token(ROLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					recog.base.set_state(1056);
					let tmp = recog.base.match_token(GRANT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1058);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(113,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1057);
							let tmp = recog.base.match_token(ROLE,&mut recog.err_handler)?;
							 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
							  

							}
						}

						_ => {}
					}
					}
				}
			,
				4 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					recog.base.set_state(1060);
					let tmp = recog.base.match_token(REVOKE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1062);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(114,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1061);
							let tmp = recog.base.match_token(ROLE,&mut recog.err_handler)?;
							 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
							  

							}
						}

						_ => {}
					}
					}
				}
			,
				5 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 5);
					recog.base.enter_outer_alt(None, 5);
					{
					recog.base.set_state(1064);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1065);
					let tmp = recog.base.match_token(GRANT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				6 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 6);
					recog.base.enter_outer_alt(None, 6);
					{
					recog.base.set_state(1066);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1067);
					let tmp = recog.base.match_token(ROLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					recog.base.set_state(1069);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(115,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1068);
							let tmp = recog.base.match_token(GRANT,&mut recog.err_handler)?;
							 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
							  

							}
						}

						_ => {}
					}
					}
				}
			,
				7 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 7);
					recog.base.enter_outer_alt(None, 7);
					{
					recog.base.set_state(1071);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1072);
					let tmp = recog.base.match_token(PRINCIPALS,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				8 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 8);
					recog.base.enter_outer_alt(None, 8);
					{
					recog.base.set_state(1073);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1074);
					let tmp = recog.base.match_token(ROLES,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				9 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 9);
					recog.base.enter_outer_alt(None, 9);
					{
					recog.base.set_state(1075);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1076);
					let tmp = recog.base.match_token(CURRENT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					recog.base.set_state(1077);
					let tmp = recog.base.match_token(ROLES,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					}
				}
			,
				10 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 10);
					recog.base.enter_outer_alt(None, 10);
					{
					recog.base.set_state(1078);
					let tmp = recog.base.match_token(EXPORT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1079);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				11 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 11);
					recog.base.enter_outer_alt(None, 11);
					{
					recog.base.set_state(1080);
					let tmp = recog.base.match_token(IMPORT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1081);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				12 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 12);
					recog.base.enter_outer_alt(None, 12);
					{
					recog.base.set_state(1082);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1083);
					let tmp = recog.base.match_token(COMPACTIONS,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				13 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 13);
					recog.base.enter_outer_alt(None, 13);
					{
					recog.base.set_state(1084);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1085);
					let tmp = recog.base.match_token(CREATE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					recog.base.set_state(1086);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					}
				}
			,
				14 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 14);
					recog.base.enter_outer_alt(None, 14);
					{
					recog.base.set_state(1087);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1088);
					let tmp = recog.base.match_token(TRANSACTIONS,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				15 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 15);
					recog.base.enter_outer_alt(None, 15);
					{
					recog.base.set_state(1089);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1090);
					let tmp = recog.base.match_token(INDEXES,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				16 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 16);
					recog.base.enter_outer_alt(None, 16);
					{
					recog.base.set_state(1091);
					let tmp = recog.base.match_token(SHOW,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1092);
					let tmp = recog.base.match_token(LOCKS,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				17 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 17);
					recog.base.enter_outer_alt(None, 17);
					{
					recog.base.set_state(1093);
					let tmp = recog.base.match_token(CREATE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1094);
					let tmp = recog.base.match_token(INDEX,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				18 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 18);
					recog.base.enter_outer_alt(None, 18);
					{
					recog.base.set_state(1095);
					let tmp = recog.base.match_token(DROP,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1096);
					let tmp = recog.base.match_token(INDEX,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				19 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 19);
					recog.base.enter_outer_alt(None, 19);
					{
					recog.base.set_state(1097);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1098);
					let tmp = recog.base.match_token(INDEX,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				20 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 20);
					recog.base.enter_outer_alt(None, 20);
					{
					recog.base.set_state(1099);
					let tmp = recog.base.match_token(LOCK,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1100);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				21 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 21);
					recog.base.enter_outer_alt(None, 21);
					{
					recog.base.set_state(1101);
					let tmp = recog.base.match_token(LOCK,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1102);
					let tmp = recog.base.match_token(DATABASE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				22 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 22);
					recog.base.enter_outer_alt(None, 22);
					{
					recog.base.set_state(1103);
					let tmp = recog.base.match_token(UNLOCK,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1104);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				23 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 23);
					recog.base.enter_outer_alt(None, 23);
					{
					recog.base.set_state(1105);
					let tmp = recog.base.match_token(UNLOCK,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1106);
					let tmp = recog.base.match_token(DATABASE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				24 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 24);
					recog.base.enter_outer_alt(None, 24);
					{
					recog.base.set_state(1107);
					let tmp = recog.base.match_token(CREATE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1108);
					let tmp = recog.base.match_token(TEMPORARY,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					recog.base.set_state(1109);
					let tmp = recog.base.match_token(MACRO,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					}
				}
			,
				25 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 25);
					recog.base.enter_outer_alt(None, 25);
					{
					recog.base.set_state(1110);
					let tmp = recog.base.match_token(DROP,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1111);
					let tmp = recog.base.match_token(TEMPORARY,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					recog.base.set_state(1112);
					let tmp = recog.base.match_token(MACRO,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					}
				}
			,
				26 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 26);
					recog.base.enter_outer_alt(None, 26);
					{
					recog.base.set_state(1113);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1114);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1115);
					recog.tableIdentifier()?;

					recog.base.set_state(1116);
					let tmp = recog.base.match_token(NOT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1117);
					let tmp = recog.base.match_token(CLUSTERED,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				27 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 27);
					recog.base.enter_outer_alt(None, 27);
					{
					recog.base.set_state(1119);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1120);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1121);
					recog.tableIdentifier()?;

					recog.base.set_state(1122);
					let tmp = recog.base.match_token(CLUSTERED,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1123);
					let tmp = recog.base.match_token(BY,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				28 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 28);
					recog.base.enter_outer_alt(None, 28);
					{
					recog.base.set_state(1125);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1126);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1127);
					recog.tableIdentifier()?;

					recog.base.set_state(1128);
					let tmp = recog.base.match_token(NOT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1129);
					let tmp = recog.base.match_token(SORTED,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				29 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 29);
					recog.base.enter_outer_alt(None, 29);
					{
					recog.base.set_state(1131);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1132);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1133);
					recog.tableIdentifier()?;

					recog.base.set_state(1134);
					let tmp = recog.base.match_token(SKEWED,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1135);
					let tmp = recog.base.match_token(BY,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				30 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 30);
					recog.base.enter_outer_alt(None, 30);
					{
					recog.base.set_state(1137);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1138);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1139);
					recog.tableIdentifier()?;

					recog.base.set_state(1140);
					let tmp = recog.base.match_token(NOT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1141);
					let tmp = recog.base.match_token(SKEWED,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				31 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 31);
					recog.base.enter_outer_alt(None, 31);
					{
					recog.base.set_state(1143);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1144);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1145);
					recog.tableIdentifier()?;

					recog.base.set_state(1146);
					let tmp = recog.base.match_token(NOT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1147);
					let tmp = recog.base.match_token(STORED,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					recog.base.set_state(1148);
					let tmp = recog.base.match_token(AS,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw5 = Some(tmp.clone());
					  

					recog.base.set_state(1149);
					let tmp = recog.base.match_token(DIRECTORIES,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw6 = Some(tmp.clone());
					  

					}
				}
			,
				32 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 32);
					recog.base.enter_outer_alt(None, 32);
					{
					recog.base.set_state(1151);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1152);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1153);
					recog.tableIdentifier()?;

					recog.base.set_state(1154);
					let tmp = recog.base.match_token(SET,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1155);
					let tmp = recog.base.match_token(SKEWED,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					recog.base.set_state(1156);
					let tmp = recog.base.match_token(LOCATION,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw5 = Some(tmp.clone());
					  

					}
				}
			,
				33 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 33);
					recog.base.enter_outer_alt(None, 33);
					{
					recog.base.set_state(1158);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1159);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1160);
					recog.tableIdentifier()?;

					recog.base.set_state(1161);
					let tmp = recog.base.match_token(EXCHANGE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1162);
					let tmp = recog.base.match_token(PARTITION,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				34 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 34);
					recog.base.enter_outer_alt(None, 34);
					{
					recog.base.set_state(1164);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1165);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1166);
					recog.tableIdentifier()?;

					recog.base.set_state(1167);
					let tmp = recog.base.match_token(ARCHIVE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1168);
					let tmp = recog.base.match_token(PARTITION,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				35 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 35);
					recog.base.enter_outer_alt(None, 35);
					{
					recog.base.set_state(1170);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1171);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1172);
					recog.tableIdentifier()?;

					recog.base.set_state(1173);
					let tmp = recog.base.match_token(UNARCHIVE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1174);
					let tmp = recog.base.match_token(PARTITION,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				36 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 36);
					recog.base.enter_outer_alt(None, 36);
					{
					recog.base.set_state(1176);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1177);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1178);
					recog.tableIdentifier()?;

					recog.base.set_state(1179);
					let tmp = recog.base.match_token(TOUCH,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					}
				}
			,
				37 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 37);
					recog.base.enter_outer_alt(None, 37);
					{
					recog.base.set_state(1181);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1182);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1183);
					recog.tableIdentifier()?;

					recog.base.set_state(1185);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(1184);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(1187);
					let tmp = recog.base.match_token(COMPACT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					}
				}
			,
				38 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 38);
					recog.base.enter_outer_alt(None, 38);
					{
					recog.base.set_state(1189);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1190);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1191);
					recog.tableIdentifier()?;

					recog.base.set_state(1193);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(1192);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(1195);
					let tmp = recog.base.match_token(CONCATENATE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					}
				}
			,
				39 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 39);
					recog.base.enter_outer_alt(None, 39);
					{
					recog.base.set_state(1197);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1198);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1199);
					recog.tableIdentifier()?;

					recog.base.set_state(1201);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(1200);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(1203);
					let tmp = recog.base.match_token(SET,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1204);
					let tmp = recog.base.match_token(FILEFORMAT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				40 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 40);
					recog.base.enter_outer_alt(None, 40);
					{
					recog.base.set_state(1206);
					let tmp = recog.base.match_token(ALTER,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1207);
					let tmp = recog.base.match_token(TABLE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					/*InvokeRule tableIdentifier*/
					recog.base.set_state(1208);
					recog.tableIdentifier()?;

					recog.base.set_state(1210);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(1209);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(1212);
					let tmp = recog.base.match_token(REPLACE,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw3 = Some(tmp.clone());
					  

					recog.base.set_state(1213);
					let tmp = recog.base.match_token(COLUMNS,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw4 = Some(tmp.clone());
					  

					}
				}
			,
				41 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 41);
					recog.base.enter_outer_alt(None, 41);
					{
					recog.base.set_state(1215);
					let tmp = recog.base.match_token(START,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					recog.base.set_state(1216);
					let tmp = recog.base.match_token(TRANSACTION,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw2 = Some(tmp.clone());
					  

					}
				}
			,
				42 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 42);
					recog.base.enter_outer_alt(None, 42);
					{
					recog.base.set_state(1217);
					let tmp = recog.base.match_token(COMMIT,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					}
				}
			,
				43 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 43);
					recog.base.enter_outer_alt(None, 43);
					{
					recog.base.set_state(1218);
					let tmp = recog.base.match_token(ROLLBACK,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					}
				}
			,
				44 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 44);
					recog.base.enter_outer_alt(None, 44);
					{
					recog.base.set_state(1219);
					let tmp = recog.base.match_token(DFS,&mut recog.err_handler)?;
					 cast_mut::<_,UnsupportedHiveNativeCommandsContext >(&mut _localctx).kw1 = Some(tmp.clone());
					  

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- createTableHeader ----------------
pub type CreateTableHeaderContextAll<'input> = CreateTableHeaderContext<'input>;


pub type CreateTableHeaderContext<'input> = BaseParserRuleContext<'input,CreateTableHeaderContextExt<'input>>;

#[derive(Clone)]
pub struct CreateTableHeaderContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for CreateTableHeaderContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateTableHeaderContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_createTableHeader(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_createTableHeader(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for CreateTableHeaderContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_createTableHeader }
	//fn type_rule_index() -> usize where Self: Sized { RULE_createTableHeader }
}
crate::tid!{CreateTableHeaderContextExt<'a>}

impl<'input> CreateTableHeaderContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<CreateTableHeaderContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,CreateTableHeaderContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait CreateTableHeaderContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<CreateTableHeaderContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token CREATE
/// Returns `None` if there is no child corresponding to token CREATE
fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CREATE, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLE
/// Returns `None` if there is no child corresponding to token TABLE
fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLE, 0)
}
fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token TEMPORARY
/// Returns `None` if there is no child corresponding to token TEMPORARY
fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TEMPORARY, 0)
}
/// Retrieves first TerminalNode corresponding to token EXTERNAL
/// Returns `None` if there is no child corresponding to token EXTERNAL
fn EXTERNAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXTERNAL, 0)
}
/// Retrieves first TerminalNode corresponding to token IF
/// Returns `None` if there is no child corresponding to token IF
fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IF, 0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token EXISTS
/// Returns `None` if there is no child corresponding to token EXISTS
fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXISTS, 0)
}

}

impl<'input> CreateTableHeaderContextAttrs<'input> for CreateTableHeaderContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn createTableHeader(&mut self,)
	-> Result<Rc<CreateTableHeaderContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = CreateTableHeaderContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 22, RULE_createTableHeader);
        let mut _localctx: Rc<CreateTableHeaderContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1222);
			recog.base.match_token(CREATE,&mut recog.err_handler)?;

			recog.base.set_state(1224);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==TEMPORARY {
				{
				recog.base.set_state(1223);
				recog.base.match_token(TEMPORARY,&mut recog.err_handler)?;

				}
			}

			recog.base.set_state(1227);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==EXTERNAL {
				{
				recog.base.set_state(1226);
				recog.base.match_token(EXTERNAL,&mut recog.err_handler)?;

				}
			}

			recog.base.set_state(1229);
			recog.base.match_token(TABLE,&mut recog.err_handler)?;

			recog.base.set_state(1233);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(123,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1230);
					recog.base.match_token(IF,&mut recog.err_handler)?;

					recog.base.set_state(1231);
					recog.base.match_token(NOT,&mut recog.err_handler)?;

					recog.base.set_state(1232);
					recog.base.match_token(EXISTS,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			/*InvokeRule multipartIdentifier*/
			recog.base.set_state(1235);
			recog.multipartIdentifier()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- replaceTableHeader ----------------
pub type ReplaceTableHeaderContextAll<'input> = ReplaceTableHeaderContext<'input>;


pub type ReplaceTableHeaderContext<'input> = BaseParserRuleContext<'input,ReplaceTableHeaderContextExt<'input>>;

#[derive(Clone)]
pub struct ReplaceTableHeaderContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ReplaceTableHeaderContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ReplaceTableHeaderContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_replaceTableHeader(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_replaceTableHeader(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ReplaceTableHeaderContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_replaceTableHeader }
	//fn type_rule_index() -> usize where Self: Sized { RULE_replaceTableHeader }
}
crate::tid!{ReplaceTableHeaderContextExt<'a>}

impl<'input> ReplaceTableHeaderContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ReplaceTableHeaderContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ReplaceTableHeaderContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ReplaceTableHeaderContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ReplaceTableHeaderContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token REPLACE
/// Returns `None` if there is no child corresponding to token REPLACE
fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REPLACE, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLE
/// Returns `None` if there is no child corresponding to token TABLE
fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLE, 0)
}
fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token CREATE
/// Returns `None` if there is no child corresponding to token CREATE
fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CREATE, 0)
}
/// Retrieves first TerminalNode corresponding to token OR
/// Returns `None` if there is no child corresponding to token OR
fn OR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OR, 0)
}

}

impl<'input> ReplaceTableHeaderContextAttrs<'input> for ReplaceTableHeaderContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn replaceTableHeader(&mut self,)
	-> Result<Rc<ReplaceTableHeaderContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ReplaceTableHeaderContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 24, RULE_replaceTableHeader);
        let mut _localctx: Rc<ReplaceTableHeaderContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1239);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==CREATE {
				{
				recog.base.set_state(1237);
				recog.base.match_token(CREATE,&mut recog.err_handler)?;

				recog.base.set_state(1238);
				recog.base.match_token(OR,&mut recog.err_handler)?;

				}
			}

			recog.base.set_state(1241);
			recog.base.match_token(REPLACE,&mut recog.err_handler)?;

			recog.base.set_state(1242);
			recog.base.match_token(TABLE,&mut recog.err_handler)?;

			/*InvokeRule multipartIdentifier*/
			recog.base.set_state(1243);
			recog.multipartIdentifier()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- bucketSpec ----------------
pub type BucketSpecContextAll<'input> = BucketSpecContext<'input>;


pub type BucketSpecContext<'input> = BaseParserRuleContext<'input,BucketSpecContextExt<'input>>;

#[derive(Clone)]
pub struct BucketSpecContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for BucketSpecContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for BucketSpecContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_bucketSpec(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_bucketSpec(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for BucketSpecContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_bucketSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_bucketSpec }
}
crate::tid!{BucketSpecContextExt<'a>}

impl<'input> BucketSpecContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<BucketSpecContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,BucketSpecContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait BucketSpecContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<BucketSpecContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token CLUSTERED
/// Returns `None` if there is no child corresponding to token CLUSTERED
fn CLUSTERED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLUSTERED, 0)
}
/// Retrieves all `TerminalNode`s corresponding to token BY in current rule
fn BY_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token BY, starting from 0.
/// Returns `None` if number of children corresponding to token BY is less or equal than `i`.
fn BY(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, i)
}
fn identifierList(&self) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token INTO
/// Returns `None` if there is no child corresponding to token INTO
fn INTO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTO, 0)
}
/// Retrieves first TerminalNode corresponding to token INTEGER_VALUE
/// Returns `None` if there is no child corresponding to token INTEGER_VALUE
fn INTEGER_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTEGER_VALUE, 0)
}
/// Retrieves first TerminalNode corresponding to token BUCKETS
/// Returns `None` if there is no child corresponding to token BUCKETS
fn BUCKETS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BUCKETS, 0)
}
/// Retrieves first TerminalNode corresponding to token SORTED
/// Returns `None` if there is no child corresponding to token SORTED
fn SORTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SORTED, 0)
}
fn orderedIdentifierList(&self) -> Option<Rc<OrderedIdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> BucketSpecContextAttrs<'input> for BucketSpecContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn bucketSpec(&mut self,)
	-> Result<Rc<BucketSpecContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = BucketSpecContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 26, RULE_bucketSpec);
        let mut _localctx: Rc<BucketSpecContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1245);
			recog.base.match_token(CLUSTERED,&mut recog.err_handler)?;

			recog.base.set_state(1246);
			recog.base.match_token(BY,&mut recog.err_handler)?;

			/*InvokeRule identifierList*/
			recog.base.set_state(1247);
			recog.identifierList()?;

			recog.base.set_state(1251);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==SORTED {
				{
				recog.base.set_state(1248);
				recog.base.match_token(SORTED,&mut recog.err_handler)?;

				recog.base.set_state(1249);
				recog.base.match_token(BY,&mut recog.err_handler)?;

				/*InvokeRule orderedIdentifierList*/
				recog.base.set_state(1250);
				recog.orderedIdentifierList()?;

				}
			}

			recog.base.set_state(1253);
			recog.base.match_token(INTO,&mut recog.err_handler)?;

			recog.base.set_state(1254);
			recog.base.match_token(INTEGER_VALUE,&mut recog.err_handler)?;

			recog.base.set_state(1255);
			recog.base.match_token(BUCKETS,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- skewSpec ----------------
pub type SkewSpecContextAll<'input> = SkewSpecContext<'input>;


pub type SkewSpecContext<'input> = BaseParserRuleContext<'input,SkewSpecContextExt<'input>>;

#[derive(Clone)]
pub struct SkewSpecContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SkewSpecContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SkewSpecContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_skewSpec(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_skewSpec(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SkewSpecContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_skewSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_skewSpec }
}
crate::tid!{SkewSpecContextExt<'a>}

impl<'input> SkewSpecContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SkewSpecContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SkewSpecContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SkewSpecContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SkewSpecContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token SKEWED
/// Returns `None` if there is no child corresponding to token SKEWED
fn SKEWED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SKEWED, 0)
}
/// Retrieves first TerminalNode corresponding to token BY
/// Returns `None` if there is no child corresponding to token BY
fn BY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, 0)
}
fn identifierList(&self) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token ON
/// Returns `None` if there is no child corresponding to token ON
fn ON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ON, 0)
}
fn constantList(&self) -> Option<Rc<ConstantListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn nestedConstantList(&self) -> Option<Rc<NestedConstantListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token STORED
/// Returns `None` if there is no child corresponding to token STORED
fn STORED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STORED, 0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
/// Retrieves first TerminalNode corresponding to token DIRECTORIES
/// Returns `None` if there is no child corresponding to token DIRECTORIES
fn DIRECTORIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIRECTORIES, 0)
}

}

impl<'input> SkewSpecContextAttrs<'input> for SkewSpecContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn skewSpec(&mut self,)
	-> Result<Rc<SkewSpecContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SkewSpecContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 28, RULE_skewSpec);
        let mut _localctx: Rc<SkewSpecContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1257);
			recog.base.match_token(SKEWED,&mut recog.err_handler)?;

			recog.base.set_state(1258);
			recog.base.match_token(BY,&mut recog.err_handler)?;

			/*InvokeRule identifierList*/
			recog.base.set_state(1259);
			recog.identifierList()?;

			recog.base.set_state(1260);
			recog.base.match_token(ON,&mut recog.err_handler)?;

			recog.base.set_state(1263);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(126,&mut recog.base)? {
				1 =>{
					{
					/*InvokeRule constantList*/
					recog.base.set_state(1261);
					recog.constantList()?;

					}
				}
			,
				2 =>{
					{
					/*InvokeRule nestedConstantList*/
					recog.base.set_state(1262);
					recog.nestedConstantList()?;

					}
				}

				_ => {}
			}
			recog.base.set_state(1268);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(127,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1265);
					recog.base.match_token(STORED,&mut recog.err_handler)?;

					recog.base.set_state(1266);
					recog.base.match_token(AS,&mut recog.err_handler)?;

					recog.base.set_state(1267);
					recog.base.match_token(DIRECTORIES,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- locationSpec ----------------
pub type LocationSpecContextAll<'input> = LocationSpecContext<'input>;


pub type LocationSpecContext<'input> = BaseParserRuleContext<'input,LocationSpecContextExt<'input>>;

#[derive(Clone)]
pub struct LocationSpecContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for LocationSpecContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LocationSpecContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_locationSpec(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_locationSpec(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for LocationSpecContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_locationSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_locationSpec }
}
crate::tid!{LocationSpecContextExt<'a>}

impl<'input> LocationSpecContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<LocationSpecContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,LocationSpecContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait LocationSpecContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<LocationSpecContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token LOCATION
/// Returns `None` if there is no child corresponding to token LOCATION
fn LOCATION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCATION, 0)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}

}

impl<'input> LocationSpecContextAttrs<'input> for LocationSpecContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn locationSpec(&mut self,)
	-> Result<Rc<LocationSpecContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = LocationSpecContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 30, RULE_locationSpec);
        let mut _localctx: Rc<LocationSpecContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1270);
			recog.base.match_token(LOCATION,&mut recog.err_handler)?;

			recog.base.set_state(1271);
			recog.base.match_token(STRING,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- commentSpec ----------------
pub type CommentSpecContextAll<'input> = CommentSpecContext<'input>;


pub type CommentSpecContext<'input> = BaseParserRuleContext<'input,CommentSpecContextExt<'input>>;

#[derive(Clone)]
pub struct CommentSpecContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for CommentSpecContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CommentSpecContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_commentSpec(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_commentSpec(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for CommentSpecContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_commentSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_commentSpec }
}
crate::tid!{CommentSpecContextExt<'a>}

impl<'input> CommentSpecContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<CommentSpecContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,CommentSpecContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait CommentSpecContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<CommentSpecContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token COMMENT
/// Returns `None` if there is no child corresponding to token COMMENT
fn COMMENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMMENT, 0)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}

}

impl<'input> CommentSpecContextAttrs<'input> for CommentSpecContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn commentSpec(&mut self,)
	-> Result<Rc<CommentSpecContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = CommentSpecContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 32, RULE_commentSpec);
        let mut _localctx: Rc<CommentSpecContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1273);
			recog.base.match_token(COMMENT,&mut recog.err_handler)?;

			recog.base.set_state(1274);
			recog.base.match_token(STRING,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- query ----------------
pub type QueryContextAll<'input> = QueryContext<'input>;


pub type QueryContext<'input> = BaseParserRuleContext<'input,QueryContextExt<'input>>;

#[derive(Clone)]
pub struct QueryContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_query(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_query(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for QueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_query }
	//fn type_rule_index() -> usize where Self: Sized { RULE_query }
}
crate::tid!{QueryContextExt<'a>}

impl<'input> QueryContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QueryContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QueryContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait QueryContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QueryContextExt<'input>>{

fn queryTerm(&self) -> Option<Rc<QueryTermContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn queryOrganization(&self) -> Option<Rc<QueryOrganizationContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn ctes(&self) -> Option<Rc<CtesContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> QueryContextAttrs<'input> for QueryContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn query(&mut self,)
	-> Result<Rc<QueryContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QueryContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 34, RULE_query);
        let mut _localctx: Rc<QueryContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1277);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==WITH {
				{
				/*InvokeRule ctes*/
				recog.base.set_state(1276);
				recog.ctes()?;

				}
			}

			/*InvokeRule queryTerm*/
			recog.base.set_state(1279);
			recog.queryTerm_rec(0)?;

			/*InvokeRule queryOrganization*/
			recog.base.set_state(1280);
			recog.queryOrganization()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- insertInto ----------------
#[derive(Debug)]
pub enum InsertIntoContextAll<'input>{
	InsertOverwriteHiveDirContext(InsertOverwriteHiveDirContext<'input>),
	InsertOverwriteDirContext(InsertOverwriteDirContext<'input>),
	InsertOverwriteTableContext(InsertOverwriteTableContext<'input>),
	InsertIntoTableContext(InsertIntoTableContext<'input>),
Error(InsertIntoContext<'input>)
}
crate::tid!{InsertIntoContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for InsertIntoContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for InsertIntoContextAll<'input>{}

impl<'input> Deref for InsertIntoContextAll<'input>{
	type Target = dyn InsertIntoContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use InsertIntoContextAll::*;
		match self{
			InsertOverwriteHiveDirContext(inner) => inner,
			InsertOverwriteDirContext(inner) => inner,
			InsertOverwriteTableContext(inner) => inner,
			InsertIntoTableContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InsertIntoContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type InsertIntoContext<'input> = BaseParserRuleContext<'input,InsertIntoContextExt<'input>>;

#[derive(Clone)]
pub struct InsertIntoContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for InsertIntoContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InsertIntoContext<'input>{
}

impl<'input> CustomRuleContext<'input> for InsertIntoContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_insertInto }
	//fn type_rule_index() -> usize where Self: Sized { RULE_insertInto }
}
crate::tid!{InsertIntoContextExt<'a>}

impl<'input> InsertIntoContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<InsertIntoContextAll<'input>> {
		Rc::new(
		InsertIntoContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,InsertIntoContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait InsertIntoContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<InsertIntoContextExt<'input>>{


}

impl<'input> InsertIntoContextAttrs<'input> for InsertIntoContext<'input>{}

pub type InsertOverwriteHiveDirContext<'input> = BaseParserRuleContext<'input,InsertOverwriteHiveDirContextExt<'input>>;

pub trait InsertOverwriteHiveDirContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token INSERT
	/// Returns `None` if there is no child corresponding to token INSERT
	fn INSERT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INSERT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OVERWRITE
	/// Returns `None` if there is no child corresponding to token OVERWRITE
	fn OVERWRITE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OVERWRITE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DIRECTORY
	/// Returns `None` if there is no child corresponding to token DIRECTORY
	fn DIRECTORY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DIRECTORY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LOCAL
	/// Returns `None` if there is no child corresponding to token LOCAL
	fn LOCAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LOCAL, 0)
	}
	fn rowFormat(&self) -> Option<Rc<RowFormatContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn createFileFormat(&self) -> Option<Rc<CreateFileFormatContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> InsertOverwriteHiveDirContextAttrs<'input> for InsertOverwriteHiveDirContext<'input>{}

pub struct InsertOverwriteHiveDirContextExt<'input>{
	base:InsertIntoContextExt<'input>,
	pub path: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{InsertOverwriteHiveDirContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for InsertOverwriteHiveDirContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InsertOverwriteHiveDirContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_insertOverwriteHiveDir(self);
	}
}

impl<'input> CustomRuleContext<'input> for InsertOverwriteHiveDirContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_insertInto }
	//fn type_rule_index() -> usize where Self: Sized { RULE_insertInto }
}

impl<'input> Borrow<InsertIntoContextExt<'input>> for InsertOverwriteHiveDirContext<'input>{
	fn borrow(&self) -> &InsertIntoContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<InsertIntoContextExt<'input>> for InsertOverwriteHiveDirContext<'input>{
	fn borrow_mut(&mut self) -> &mut InsertIntoContextExt<'input> { &mut self.base }
}

impl<'input> InsertIntoContextAttrs<'input> for InsertOverwriteHiveDirContext<'input> {}

impl<'input> InsertOverwriteHiveDirContextExt<'input>{
	fn new(ctx: &dyn InsertIntoContextAttrs<'input>) -> Rc<InsertIntoContextAll<'input>>  {
		Rc::new(
			InsertIntoContextAll::InsertOverwriteHiveDirContext(
				BaseParserRuleContext::copy_from(ctx,InsertOverwriteHiveDirContextExt{
					path:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type InsertOverwriteDirContext<'input> = BaseParserRuleContext<'input,InsertOverwriteDirContextExt<'input>>;

pub trait InsertOverwriteDirContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token INSERT
	/// Returns `None` if there is no child corresponding to token INSERT
	fn INSERT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INSERT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OVERWRITE
	/// Returns `None` if there is no child corresponding to token OVERWRITE
	fn OVERWRITE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OVERWRITE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DIRECTORY
	/// Returns `None` if there is no child corresponding to token DIRECTORY
	fn DIRECTORY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DIRECTORY, 0)
	}
	fn tableProvider(&self) -> Option<Rc<TableProviderContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token LOCAL
	/// Returns `None` if there is no child corresponding to token LOCAL
	fn LOCAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LOCAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OPTIONS
	/// Returns `None` if there is no child corresponding to token OPTIONS
	fn OPTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OPTIONS, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
}

impl<'input> InsertOverwriteDirContextAttrs<'input> for InsertOverwriteDirContext<'input>{}

pub struct InsertOverwriteDirContextExt<'input>{
	base:InsertIntoContextExt<'input>,
	pub path: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{InsertOverwriteDirContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for InsertOverwriteDirContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InsertOverwriteDirContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_insertOverwriteDir(self);
	}
}

impl<'input> CustomRuleContext<'input> for InsertOverwriteDirContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_insertInto }
	//fn type_rule_index() -> usize where Self: Sized { RULE_insertInto }
}

impl<'input> Borrow<InsertIntoContextExt<'input>> for InsertOverwriteDirContext<'input>{
	fn borrow(&self) -> &InsertIntoContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<InsertIntoContextExt<'input>> for InsertOverwriteDirContext<'input>{
	fn borrow_mut(&mut self) -> &mut InsertIntoContextExt<'input> { &mut self.base }
}

impl<'input> InsertIntoContextAttrs<'input> for InsertOverwriteDirContext<'input> {}

impl<'input> InsertOverwriteDirContextExt<'input>{
	fn new(ctx: &dyn InsertIntoContextAttrs<'input>) -> Rc<InsertIntoContextAll<'input>>  {
		Rc::new(
			InsertIntoContextAll::InsertOverwriteDirContext(
				BaseParserRuleContext::copy_from(ctx,InsertOverwriteDirContextExt{
					path:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type InsertOverwriteTableContext<'input> = BaseParserRuleContext<'input,InsertOverwriteTableContextExt<'input>>;

pub trait InsertOverwriteTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token INSERT
	/// Returns `None` if there is no child corresponding to token INSERT
	fn INSERT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INSERT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OVERWRITE
	/// Returns `None` if there is no child corresponding to token OVERWRITE
	fn OVERWRITE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OVERWRITE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
}

impl<'input> InsertOverwriteTableContextAttrs<'input> for InsertOverwriteTableContext<'input>{}

pub struct InsertOverwriteTableContextExt<'input>{
	base:InsertIntoContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{InsertOverwriteTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for InsertOverwriteTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InsertOverwriteTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_insertOverwriteTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for InsertOverwriteTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_insertInto }
	//fn type_rule_index() -> usize where Self: Sized { RULE_insertInto }
}

impl<'input> Borrow<InsertIntoContextExt<'input>> for InsertOverwriteTableContext<'input>{
	fn borrow(&self) -> &InsertIntoContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<InsertIntoContextExt<'input>> for InsertOverwriteTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut InsertIntoContextExt<'input> { &mut self.base }
}

impl<'input> InsertIntoContextAttrs<'input> for InsertOverwriteTableContext<'input> {}

impl<'input> InsertOverwriteTableContextExt<'input>{
	fn new(ctx: &dyn InsertIntoContextAttrs<'input>) -> Rc<InsertIntoContextAll<'input>>  {
		Rc::new(
			InsertIntoContextAll::InsertOverwriteTableContext(
				BaseParserRuleContext::copy_from(ctx,InsertOverwriteTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type InsertIntoTableContext<'input> = BaseParserRuleContext<'input,InsertIntoTableContextExt<'input>>;

pub trait InsertIntoTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token INSERT
	/// Returns `None` if there is no child corresponding to token INSERT
	fn INSERT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INSERT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token INTO
	/// Returns `None` if there is no child corresponding to token INTO
	fn INTO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTO, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IF
	/// Returns `None` if there is no child corresponding to token IF
	fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
}

impl<'input> InsertIntoTableContextAttrs<'input> for InsertIntoTableContext<'input>{}

pub struct InsertIntoTableContextExt<'input>{
	base:InsertIntoContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{InsertIntoTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for InsertIntoTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InsertIntoTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_insertIntoTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for InsertIntoTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_insertInto }
	//fn type_rule_index() -> usize where Self: Sized { RULE_insertInto }
}

impl<'input> Borrow<InsertIntoContextExt<'input>> for InsertIntoTableContext<'input>{
	fn borrow(&self) -> &InsertIntoContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<InsertIntoContextExt<'input>> for InsertIntoTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut InsertIntoContextExt<'input> { &mut self.base }
}

impl<'input> InsertIntoContextAttrs<'input> for InsertIntoTableContext<'input> {}

impl<'input> InsertIntoTableContextExt<'input>{
	fn new(ctx: &dyn InsertIntoContextAttrs<'input>) -> Rc<InsertIntoContextAll<'input>>  {
		Rc::new(
			InsertIntoContextAll::InsertIntoTableContext(
				BaseParserRuleContext::copy_from(ctx,InsertIntoTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn insertInto(&mut self,)
	-> Result<Rc<InsertIntoContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = InsertIntoContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 36, RULE_insertInto);
        let mut _localctx: Rc<InsertIntoContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1337);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(141,&mut recog.base)? {
				1 =>{
					let tmp = InsertOverwriteTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(1282);
					recog.base.match_token(INSERT,&mut recog.err_handler)?;

					recog.base.set_state(1283);
					recog.base.match_token(OVERWRITE,&mut recog.err_handler)?;

					recog.base.set_state(1285);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(129,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1284);
							recog.base.match_token(TABLE,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(1287);
					recog.multipartIdentifier()?;

					recog.base.set_state(1294);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(1288);
						recog.partitionSpec()?;

						recog.base.set_state(1292);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if _la==IF {
							{
							recog.base.set_state(1289);
							recog.base.match_token(IF,&mut recog.err_handler)?;

							recog.base.set_state(1290);
							recog.base.match_token(NOT,&mut recog.err_handler)?;

							recog.base.set_state(1291);
							recog.base.match_token(EXISTS,&mut recog.err_handler)?;

							}
						}

						}
					}

					}
				}
			,
				2 =>{
					let tmp = InsertIntoTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					recog.base.set_state(1296);
					recog.base.match_token(INSERT,&mut recog.err_handler)?;

					recog.base.set_state(1297);
					recog.base.match_token(INTO,&mut recog.err_handler)?;

					recog.base.set_state(1299);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(132,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1298);
							recog.base.match_token(TABLE,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(1301);
					recog.multipartIdentifier()?;

					recog.base.set_state(1303);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PARTITION {
						{
						/*InvokeRule partitionSpec*/
						recog.base.set_state(1302);
						recog.partitionSpec()?;

						}
					}

					recog.base.set_state(1308);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==IF {
						{
						recog.base.set_state(1305);
						recog.base.match_token(IF,&mut recog.err_handler)?;

						recog.base.set_state(1306);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						recog.base.set_state(1307);
						recog.base.match_token(EXISTS,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				3 =>{
					let tmp = InsertOverwriteHiveDirContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(1310);
					recog.base.match_token(INSERT,&mut recog.err_handler)?;

					recog.base.set_state(1311);
					recog.base.match_token(OVERWRITE,&mut recog.err_handler)?;

					recog.base.set_state(1313);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LOCAL {
						{
						recog.base.set_state(1312);
						recog.base.match_token(LOCAL,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(1315);
					recog.base.match_token(DIRECTORY,&mut recog.err_handler)?;

					recog.base.set_state(1316);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					if let InsertIntoContextAll::InsertOverwriteHiveDirContext(ctx) = cast_mut::<_,InsertIntoContextAll >(&mut _localctx){
					ctx.path = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(1318);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==ROW {
						{
						/*InvokeRule rowFormat*/
						recog.base.set_state(1317);
						recog.rowFormat()?;

						}
					}

					recog.base.set_state(1321);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==STORED {
						{
						/*InvokeRule createFileFormat*/
						recog.base.set_state(1320);
						recog.createFileFormat()?;

						}
					}

					}
				}
			,
				4 =>{
					let tmp = InsertOverwriteDirContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					recog.base.set_state(1323);
					recog.base.match_token(INSERT,&mut recog.err_handler)?;

					recog.base.set_state(1324);
					recog.base.match_token(OVERWRITE,&mut recog.err_handler)?;

					recog.base.set_state(1326);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LOCAL {
						{
						recog.base.set_state(1325);
						recog.base.match_token(LOCAL,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(1328);
					recog.base.match_token(DIRECTORY,&mut recog.err_handler)?;

					recog.base.set_state(1330);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==STRING {
						{
						recog.base.set_state(1329);
						let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
						if let InsertIntoContextAll::InsertOverwriteDirContext(ctx) = cast_mut::<_,InsertIntoContextAll >(&mut _localctx){
						ctx.path = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					/*InvokeRule tableProvider*/
					recog.base.set_state(1332);
					recog.tableProvider()?;

					recog.base.set_state(1335);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OPTIONS {
						{
						recog.base.set_state(1333);
						recog.base.match_token(OPTIONS,&mut recog.err_handler)?;

						/*InvokeRule tablePropertyList*/
						recog.base.set_state(1334);
						recog.tablePropertyList()?;

						}
					}

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- partitionSpecLocation ----------------
pub type PartitionSpecLocationContextAll<'input> = PartitionSpecLocationContext<'input>;


pub type PartitionSpecLocationContext<'input> = BaseParserRuleContext<'input,PartitionSpecLocationContextExt<'input>>;

#[derive(Clone)]
pub struct PartitionSpecLocationContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PartitionSpecLocationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PartitionSpecLocationContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_partitionSpecLocation(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_partitionSpecLocation(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PartitionSpecLocationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_partitionSpecLocation }
	//fn type_rule_index() -> usize where Self: Sized { RULE_partitionSpecLocation }
}
crate::tid!{PartitionSpecLocationContextExt<'a>}

impl<'input> PartitionSpecLocationContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PartitionSpecLocationContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PartitionSpecLocationContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait PartitionSpecLocationContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PartitionSpecLocationContextExt<'input>>{

fn partitionSpec(&self) -> Option<Rc<PartitionSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn locationSpec(&self) -> Option<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> PartitionSpecLocationContextAttrs<'input> for PartitionSpecLocationContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn partitionSpecLocation(&mut self,)
	-> Result<Rc<PartitionSpecLocationContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PartitionSpecLocationContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 38, RULE_partitionSpecLocation);
        let mut _localctx: Rc<PartitionSpecLocationContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule partitionSpec*/
			recog.base.set_state(1339);
			recog.partitionSpec()?;

			recog.base.set_state(1341);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==LOCATION {
				{
				/*InvokeRule locationSpec*/
				recog.base.set_state(1340);
				recog.locationSpec()?;

				}
			}

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- partitionSpec ----------------
pub type PartitionSpecContextAll<'input> = PartitionSpecContext<'input>;


pub type PartitionSpecContext<'input> = BaseParserRuleContext<'input,PartitionSpecContextExt<'input>>;

#[derive(Clone)]
pub struct PartitionSpecContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PartitionSpecContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PartitionSpecContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_partitionSpec(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_partitionSpec(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PartitionSpecContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_partitionSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_partitionSpec }
}
crate::tid!{PartitionSpecContextExt<'a>}

impl<'input> PartitionSpecContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PartitionSpecContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PartitionSpecContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait PartitionSpecContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PartitionSpecContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token PARTITION
/// Returns `None` if there is no child corresponding to token PARTITION
fn PARTITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITION, 0)
}
fn partitionVal_all(&self) ->  Vec<Rc<PartitionValContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn partitionVal(&self, i: usize) -> Option<Rc<PartitionValContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> PartitionSpecContextAttrs<'input> for PartitionSpecContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn partitionSpec(&mut self,)
	-> Result<Rc<PartitionSpecContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PartitionSpecContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 40, RULE_partitionSpec);
        let mut _localctx: Rc<PartitionSpecContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1343);
			recog.base.match_token(PARTITION,&mut recog.err_handler)?;

			recog.base.set_state(1344);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule partitionVal*/
			recog.base.set_state(1345);
			recog.partitionVal()?;

			recog.base.set_state(1350);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(1346);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule partitionVal*/
				recog.base.set_state(1347);
				recog.partitionVal()?;

				}
				}
				recog.base.set_state(1352);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(1353);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- partitionVal ----------------
pub type PartitionValContextAll<'input> = PartitionValContext<'input>;


pub type PartitionValContext<'input> = BaseParserRuleContext<'input,PartitionValContextExt<'input>>;

#[derive(Clone)]
pub struct PartitionValContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PartitionValContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PartitionValContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_partitionVal(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_partitionVal(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PartitionValContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_partitionVal }
	//fn type_rule_index() -> usize where Self: Sized { RULE_partitionVal }
}
crate::tid!{PartitionValContextExt<'a>}

impl<'input> PartitionValContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PartitionValContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PartitionValContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait PartitionValContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PartitionValContextExt<'input>>{

fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EQ
/// Returns `None` if there is no child corresponding to token EQ
fn EQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EQ, 0)
}
fn constant(&self) -> Option<Rc<ConstantContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> PartitionValContextAttrs<'input> for PartitionValContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn partitionVal(&mut self,)
	-> Result<Rc<PartitionValContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PartitionValContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 42, RULE_partitionVal);
        let mut _localctx: Rc<PartitionValContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule identifier*/
			recog.base.set_state(1355);
			recog.identifier()?;

			recog.base.set_state(1358);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==EQ {
				{
				recog.base.set_state(1356);
				recog.base.match_token(EQ,&mut recog.err_handler)?;

				/*InvokeRule constant*/
				recog.base.set_state(1357);
				recog.constant()?;

				}
			}

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- namespace ----------------
pub type NamespaceContextAll<'input> = NamespaceContext<'input>;


pub type NamespaceContext<'input> = BaseParserRuleContext<'input,NamespaceContextExt<'input>>;

#[derive(Clone)]
pub struct NamespaceContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NamespaceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NamespaceContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_namespace(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_namespace(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NamespaceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_namespace }
	//fn type_rule_index() -> usize where Self: Sized { RULE_namespace }
}
crate::tid!{NamespaceContextExt<'a>}

impl<'input> NamespaceContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NamespaceContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NamespaceContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait NamespaceContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NamespaceContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token NAMESPACE
/// Returns `None` if there is no child corresponding to token NAMESPACE
fn NAMESPACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NAMESPACE, 0)
}
/// Retrieves first TerminalNode corresponding to token DATABASE
/// Returns `None` if there is no child corresponding to token DATABASE
fn DATABASE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATABASE, 0)
}
/// Retrieves first TerminalNode corresponding to token SCHEMA
/// Returns `None` if there is no child corresponding to token SCHEMA
fn SCHEMA(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SCHEMA, 0)
}

}

impl<'input> NamespaceContextAttrs<'input> for NamespaceContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn namespace(&mut self,)
	-> Result<Rc<NamespaceContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NamespaceContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 44, RULE_namespace);
        let mut _localctx: Rc<NamespaceContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1360);
			_la = recog.base.input.la(1);
			if { !(_la==DATABASE || _la==NAMESPACE || _la==SCHEMA) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- describeFuncName ----------------
pub type DescribeFuncNameContextAll<'input> = DescribeFuncNameContext<'input>;


pub type DescribeFuncNameContext<'input> = BaseParserRuleContext<'input,DescribeFuncNameContextExt<'input>>;

#[derive(Clone)]
pub struct DescribeFuncNameContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for DescribeFuncNameContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DescribeFuncNameContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_describeFuncName(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_describeFuncName(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for DescribeFuncNameContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_describeFuncName }
	//fn type_rule_index() -> usize where Self: Sized { RULE_describeFuncName }
}
crate::tid!{DescribeFuncNameContextExt<'a>}

impl<'input> DescribeFuncNameContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<DescribeFuncNameContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,DescribeFuncNameContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait DescribeFuncNameContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<DescribeFuncNameContextExt<'input>>{

fn qualifiedName(&self) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}
fn comparisonOperator(&self) -> Option<Rc<ComparisonOperatorContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn arithmeticOperator(&self) -> Option<Rc<ArithmeticOperatorContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn predicateOperator(&self) -> Option<Rc<PredicateOperatorContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> DescribeFuncNameContextAttrs<'input> for DescribeFuncNameContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn describeFuncName(&mut self,)
	-> Result<Rc<DescribeFuncNameContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = DescribeFuncNameContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 46, RULE_describeFuncName);
        let mut _localctx: Rc<DescribeFuncNameContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1367);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(145,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule qualifiedName*/
					recog.base.set_state(1362);
					recog.qualifiedName()?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1363);
					recog.base.match_token(STRING,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					/*InvokeRule comparisonOperator*/
					recog.base.set_state(1364);
					recog.comparisonOperator()?;

					}
				}
			,
				4 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					/*InvokeRule arithmeticOperator*/
					recog.base.set_state(1365);
					recog.arithmeticOperator()?;

					}
				}
			,
				5 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 5);
					recog.base.enter_outer_alt(None, 5);
					{
					/*InvokeRule predicateOperator*/
					recog.base.set_state(1366);
					recog.predicateOperator()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- describeColName ----------------
pub type DescribeColNameContextAll<'input> = DescribeColNameContext<'input>;


pub type DescribeColNameContext<'input> = BaseParserRuleContext<'input,DescribeColNameContextExt<'input>>;

#[derive(Clone)]
pub struct DescribeColNameContextExt<'input>{
	pub identifier: Option<Rc<IdentifierContextAll<'input>>>,
	pub nameParts:Vec<Rc<IdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for DescribeColNameContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DescribeColNameContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_describeColName(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_describeColName(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for DescribeColNameContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_describeColName }
	//fn type_rule_index() -> usize where Self: Sized { RULE_describeColName }
}
crate::tid!{DescribeColNameContextExt<'a>}

impl<'input> DescribeColNameContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<DescribeColNameContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,DescribeColNameContextExt{
				identifier: None, 
				nameParts: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait DescribeColNameContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<DescribeColNameContextExt<'input>>{

fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> DescribeColNameContextAttrs<'input> for DescribeColNameContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn describeColName(&mut self,)
	-> Result<Rc<DescribeColNameContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = DescribeColNameContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 48, RULE_describeColName);
        let mut _localctx: Rc<DescribeColNameContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule identifier*/
			recog.base.set_state(1369);
			let tmp = recog.identifier()?;
			 cast_mut::<_,DescribeColNameContext >(&mut _localctx).identifier = Some(tmp.clone());
			  

			let temp =  cast_mut::<_,DescribeColNameContext >(&mut _localctx).identifier.clone().unwrap()
			 ;
			 cast_mut::<_,DescribeColNameContext >(&mut _localctx).nameParts.push(temp);
			  
			recog.base.set_state(1374);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__3 {
				{
				{
				recog.base.set_state(1370);
				recog.base.match_token(T__3,&mut recog.err_handler)?;

				/*InvokeRule identifier*/
				recog.base.set_state(1371);
				let tmp = recog.identifier()?;
				 cast_mut::<_,DescribeColNameContext >(&mut _localctx).identifier = Some(tmp.clone());
				  

				let temp =  cast_mut::<_,DescribeColNameContext >(&mut _localctx).identifier.clone().unwrap()
				 ;
				 cast_mut::<_,DescribeColNameContext >(&mut _localctx).nameParts.push(temp);
				  
				}
				}
				recog.base.set_state(1376);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- ctes ----------------
pub type CtesContextAll<'input> = CtesContext<'input>;


pub type CtesContext<'input> = BaseParserRuleContext<'input,CtesContextExt<'input>>;

#[derive(Clone)]
pub struct CtesContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for CtesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CtesContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_ctes(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_ctes(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for CtesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_ctes }
	//fn type_rule_index() -> usize where Self: Sized { RULE_ctes }
}
crate::tid!{CtesContextExt<'a>}

impl<'input> CtesContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<CtesContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,CtesContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait CtesContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<CtesContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token WITH
/// Returns `None` if there is no child corresponding to token WITH
fn WITH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WITH, 0)
}
fn namedQuery_all(&self) ->  Vec<Rc<NamedQueryContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn namedQuery(&self, i: usize) -> Option<Rc<NamedQueryContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> CtesContextAttrs<'input> for CtesContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn ctes(&mut self,)
	-> Result<Rc<CtesContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = CtesContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 50, RULE_ctes);
        let mut _localctx: Rc<CtesContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1377);
			recog.base.match_token(WITH,&mut recog.err_handler)?;

			/*InvokeRule namedQuery*/
			recog.base.set_state(1378);
			recog.namedQuery()?;

			recog.base.set_state(1383);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(1379);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule namedQuery*/
				recog.base.set_state(1380);
				recog.namedQuery()?;

				}
				}
				recog.base.set_state(1385);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- namedQuery ----------------
pub type NamedQueryContextAll<'input> = NamedQueryContext<'input>;


pub type NamedQueryContext<'input> = BaseParserRuleContext<'input,NamedQueryContextExt<'input>>;

#[derive(Clone)]
pub struct NamedQueryContextExt<'input>{
	pub name: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
	pub columnAliases: Option<Rc<IdentifierListContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NamedQueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NamedQueryContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_namedQuery(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_namedQuery(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NamedQueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_namedQuery }
	//fn type_rule_index() -> usize where Self: Sized { RULE_namedQuery }
}
crate::tid!{NamedQueryContextExt<'a>}

impl<'input> NamedQueryContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NamedQueryContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NamedQueryContextExt{
				name: None, columnAliases: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait NamedQueryContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NamedQueryContextExt<'input>>{

fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
fn identifierList(&self) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> NamedQueryContextAttrs<'input> for NamedQueryContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn namedQuery(&mut self,)
	-> Result<Rc<NamedQueryContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NamedQueryContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 52, RULE_namedQuery);
        let mut _localctx: Rc<NamedQueryContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(1386);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,NamedQueryContext >(&mut _localctx).name = Some(tmp.clone());
			  

			recog.base.set_state(1388);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(148,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule identifierList*/
					recog.base.set_state(1387);
					let tmp = recog.identifierList()?;
					 cast_mut::<_,NamedQueryContext >(&mut _localctx).columnAliases = Some(tmp.clone());
					  

					}
				}

				_ => {}
			}
			recog.base.set_state(1391);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==AS {
				{
				recog.base.set_state(1390);
				recog.base.match_token(AS,&mut recog.err_handler)?;

				}
			}

			recog.base.set_state(1393);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule query*/
			recog.base.set_state(1394);
			recog.query()?;

			recog.base.set_state(1395);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- tableProvider ----------------
pub type TableProviderContextAll<'input> = TableProviderContext<'input>;


pub type TableProviderContext<'input> = BaseParserRuleContext<'input,TableProviderContextExt<'input>>;

#[derive(Clone)]
pub struct TableProviderContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TableProviderContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TableProviderContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_tableProvider(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_tableProvider(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TableProviderContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_tableProvider }
	//fn type_rule_index() -> usize where Self: Sized { RULE_tableProvider }
}
crate::tid!{TableProviderContextExt<'a>}

impl<'input> TableProviderContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TableProviderContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TableProviderContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait TableProviderContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TableProviderContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token USING
/// Returns `None` if there is no child corresponding to token USING
fn USING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(USING, 0)
}
fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> TableProviderContextAttrs<'input> for TableProviderContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn tableProvider(&mut self,)
	-> Result<Rc<TableProviderContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TableProviderContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 54, RULE_tableProvider);
        let mut _localctx: Rc<TableProviderContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1397);
			recog.base.match_token(USING,&mut recog.err_handler)?;

			/*InvokeRule multipartIdentifier*/
			recog.base.set_state(1398);
			recog.multipartIdentifier()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- createTableClauses ----------------
pub type CreateTableClausesContextAll<'input> = CreateTableClausesContext<'input>;


pub type CreateTableClausesContext<'input> = BaseParserRuleContext<'input,CreateTableClausesContextExt<'input>>;

#[derive(Clone)]
pub struct CreateTableClausesContextExt<'input>{
	pub partitioning: Option<Rc<TransformListContextAll<'input>>>,
	pub tableProps: Option<Rc<TablePropertyListContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for CreateTableClausesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateTableClausesContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_createTableClauses(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_createTableClauses(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for CreateTableClausesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_createTableClauses }
	//fn type_rule_index() -> usize where Self: Sized { RULE_createTableClauses }
}
crate::tid!{CreateTableClausesContextExt<'a>}

impl<'input> CreateTableClausesContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<CreateTableClausesContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,CreateTableClausesContextExt{
				partitioning: None, tableProps: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait CreateTableClausesContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<CreateTableClausesContextExt<'input>>{

fn bucketSpec_all(&self) ->  Vec<Rc<BucketSpecContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn bucketSpec(&self, i: usize) -> Option<Rc<BucketSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn locationSpec_all(&self) ->  Vec<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn locationSpec(&self, i: usize) -> Option<Rc<LocationSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn commentSpec_all(&self) ->  Vec<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn commentSpec(&self, i: usize) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves all `TerminalNode`s corresponding to token OPTIONS in current rule
fn OPTIONS_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token OPTIONS, starting from 0.
/// Returns `None` if number of children corresponding to token OPTIONS is less or equal than `i`.
fn OPTIONS(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OPTIONS, i)
}
fn tablePropertyList_all(&self) ->  Vec<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn tablePropertyList(&self, i: usize) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves all `TerminalNode`s corresponding to token PARTITIONED in current rule
fn PARTITIONED_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token PARTITIONED, starting from 0.
/// Returns `None` if number of children corresponding to token PARTITIONED is less or equal than `i`.
fn PARTITIONED(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITIONED, i)
}
/// Retrieves all `TerminalNode`s corresponding to token BY in current rule
fn BY_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token BY, starting from 0.
/// Returns `None` if number of children corresponding to token BY is less or equal than `i`.
fn BY(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, i)
}
/// Retrieves all `TerminalNode`s corresponding to token TBLPROPERTIES in current rule
fn TBLPROPERTIES_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token TBLPROPERTIES, starting from 0.
/// Returns `None` if number of children corresponding to token TBLPROPERTIES is less or equal than `i`.
fn TBLPROPERTIES(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TBLPROPERTIES, i)
}
fn transformList_all(&self) ->  Vec<Rc<TransformListContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn transformList(&self, i: usize) -> Option<Rc<TransformListContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> CreateTableClausesContextAttrs<'input> for CreateTableClausesContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn createTableClauses(&mut self,)
	-> Result<Rc<CreateTableClausesContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = CreateTableClausesContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 56, RULE_createTableClauses);
        let mut _localctx: Rc<CreateTableClausesContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1412);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(151,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					recog.base.set_state(1410);
					recog.err_handler.sync(&mut recog.base)?;
					match recog.base.input.la(1) {
					 OPTIONS 
						=> {
							{
							{
							recog.base.set_state(1400);
							recog.base.match_token(OPTIONS,&mut recog.err_handler)?;

							/*InvokeRule tablePropertyList*/
							recog.base.set_state(1401);
							recog.tablePropertyList()?;

							}
							}
						}

					 PARTITIONED 
						=> {
							{
							{
							recog.base.set_state(1402);
							recog.base.match_token(PARTITIONED,&mut recog.err_handler)?;

							recog.base.set_state(1403);
							recog.base.match_token(BY,&mut recog.err_handler)?;

							/*InvokeRule transformList*/
							recog.base.set_state(1404);
							let tmp = recog.transformList()?;
							 cast_mut::<_,CreateTableClausesContext >(&mut _localctx).partitioning = Some(tmp.clone());
							  

							}
							}
						}

					 CLUSTERED 
						=> {
							{
							/*InvokeRule bucketSpec*/
							recog.base.set_state(1405);
							recog.bucketSpec()?;

							}
						}

					 LOCATION 
						=> {
							{
							/*InvokeRule locationSpec*/
							recog.base.set_state(1406);
							recog.locationSpec()?;

							}
						}

					 COMMENT 
						=> {
							{
							/*InvokeRule commentSpec*/
							recog.base.set_state(1407);
							recog.commentSpec()?;

							}
						}

					 TBLPROPERTIES 
						=> {
							{
							{
							recog.base.set_state(1408);
							recog.base.match_token(TBLPROPERTIES,&mut recog.err_handler)?;

							/*InvokeRule tablePropertyList*/
							recog.base.set_state(1409);
							let tmp = recog.tablePropertyList()?;
							 cast_mut::<_,CreateTableClausesContext >(&mut _localctx).tableProps = Some(tmp.clone());
							  

							}
							}
						}

						_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
					}
					} 
				}
				recog.base.set_state(1414);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(151,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- tablePropertyList ----------------
pub type TablePropertyListContextAll<'input> = TablePropertyListContext<'input>;


pub type TablePropertyListContext<'input> = BaseParserRuleContext<'input,TablePropertyListContextExt<'input>>;

#[derive(Clone)]
pub struct TablePropertyListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TablePropertyListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TablePropertyListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_tablePropertyList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_tablePropertyList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TablePropertyListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_tablePropertyList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_tablePropertyList }
}
crate::tid!{TablePropertyListContextExt<'a>}

impl<'input> TablePropertyListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TablePropertyListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TablePropertyListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait TablePropertyListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TablePropertyListContextExt<'input>>{

fn tableProperty_all(&self) ->  Vec<Rc<TablePropertyContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn tableProperty(&self, i: usize) -> Option<Rc<TablePropertyContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> TablePropertyListContextAttrs<'input> for TablePropertyListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn tablePropertyList(&mut self,)
	-> Result<Rc<TablePropertyListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TablePropertyListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 58, RULE_tablePropertyList);
        let mut _localctx: Rc<TablePropertyListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1415);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule tableProperty*/
			recog.base.set_state(1416);
			recog.tableProperty()?;

			recog.base.set_state(1421);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(1417);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule tableProperty*/
				recog.base.set_state(1418);
				recog.tableProperty()?;

				}
				}
				recog.base.set_state(1423);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(1424);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- tableProperty ----------------
pub type TablePropertyContextAll<'input> = TablePropertyContext<'input>;


pub type TablePropertyContext<'input> = BaseParserRuleContext<'input,TablePropertyContextExt<'input>>;

#[derive(Clone)]
pub struct TablePropertyContextExt<'input>{
	pub key: Option<Rc<TablePropertyKeyContextAll<'input>>>,
	pub value: Option<Rc<TablePropertyValueContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TablePropertyContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TablePropertyContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_tableProperty(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_tableProperty(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TablePropertyContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_tableProperty }
	//fn type_rule_index() -> usize where Self: Sized { RULE_tableProperty }
}
crate::tid!{TablePropertyContextExt<'a>}

impl<'input> TablePropertyContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TablePropertyContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TablePropertyContextExt{
				key: None, value: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait TablePropertyContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TablePropertyContextExt<'input>>{

fn tablePropertyKey(&self) -> Option<Rc<TablePropertyKeyContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn tablePropertyValue(&self) -> Option<Rc<TablePropertyValueContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token EQ
/// Returns `None` if there is no child corresponding to token EQ
fn EQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EQ, 0)
}

}

impl<'input> TablePropertyContextAttrs<'input> for TablePropertyContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn tableProperty(&mut self,)
	-> Result<Rc<TablePropertyContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TablePropertyContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 60, RULE_tableProperty);
        let mut _localctx: Rc<TablePropertyContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule tablePropertyKey*/
			recog.base.set_state(1426);
			let tmp = recog.tablePropertyKey()?;
			 cast_mut::<_,TablePropertyContext >(&mut _localctx).key = Some(tmp.clone());
			  

			recog.base.set_state(1431);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==FALSE || _la==TRUE || _la==EQ || ((((_la - 279)) & !0x3f) == 0 && ((1usize << (_la - 279)) & ((1usize << (STRING - 279)) | (1usize << (INTEGER_VALUE - 279)) | (1usize << (DECIMAL_VALUE - 279)))) != 0) {
				{
				recog.base.set_state(1428);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
				if _la==EQ {
					{
					recog.base.set_state(1427);
					recog.base.match_token(EQ,&mut recog.err_handler)?;

					}
				}

				/*InvokeRule tablePropertyValue*/
				recog.base.set_state(1430);
				let tmp = recog.tablePropertyValue()?;
				 cast_mut::<_,TablePropertyContext >(&mut _localctx).value = Some(tmp.clone());
				  

				}
			}

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- tablePropertyKey ----------------
pub type TablePropertyKeyContextAll<'input> = TablePropertyKeyContext<'input>;


pub type TablePropertyKeyContext<'input> = BaseParserRuleContext<'input,TablePropertyKeyContextExt<'input>>;

#[derive(Clone)]
pub struct TablePropertyKeyContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TablePropertyKeyContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TablePropertyKeyContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_tablePropertyKey(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_tablePropertyKey(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TablePropertyKeyContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_tablePropertyKey }
	//fn type_rule_index() -> usize where Self: Sized { RULE_tablePropertyKey }
}
crate::tid!{TablePropertyKeyContextExt<'a>}

impl<'input> TablePropertyKeyContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TablePropertyKeyContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TablePropertyKeyContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait TablePropertyKeyContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TablePropertyKeyContextExt<'input>>{

fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}

}

impl<'input> TablePropertyKeyContextAttrs<'input> for TablePropertyKeyContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn tablePropertyKey(&mut self,)
	-> Result<Rc<TablePropertyKeyContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TablePropertyKeyContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 62, RULE_tablePropertyKey);
        let mut _localctx: Rc<TablePropertyKeyContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1442);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(156,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule identifier*/
					recog.base.set_state(1433);
					recog.identifier()?;

					recog.base.set_state(1438);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__3 {
						{
						{
						recog.base.set_state(1434);
						recog.base.match_token(T__3,&mut recog.err_handler)?;

						/*InvokeRule identifier*/
						recog.base.set_state(1435);
						recog.identifier()?;

						}
						}
						recog.base.set_state(1440);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1441);
					recog.base.match_token(STRING,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- tablePropertyValue ----------------
pub type TablePropertyValueContextAll<'input> = TablePropertyValueContext<'input>;


pub type TablePropertyValueContext<'input> = BaseParserRuleContext<'input,TablePropertyValueContextExt<'input>>;

#[derive(Clone)]
pub struct TablePropertyValueContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TablePropertyValueContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TablePropertyValueContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_tablePropertyValue(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_tablePropertyValue(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TablePropertyValueContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_tablePropertyValue }
	//fn type_rule_index() -> usize where Self: Sized { RULE_tablePropertyValue }
}
crate::tid!{TablePropertyValueContextExt<'a>}

impl<'input> TablePropertyValueContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TablePropertyValueContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TablePropertyValueContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait TablePropertyValueContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TablePropertyValueContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token INTEGER_VALUE
/// Returns `None` if there is no child corresponding to token INTEGER_VALUE
fn INTEGER_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTEGER_VALUE, 0)
}
/// Retrieves first TerminalNode corresponding to token DECIMAL_VALUE
/// Returns `None` if there is no child corresponding to token DECIMAL_VALUE
fn DECIMAL_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DECIMAL_VALUE, 0)
}
fn booleanValue(&self) -> Option<Rc<BooleanValueContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}

}

impl<'input> TablePropertyValueContextAttrs<'input> for TablePropertyValueContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn tablePropertyValue(&mut self,)
	-> Result<Rc<TablePropertyValueContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TablePropertyValueContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 64, RULE_tablePropertyValue);
        let mut _localctx: Rc<TablePropertyValueContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1448);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 INTEGER_VALUE 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(1444);
					recog.base.match_token(INTEGER_VALUE,&mut recog.err_handler)?;

					}
				}

			 DECIMAL_VALUE 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1445);
					recog.base.match_token(DECIMAL_VALUE,&mut recog.err_handler)?;

					}
				}

			 FALSE | TRUE 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					/*InvokeRule booleanValue*/
					recog.base.set_state(1446);
					recog.booleanValue()?;

					}
				}

			 STRING 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					recog.base.set_state(1447);
					recog.base.match_token(STRING,&mut recog.err_handler)?;

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- constantList ----------------
pub type ConstantListContextAll<'input> = ConstantListContext<'input>;


pub type ConstantListContext<'input> = BaseParserRuleContext<'input,ConstantListContextExt<'input>>;

#[derive(Clone)]
pub struct ConstantListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ConstantListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ConstantListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_constantList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_constantList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ConstantListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constantList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constantList }
}
crate::tid!{ConstantListContextExt<'a>}

impl<'input> ConstantListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ConstantListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ConstantListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ConstantListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ConstantListContextExt<'input>>{

fn constant_all(&self) ->  Vec<Rc<ConstantContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn constant(&self, i: usize) -> Option<Rc<ConstantContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> ConstantListContextAttrs<'input> for ConstantListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn constantList(&mut self,)
	-> Result<Rc<ConstantListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ConstantListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 66, RULE_constantList);
        let mut _localctx: Rc<ConstantListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1450);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule constant*/
			recog.base.set_state(1451);
			recog.constant()?;

			recog.base.set_state(1456);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(1452);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule constant*/
				recog.base.set_state(1453);
				recog.constant()?;

				}
				}
				recog.base.set_state(1458);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(1459);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- nestedConstantList ----------------
pub type NestedConstantListContextAll<'input> = NestedConstantListContext<'input>;


pub type NestedConstantListContext<'input> = BaseParserRuleContext<'input,NestedConstantListContextExt<'input>>;

#[derive(Clone)]
pub struct NestedConstantListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NestedConstantListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NestedConstantListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_nestedConstantList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_nestedConstantList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NestedConstantListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_nestedConstantList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_nestedConstantList }
}
crate::tid!{NestedConstantListContextExt<'a>}

impl<'input> NestedConstantListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NestedConstantListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NestedConstantListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait NestedConstantListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NestedConstantListContextExt<'input>>{

fn constantList_all(&self) ->  Vec<Rc<ConstantListContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn constantList(&self, i: usize) -> Option<Rc<ConstantListContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> NestedConstantListContextAttrs<'input> for NestedConstantListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn nestedConstantList(&mut self,)
	-> Result<Rc<NestedConstantListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NestedConstantListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 68, RULE_nestedConstantList);
        let mut _localctx: Rc<NestedConstantListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1461);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule constantList*/
			recog.base.set_state(1462);
			recog.constantList()?;

			recog.base.set_state(1467);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(1463);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule constantList*/
				recog.base.set_state(1464);
				recog.constantList()?;

				}
				}
				recog.base.set_state(1469);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(1470);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- createFileFormat ----------------
pub type CreateFileFormatContextAll<'input> = CreateFileFormatContext<'input>;


pub type CreateFileFormatContext<'input> = BaseParserRuleContext<'input,CreateFileFormatContextExt<'input>>;

#[derive(Clone)]
pub struct CreateFileFormatContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for CreateFileFormatContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CreateFileFormatContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_createFileFormat(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_createFileFormat(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for CreateFileFormatContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_createFileFormat }
	//fn type_rule_index() -> usize where Self: Sized { RULE_createFileFormat }
}
crate::tid!{CreateFileFormatContextExt<'a>}

impl<'input> CreateFileFormatContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<CreateFileFormatContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,CreateFileFormatContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait CreateFileFormatContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<CreateFileFormatContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token STORED
/// Returns `None` if there is no child corresponding to token STORED
fn STORED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STORED, 0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
fn fileFormat(&self) -> Option<Rc<FileFormatContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token BY
/// Returns `None` if there is no child corresponding to token BY
fn BY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, 0)
}
fn storageHandler(&self) -> Option<Rc<StorageHandlerContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> CreateFileFormatContextAttrs<'input> for CreateFileFormatContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn createFileFormat(&mut self,)
	-> Result<Rc<CreateFileFormatContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = CreateFileFormatContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 70, RULE_createFileFormat);
        let mut _localctx: Rc<CreateFileFormatContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1478);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(160,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(1472);
					recog.base.match_token(STORED,&mut recog.err_handler)?;

					recog.base.set_state(1473);
					recog.base.match_token(AS,&mut recog.err_handler)?;

					/*InvokeRule fileFormat*/
					recog.base.set_state(1474);
					recog.fileFormat()?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1475);
					recog.base.match_token(STORED,&mut recog.err_handler)?;

					recog.base.set_state(1476);
					recog.base.match_token(BY,&mut recog.err_handler)?;

					/*InvokeRule storageHandler*/
					recog.base.set_state(1477);
					recog.storageHandler()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- fileFormat ----------------
#[derive(Debug)]
pub enum FileFormatContextAll<'input>{
	TableFileFormatContext(TableFileFormatContext<'input>),
	GenericFileFormatContext(GenericFileFormatContext<'input>),
Error(FileFormatContext<'input>)
}
crate::tid!{FileFormatContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for FileFormatContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for FileFormatContextAll<'input>{}

impl<'input> Deref for FileFormatContextAll<'input>{
	type Target = dyn FileFormatContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use FileFormatContextAll::*;
		match self{
			TableFileFormatContext(inner) => inner,
			GenericFileFormatContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FileFormatContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type FileFormatContext<'input> = BaseParserRuleContext<'input,FileFormatContextExt<'input>>;

#[derive(Clone)]
pub struct FileFormatContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for FileFormatContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FileFormatContext<'input>{
}

impl<'input> CustomRuleContext<'input> for FileFormatContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_fileFormat }
	//fn type_rule_index() -> usize where Self: Sized { RULE_fileFormat }
}
crate::tid!{FileFormatContextExt<'a>}

impl<'input> FileFormatContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<FileFormatContextAll<'input>> {
		Rc::new(
		FileFormatContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,FileFormatContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait FileFormatContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<FileFormatContextExt<'input>>{


}

impl<'input> FileFormatContextAttrs<'input> for FileFormatContext<'input>{}

pub type TableFileFormatContext<'input> = BaseParserRuleContext<'input,TableFileFormatContextExt<'input>>;

pub trait TableFileFormatContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token INPUTFORMAT
	/// Returns `None` if there is no child corresponding to token INPUTFORMAT
	fn INPUTFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INPUTFORMAT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OUTPUTFORMAT
	/// Returns `None` if there is no child corresponding to token OUTPUTFORMAT
	fn OUTPUTFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OUTPUTFORMAT, 0)
	}
	/// Retrieves all `TerminalNode`s corresponding to token STRING in current rule
	fn STRING_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token STRING, starting from 0.
	/// Returns `None` if number of children corresponding to token STRING is less or equal than `i`.
	fn STRING(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, i)
	}
}

impl<'input> TableFileFormatContextAttrs<'input> for TableFileFormatContext<'input>{}

pub struct TableFileFormatContextExt<'input>{
	base:FileFormatContextExt<'input>,
	pub inFmt: Option<TokenType<'input>>,
	pub outFmt: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{TableFileFormatContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TableFileFormatContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TableFileFormatContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_tableFileFormat(self);
	}
}

impl<'input> CustomRuleContext<'input> for TableFileFormatContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_fileFormat }
	//fn type_rule_index() -> usize where Self: Sized { RULE_fileFormat }
}

impl<'input> Borrow<FileFormatContextExt<'input>> for TableFileFormatContext<'input>{
	fn borrow(&self) -> &FileFormatContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<FileFormatContextExt<'input>> for TableFileFormatContext<'input>{
	fn borrow_mut(&mut self) -> &mut FileFormatContextExt<'input> { &mut self.base }
}

impl<'input> FileFormatContextAttrs<'input> for TableFileFormatContext<'input> {}

impl<'input> TableFileFormatContextExt<'input>{
	fn new(ctx: &dyn FileFormatContextAttrs<'input>) -> Rc<FileFormatContextAll<'input>>  {
		Rc::new(
			FileFormatContextAll::TableFileFormatContext(
				BaseParserRuleContext::copy_from(ctx,TableFileFormatContextExt{
					inFmt:None, outFmt:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type GenericFileFormatContext<'input> = BaseParserRuleContext<'input,GenericFileFormatContextExt<'input>>;

pub trait GenericFileFormatContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> GenericFileFormatContextAttrs<'input> for GenericFileFormatContext<'input>{}

pub struct GenericFileFormatContextExt<'input>{
	base:FileFormatContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{GenericFileFormatContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for GenericFileFormatContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for GenericFileFormatContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_genericFileFormat(self);
	}
}

impl<'input> CustomRuleContext<'input> for GenericFileFormatContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_fileFormat }
	//fn type_rule_index() -> usize where Self: Sized { RULE_fileFormat }
}

impl<'input> Borrow<FileFormatContextExt<'input>> for GenericFileFormatContext<'input>{
	fn borrow(&self) -> &FileFormatContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<FileFormatContextExt<'input>> for GenericFileFormatContext<'input>{
	fn borrow_mut(&mut self) -> &mut FileFormatContextExt<'input> { &mut self.base }
}

impl<'input> FileFormatContextAttrs<'input> for GenericFileFormatContext<'input> {}

impl<'input> GenericFileFormatContextExt<'input>{
	fn new(ctx: &dyn FileFormatContextAttrs<'input>) -> Rc<FileFormatContextAll<'input>>  {
		Rc::new(
			FileFormatContextAll::GenericFileFormatContext(
				BaseParserRuleContext::copy_from(ctx,GenericFileFormatContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn fileFormat(&mut self,)
	-> Result<Rc<FileFormatContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = FileFormatContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 72, RULE_fileFormat);
        let mut _localctx: Rc<FileFormatContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1485);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(161,&mut recog.base)? {
				1 =>{
					let tmp = TableFileFormatContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(1480);
					recog.base.match_token(INPUTFORMAT,&mut recog.err_handler)?;

					recog.base.set_state(1481);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					if let FileFormatContextAll::TableFileFormatContext(ctx) = cast_mut::<_,FileFormatContextAll >(&mut _localctx){
					ctx.inFmt = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(1482);
					recog.base.match_token(OUTPUTFORMAT,&mut recog.err_handler)?;

					recog.base.set_state(1483);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					if let FileFormatContextAll::TableFileFormatContext(ctx) = cast_mut::<_,FileFormatContextAll >(&mut _localctx){
					ctx.outFmt = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}
			,
				2 =>{
					let tmp = GenericFileFormatContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule identifier*/
					recog.base.set_state(1484);
					recog.identifier()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- storageHandler ----------------
pub type StorageHandlerContextAll<'input> = StorageHandlerContext<'input>;


pub type StorageHandlerContext<'input> = BaseParserRuleContext<'input,StorageHandlerContextExt<'input>>;

#[derive(Clone)]
pub struct StorageHandlerContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for StorageHandlerContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StorageHandlerContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_storageHandler(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_storageHandler(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for StorageHandlerContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_storageHandler }
	//fn type_rule_index() -> usize where Self: Sized { RULE_storageHandler }
}
crate::tid!{StorageHandlerContextExt<'a>}

impl<'input> StorageHandlerContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<StorageHandlerContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,StorageHandlerContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait StorageHandlerContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<StorageHandlerContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}
/// Retrieves first TerminalNode corresponding to token WITH
/// Returns `None` if there is no child corresponding to token WITH
fn WITH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WITH, 0)
}
/// Retrieves first TerminalNode corresponding to token SERDEPROPERTIES
/// Returns `None` if there is no child corresponding to token SERDEPROPERTIES
fn SERDEPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SERDEPROPERTIES, 0)
}
fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> StorageHandlerContextAttrs<'input> for StorageHandlerContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn storageHandler(&mut self,)
	-> Result<Rc<StorageHandlerContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = StorageHandlerContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 74, RULE_storageHandler);
        let mut _localctx: Rc<StorageHandlerContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1487);
			recog.base.match_token(STRING,&mut recog.err_handler)?;

			recog.base.set_state(1491);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(162,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1488);
					recog.base.match_token(WITH,&mut recog.err_handler)?;

					recog.base.set_state(1489);
					recog.base.match_token(SERDEPROPERTIES,&mut recog.err_handler)?;

					/*InvokeRule tablePropertyList*/
					recog.base.set_state(1490);
					recog.tablePropertyList()?;

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- resource ----------------
pub type ResourceContextAll<'input> = ResourceContext<'input>;


pub type ResourceContext<'input> = BaseParserRuleContext<'input,ResourceContextExt<'input>>;

#[derive(Clone)]
pub struct ResourceContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ResourceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ResourceContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_resource(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_resource(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ResourceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_resource }
	//fn type_rule_index() -> usize where Self: Sized { RULE_resource }
}
crate::tid!{ResourceContextExt<'a>}

impl<'input> ResourceContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ResourceContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ResourceContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ResourceContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ResourceContextExt<'input>>{

fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}

}

impl<'input> ResourceContextAttrs<'input> for ResourceContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn resource(&mut self,)
	-> Result<Rc<ResourceContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ResourceContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 76, RULE_resource);
        let mut _localctx: Rc<ResourceContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule identifier*/
			recog.base.set_state(1493);
			recog.identifier()?;

			recog.base.set_state(1494);
			recog.base.match_token(STRING,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- dmlStatementNoWith ----------------
#[derive(Debug)]
pub enum DmlStatementNoWithContextAll<'input>{
	DeleteFromTableContext(DeleteFromTableContext<'input>),
	SingleInsertQueryContext(SingleInsertQueryContext<'input>),
	MultiInsertQueryContext(MultiInsertQueryContext<'input>),
	UpdateTableContext(UpdateTableContext<'input>),
	MergeIntoTableContext(MergeIntoTableContext<'input>),
Error(DmlStatementNoWithContext<'input>)
}
crate::tid!{DmlStatementNoWithContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for DmlStatementNoWithContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for DmlStatementNoWithContextAll<'input>{}

impl<'input> Deref for DmlStatementNoWithContextAll<'input>{
	type Target = dyn DmlStatementNoWithContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use DmlStatementNoWithContextAll::*;
		match self{
			DeleteFromTableContext(inner) => inner,
			SingleInsertQueryContext(inner) => inner,
			MultiInsertQueryContext(inner) => inner,
			UpdateTableContext(inner) => inner,
			MergeIntoTableContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DmlStatementNoWithContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type DmlStatementNoWithContext<'input> = BaseParserRuleContext<'input,DmlStatementNoWithContextExt<'input>>;

#[derive(Clone)]
pub struct DmlStatementNoWithContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for DmlStatementNoWithContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DmlStatementNoWithContext<'input>{
}

impl<'input> CustomRuleContext<'input> for DmlStatementNoWithContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dmlStatementNoWith }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dmlStatementNoWith }
}
crate::tid!{DmlStatementNoWithContextExt<'a>}

impl<'input> DmlStatementNoWithContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<DmlStatementNoWithContextAll<'input>> {
		Rc::new(
		DmlStatementNoWithContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,DmlStatementNoWithContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait DmlStatementNoWithContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<DmlStatementNoWithContextExt<'input>>{


}

impl<'input> DmlStatementNoWithContextAttrs<'input> for DmlStatementNoWithContext<'input>{}

pub type DeleteFromTableContext<'input> = BaseParserRuleContext<'input,DeleteFromTableContextExt<'input>>;

pub trait DeleteFromTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token DELETE
	/// Returns `None` if there is no child corresponding to token DELETE
	fn DELETE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DELETE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableAlias(&self) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn whereClause(&self) -> Option<Rc<WhereClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> DeleteFromTableContextAttrs<'input> for DeleteFromTableContext<'input>{}

pub struct DeleteFromTableContextExt<'input>{
	base:DmlStatementNoWithContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DeleteFromTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DeleteFromTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DeleteFromTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_deleteFromTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for DeleteFromTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dmlStatementNoWith }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dmlStatementNoWith }
}

impl<'input> Borrow<DmlStatementNoWithContextExt<'input>> for DeleteFromTableContext<'input>{
	fn borrow(&self) -> &DmlStatementNoWithContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<DmlStatementNoWithContextExt<'input>> for DeleteFromTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut DmlStatementNoWithContextExt<'input> { &mut self.base }
}

impl<'input> DmlStatementNoWithContextAttrs<'input> for DeleteFromTableContext<'input> {}

impl<'input> DeleteFromTableContextExt<'input>{
	fn new(ctx: &dyn DmlStatementNoWithContextAttrs<'input>) -> Rc<DmlStatementNoWithContextAll<'input>>  {
		Rc::new(
			DmlStatementNoWithContextAll::DeleteFromTableContext(
				BaseParserRuleContext::copy_from(ctx,DeleteFromTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SingleInsertQueryContext<'input> = BaseParserRuleContext<'input,SingleInsertQueryContextExt<'input>>;

pub trait SingleInsertQueryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn insertInto(&self) -> Option<Rc<InsertIntoContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn queryTerm(&self) -> Option<Rc<QueryTermContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn queryOrganization(&self) -> Option<Rc<QueryOrganizationContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SingleInsertQueryContextAttrs<'input> for SingleInsertQueryContext<'input>{}

pub struct SingleInsertQueryContextExt<'input>{
	base:DmlStatementNoWithContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SingleInsertQueryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SingleInsertQueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SingleInsertQueryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_singleInsertQuery(self);
	}
}

impl<'input> CustomRuleContext<'input> for SingleInsertQueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dmlStatementNoWith }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dmlStatementNoWith }
}

impl<'input> Borrow<DmlStatementNoWithContextExt<'input>> for SingleInsertQueryContext<'input>{
	fn borrow(&self) -> &DmlStatementNoWithContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<DmlStatementNoWithContextExt<'input>> for SingleInsertQueryContext<'input>{
	fn borrow_mut(&mut self) -> &mut DmlStatementNoWithContextExt<'input> { &mut self.base }
}

impl<'input> DmlStatementNoWithContextAttrs<'input> for SingleInsertQueryContext<'input> {}

impl<'input> SingleInsertQueryContextExt<'input>{
	fn new(ctx: &dyn DmlStatementNoWithContextAttrs<'input>) -> Rc<DmlStatementNoWithContextAll<'input>>  {
		Rc::new(
			DmlStatementNoWithContextAll::SingleInsertQueryContext(
				BaseParserRuleContext::copy_from(ctx,SingleInsertQueryContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type MultiInsertQueryContext<'input> = BaseParserRuleContext<'input,MultiInsertQueryContextExt<'input>>;

pub trait MultiInsertQueryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn fromClause(&self) -> Option<Rc<FromClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn multiInsertQueryBody_all(&self) ->  Vec<Rc<MultiInsertQueryBodyContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn multiInsertQueryBody(&self, i: usize) -> Option<Rc<MultiInsertQueryBodyContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> MultiInsertQueryContextAttrs<'input> for MultiInsertQueryContext<'input>{}

pub struct MultiInsertQueryContextExt<'input>{
	base:DmlStatementNoWithContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{MultiInsertQueryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for MultiInsertQueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MultiInsertQueryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_multiInsertQuery(self);
	}
}

impl<'input> CustomRuleContext<'input> for MultiInsertQueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dmlStatementNoWith }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dmlStatementNoWith }
}

impl<'input> Borrow<DmlStatementNoWithContextExt<'input>> for MultiInsertQueryContext<'input>{
	fn borrow(&self) -> &DmlStatementNoWithContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<DmlStatementNoWithContextExt<'input>> for MultiInsertQueryContext<'input>{
	fn borrow_mut(&mut self) -> &mut DmlStatementNoWithContextExt<'input> { &mut self.base }
}

impl<'input> DmlStatementNoWithContextAttrs<'input> for MultiInsertQueryContext<'input> {}

impl<'input> MultiInsertQueryContextExt<'input>{
	fn new(ctx: &dyn DmlStatementNoWithContextAttrs<'input>) -> Rc<DmlStatementNoWithContextAll<'input>>  {
		Rc::new(
			DmlStatementNoWithContextAll::MultiInsertQueryContext(
				BaseParserRuleContext::copy_from(ctx,MultiInsertQueryContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type UpdateTableContext<'input> = BaseParserRuleContext<'input,UpdateTableContextExt<'input>>;

pub trait UpdateTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token UPDATE
	/// Returns `None` if there is no child corresponding to token UPDATE
	fn UPDATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(UPDATE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableAlias(&self) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn setClause(&self) -> Option<Rc<SetClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn whereClause(&self) -> Option<Rc<WhereClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> UpdateTableContextAttrs<'input> for UpdateTableContext<'input>{}

pub struct UpdateTableContextExt<'input>{
	base:DmlStatementNoWithContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{UpdateTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for UpdateTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for UpdateTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_updateTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for UpdateTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dmlStatementNoWith }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dmlStatementNoWith }
}

impl<'input> Borrow<DmlStatementNoWithContextExt<'input>> for UpdateTableContext<'input>{
	fn borrow(&self) -> &DmlStatementNoWithContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<DmlStatementNoWithContextExt<'input>> for UpdateTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut DmlStatementNoWithContextExt<'input> { &mut self.base }
}

impl<'input> DmlStatementNoWithContextAttrs<'input> for UpdateTableContext<'input> {}

impl<'input> UpdateTableContextExt<'input>{
	fn new(ctx: &dyn DmlStatementNoWithContextAttrs<'input>) -> Rc<DmlStatementNoWithContextAll<'input>>  {
		Rc::new(
			DmlStatementNoWithContextAll::UpdateTableContext(
				BaseParserRuleContext::copy_from(ctx,UpdateTableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type MergeIntoTableContext<'input> = BaseParserRuleContext<'input,MergeIntoTableContextExt<'input>>;

pub trait MergeIntoTableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token MERGE
	/// Returns `None` if there is no child corresponding to token MERGE
	fn MERGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MERGE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token INTO
	/// Returns `None` if there is no child corresponding to token INTO
	fn INTO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTO, 0)
	}
	/// Retrieves first TerminalNode corresponding to token USING
	/// Returns `None` if there is no child corresponding to token USING
	fn USING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(USING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ON
	/// Returns `None` if there is no child corresponding to token ON
	fn ON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ON, 0)
	}
	fn multipartIdentifier_all(&self) ->  Vec<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn multipartIdentifier(&self, i: usize) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn tableAlias_all(&self) ->  Vec<Rc<TableAliasContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn tableAlias(&self, i: usize) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn matchedClause_all(&self) ->  Vec<Rc<MatchedClauseContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn matchedClause(&self, i: usize) -> Option<Rc<MatchedClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn notMatchedClause_all(&self) ->  Vec<Rc<NotMatchedClauseContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn notMatchedClause(&self, i: usize) -> Option<Rc<NotMatchedClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> MergeIntoTableContextAttrs<'input> for MergeIntoTableContext<'input>{}

pub struct MergeIntoTableContextExt<'input>{
	base:DmlStatementNoWithContextExt<'input>,
	pub target: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub targetAlias: Option<Rc<TableAliasContextAll<'input>>>,
	pub source: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub sourceQuery: Option<Rc<QueryContextAll<'input>>>,
	pub sourceAlias: Option<Rc<TableAliasContextAll<'input>>>,
	pub mergeCondition: Option<Rc<BooleanExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{MergeIntoTableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for MergeIntoTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MergeIntoTableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_mergeIntoTable(self);
	}
}

impl<'input> CustomRuleContext<'input> for MergeIntoTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dmlStatementNoWith }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dmlStatementNoWith }
}

impl<'input> Borrow<DmlStatementNoWithContextExt<'input>> for MergeIntoTableContext<'input>{
	fn borrow(&self) -> &DmlStatementNoWithContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<DmlStatementNoWithContextExt<'input>> for MergeIntoTableContext<'input>{
	fn borrow_mut(&mut self) -> &mut DmlStatementNoWithContextExt<'input> { &mut self.base }
}

impl<'input> DmlStatementNoWithContextAttrs<'input> for MergeIntoTableContext<'input> {}

impl<'input> MergeIntoTableContextExt<'input>{
	fn new(ctx: &dyn DmlStatementNoWithContextAttrs<'input>) -> Rc<DmlStatementNoWithContextAll<'input>>  {
		Rc::new(
			DmlStatementNoWithContextAll::MergeIntoTableContext(
				BaseParserRuleContext::copy_from(ctx,MergeIntoTableContextExt{
        			target:None, targetAlias:None, source:None, sourceQuery:None, sourceAlias:None, mergeCondition:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn dmlStatementNoWith(&mut self,)
	-> Result<Rc<DmlStatementNoWithContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = DmlStatementNoWithContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 78, RULE_dmlStatementNoWith);
        let mut _localctx: Rc<DmlStatementNoWithContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			recog.base.set_state(1547);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 INSERT 
				=> {
					let tmp = SingleInsertQueryContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					/*InvokeRule insertInto*/
					recog.base.set_state(1496);
					recog.insertInto()?;

					/*InvokeRule queryTerm*/
					recog.base.set_state(1497);
					recog.queryTerm_rec(0)?;

					/*InvokeRule queryOrganization*/
					recog.base.set_state(1498);
					recog.queryOrganization()?;

					}
				}

			 FROM 
				=> {
					let tmp = MultiInsertQueryContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule fromClause*/
					recog.base.set_state(1500);
					recog.fromClause()?;

					recog.base.set_state(1502); 
					recog.err_handler.sync(&mut recog.base)?;
					_alt = 1;
					loop {
						match _alt {
						    x if x == 1=>
							{
							{
							/*InvokeRule multiInsertQueryBody*/
							recog.base.set_state(1501);
							recog.multiInsertQueryBody()?;

							}
							}

						_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
						}
						recog.base.set_state(1504); 
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(163,&mut recog.base)?;
						if _alt==2 || _alt==INVALID_ALT { break }
					}
					}
				}

			 DELETE 
				=> {
					let tmp = DeleteFromTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(1506);
					recog.base.match_token(DELETE,&mut recog.err_handler)?;

					recog.base.set_state(1507);
					recog.base.match_token(FROM,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(1508);
					recog.multipartIdentifier()?;

					/*InvokeRule tableAlias*/
					recog.base.set_state(1509);
					recog.tableAlias()?;

					recog.base.set_state(1511);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==WHERE {
						{
						/*InvokeRule whereClause*/
						recog.base.set_state(1510);
						recog.whereClause()?;

						}
					}

					}
				}

			 UPDATE 
				=> {
					let tmp = UpdateTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					recog.base.set_state(1513);
					recog.base.match_token(UPDATE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(1514);
					recog.multipartIdentifier()?;

					/*InvokeRule tableAlias*/
					recog.base.set_state(1515);
					recog.tableAlias()?;

					/*InvokeRule setClause*/
					recog.base.set_state(1516);
					recog.setClause()?;

					recog.base.set_state(1518);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==WHERE {
						{
						/*InvokeRule whereClause*/
						recog.base.set_state(1517);
						recog.whereClause()?;

						}
					}

					}
				}

			 MERGE 
				=> {
					let tmp = MergeIntoTableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 5);
					_localctx = tmp;
					{
					recog.base.set_state(1520);
					recog.base.match_token(MERGE,&mut recog.err_handler)?;

					recog.base.set_state(1521);
					recog.base.match_token(INTO,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(1522);
					let tmp = recog.multipartIdentifier()?;
					if let DmlStatementNoWithContextAll::MergeIntoTableContext(ctx) = cast_mut::<_,DmlStatementNoWithContextAll >(&mut _localctx){
					ctx.target = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					/*InvokeRule tableAlias*/
					recog.base.set_state(1523);
					let tmp = recog.tableAlias()?;
					if let DmlStatementNoWithContextAll::MergeIntoTableContext(ctx) = cast_mut::<_,DmlStatementNoWithContextAll >(&mut _localctx){
					ctx.targetAlias = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(1524);
					recog.base.match_token(USING,&mut recog.err_handler)?;

					recog.base.set_state(1530);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(166,&mut recog.base)? {
						1 =>{
							{
							/*InvokeRule multipartIdentifier*/
							recog.base.set_state(1525);
							let tmp = recog.multipartIdentifier()?;
							if let DmlStatementNoWithContextAll::MergeIntoTableContext(ctx) = cast_mut::<_,DmlStatementNoWithContextAll >(&mut _localctx){
							ctx.source = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						2 =>{
							{
							recog.base.set_state(1526);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							/*InvokeRule query*/
							recog.base.set_state(1527);
							let tmp = recog.query()?;
							if let DmlStatementNoWithContextAll::MergeIntoTableContext(ctx) = cast_mut::<_,DmlStatementNoWithContextAll >(&mut _localctx){
							ctx.sourceQuery = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							recog.base.set_state(1528);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule tableAlias*/
					recog.base.set_state(1532);
					let tmp = recog.tableAlias()?;
					if let DmlStatementNoWithContextAll::MergeIntoTableContext(ctx) = cast_mut::<_,DmlStatementNoWithContextAll >(&mut _localctx){
					ctx.sourceAlias = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(1533);
					recog.base.match_token(ON,&mut recog.err_handler)?;

					/*InvokeRule booleanExpression*/
					recog.base.set_state(1534);
					let tmp = recog.booleanExpression_rec(0)?;
					if let DmlStatementNoWithContextAll::MergeIntoTableContext(ctx) = cast_mut::<_,DmlStatementNoWithContextAll >(&mut _localctx){
					ctx.mergeCondition = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(1538);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(167,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							/*InvokeRule matchedClause*/
							recog.base.set_state(1535);
							recog.matchedClause()?;

							}
							} 
						}
						recog.base.set_state(1540);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(167,&mut recog.base)?;
					}
					recog.base.set_state(1544);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==WHEN {
						{
						{
						/*InvokeRule notMatchedClause*/
						recog.base.set_state(1541);
						recog.notMatchedClause()?;

						}
						}
						recog.base.set_state(1546);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- queryOrganization ----------------
pub type QueryOrganizationContextAll<'input> = QueryOrganizationContext<'input>;


pub type QueryOrganizationContext<'input> = BaseParserRuleContext<'input,QueryOrganizationContextExt<'input>>;

#[derive(Clone)]
pub struct QueryOrganizationContextExt<'input>{
	pub sortItem: Option<Rc<SortItemContextAll<'input>>>,
	pub order:Vec<Rc<SortItemContextAll<'input>>>,
	pub expression: Option<Rc<ExpressionContextAll<'input>>>,
	pub clusterBy:Vec<Rc<ExpressionContextAll<'input>>>,
	pub distributeBy:Vec<Rc<ExpressionContextAll<'input>>>,
	pub sort:Vec<Rc<SortItemContextAll<'input>>>,
	pub limit: Option<Rc<ExpressionContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QueryOrganizationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryOrganizationContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_queryOrganization(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_queryOrganization(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for QueryOrganizationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryOrganization }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryOrganization }
}
crate::tid!{QueryOrganizationContextExt<'a>}

impl<'input> QueryOrganizationContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QueryOrganizationContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QueryOrganizationContextExt{
				sortItem: None, expression: None, limit: None, 
				order: Vec::new(), clusterBy: Vec::new(), distributeBy: Vec::new(), sort: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait QueryOrganizationContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QueryOrganizationContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token ORDER
/// Returns `None` if there is no child corresponding to token ORDER
fn ORDER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ORDER, 0)
}
/// Retrieves all `TerminalNode`s corresponding to token BY in current rule
fn BY_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token BY, starting from 0.
/// Returns `None` if number of children corresponding to token BY is less or equal than `i`.
fn BY(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, i)
}
/// Retrieves first TerminalNode corresponding to token CLUSTER
/// Returns `None` if there is no child corresponding to token CLUSTER
fn CLUSTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLUSTER, 0)
}
/// Retrieves first TerminalNode corresponding to token DISTRIBUTE
/// Returns `None` if there is no child corresponding to token DISTRIBUTE
fn DISTRIBUTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DISTRIBUTE, 0)
}
/// Retrieves first TerminalNode corresponding to token SORT
/// Returns `None` if there is no child corresponding to token SORT
fn SORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SORT, 0)
}
fn windowClause(&self) -> Option<Rc<WindowClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token LIMIT
/// Returns `None` if there is no child corresponding to token LIMIT
fn LIMIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIMIT, 0)
}
fn sortItem_all(&self) ->  Vec<Rc<SortItemContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn sortItem(&self, i: usize) -> Option<Rc<SortItemContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token ALL
/// Returns `None` if there is no child corresponding to token ALL
fn ALL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ALL, 0)
}

}

impl<'input> QueryOrganizationContextAttrs<'input> for QueryOrganizationContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn queryOrganization(&mut self,)
	-> Result<Rc<QueryOrganizationContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QueryOrganizationContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 80, RULE_queryOrganization);
        let mut _localctx: Rc<QueryOrganizationContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1559);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(171,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1549);
					recog.base.match_token(ORDER,&mut recog.err_handler)?;

					recog.base.set_state(1550);
					recog.base.match_token(BY,&mut recog.err_handler)?;

					/*InvokeRule sortItem*/
					recog.base.set_state(1551);
					let tmp = recog.sortItem()?;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem.clone().unwrap()
					 ;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).order.push(temp);
					  
					recog.base.set_state(1556);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(170,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							recog.base.set_state(1552);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							/*InvokeRule sortItem*/
							recog.base.set_state(1553);
							let tmp = recog.sortItem()?;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem = Some(tmp.clone());
							  

							let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem.clone().unwrap()
							 ;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).order.push(temp);
							  
							}
							} 
						}
						recog.base.set_state(1558);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(170,&mut recog.base)?;
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(1571);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(173,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1561);
					recog.base.match_token(CLUSTER,&mut recog.err_handler)?;

					recog.base.set_state(1562);
					recog.base.match_token(BY,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(1563);
					let tmp = recog.expression()?;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression.clone().unwrap()
					 ;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).clusterBy.push(temp);
					  
					recog.base.set_state(1568);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(172,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							recog.base.set_state(1564);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							/*InvokeRule expression*/
							recog.base.set_state(1565);
							let tmp = recog.expression()?;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression = Some(tmp.clone());
							  

							let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression.clone().unwrap()
							 ;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).clusterBy.push(temp);
							  
							}
							} 
						}
						recog.base.set_state(1570);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(172,&mut recog.base)?;
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(1583);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(175,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1573);
					recog.base.match_token(DISTRIBUTE,&mut recog.err_handler)?;

					recog.base.set_state(1574);
					recog.base.match_token(BY,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(1575);
					let tmp = recog.expression()?;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression.clone().unwrap()
					 ;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).distributeBy.push(temp);
					  
					recog.base.set_state(1580);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(174,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							recog.base.set_state(1576);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							/*InvokeRule expression*/
							recog.base.set_state(1577);
							let tmp = recog.expression()?;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression = Some(tmp.clone());
							  

							let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).expression.clone().unwrap()
							 ;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).distributeBy.push(temp);
							  
							}
							} 
						}
						recog.base.set_state(1582);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(174,&mut recog.base)?;
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(1595);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(177,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1585);
					recog.base.match_token(SORT,&mut recog.err_handler)?;

					recog.base.set_state(1586);
					recog.base.match_token(BY,&mut recog.err_handler)?;

					/*InvokeRule sortItem*/
					recog.base.set_state(1587);
					let tmp = recog.sortItem()?;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem.clone().unwrap()
					 ;
					 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sort.push(temp);
					  
					recog.base.set_state(1592);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(176,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							recog.base.set_state(1588);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							/*InvokeRule sortItem*/
							recog.base.set_state(1589);
							let tmp = recog.sortItem()?;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem = Some(tmp.clone());
							  

							let temp =  cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sortItem.clone().unwrap()
							 ;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).sort.push(temp);
							  
							}
							} 
						}
						recog.base.set_state(1594);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(176,&mut recog.base)?;
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(1598);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(178,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule windowClause*/
					recog.base.set_state(1597);
					recog.windowClause()?;

					}
				}

				_ => {}
			}
			recog.base.set_state(1605);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(180,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1600);
					recog.base.match_token(LIMIT,&mut recog.err_handler)?;

					recog.base.set_state(1603);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(179,&mut recog.base)? {
						1 =>{
							{
							recog.base.set_state(1601);
							recog.base.match_token(ALL,&mut recog.err_handler)?;

							}
						}
					,
						2 =>{
							{
							/*InvokeRule expression*/
							recog.base.set_state(1602);
							let tmp = recog.expression()?;
							 cast_mut::<_,QueryOrganizationContext >(&mut _localctx).limit = Some(tmp.clone());
							  

							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- multiInsertQueryBody ----------------
pub type MultiInsertQueryBodyContextAll<'input> = MultiInsertQueryBodyContext<'input>;


pub type MultiInsertQueryBodyContext<'input> = BaseParserRuleContext<'input,MultiInsertQueryBodyContextExt<'input>>;

#[derive(Clone)]
pub struct MultiInsertQueryBodyContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for MultiInsertQueryBodyContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MultiInsertQueryBodyContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_multiInsertQueryBody(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_multiInsertQueryBody(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for MultiInsertQueryBodyContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_multiInsertQueryBody }
	//fn type_rule_index() -> usize where Self: Sized { RULE_multiInsertQueryBody }
}
crate::tid!{MultiInsertQueryBodyContextExt<'a>}

impl<'input> MultiInsertQueryBodyContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<MultiInsertQueryBodyContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,MultiInsertQueryBodyContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait MultiInsertQueryBodyContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<MultiInsertQueryBodyContextExt<'input>>{

fn insertInto(&self) -> Option<Rc<InsertIntoContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn fromStatementBody(&self) -> Option<Rc<FromStatementBodyContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> MultiInsertQueryBodyContextAttrs<'input> for MultiInsertQueryBodyContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn multiInsertQueryBody(&mut self,)
	-> Result<Rc<MultiInsertQueryBodyContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = MultiInsertQueryBodyContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 82, RULE_multiInsertQueryBody);
        let mut _localctx: Rc<MultiInsertQueryBodyContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule insertInto*/
			recog.base.set_state(1607);
			recog.insertInto()?;

			/*InvokeRule fromStatementBody*/
			recog.base.set_state(1608);
			recog.fromStatementBody()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- queryTerm ----------------
#[derive(Debug)]
pub enum QueryTermContextAll<'input>{
	QueryTermDefaultContext(QueryTermDefaultContext<'input>),
	SetOperationContext(SetOperationContext<'input>),
Error(QueryTermContext<'input>)
}
crate::tid!{QueryTermContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for QueryTermContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for QueryTermContextAll<'input>{}

impl<'input> Deref for QueryTermContextAll<'input>{
	type Target = dyn QueryTermContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use QueryTermContextAll::*;
		match self{
			QueryTermDefaultContext(inner) => inner,
			SetOperationContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryTermContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type QueryTermContext<'input> = BaseParserRuleContext<'input,QueryTermContextExt<'input>>;

#[derive(Clone)]
pub struct QueryTermContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QueryTermContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryTermContext<'input>{
}

impl<'input> CustomRuleContext<'input> for QueryTermContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryTerm }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryTerm }
}
crate::tid!{QueryTermContextExt<'a>}

impl<'input> QueryTermContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QueryTermContextAll<'input>> {
		Rc::new(
		QueryTermContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QueryTermContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait QueryTermContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QueryTermContextExt<'input>>{


}

impl<'input> QueryTermContextAttrs<'input> for QueryTermContext<'input>{}

pub type QueryTermDefaultContext<'input> = BaseParserRuleContext<'input,QueryTermDefaultContextExt<'input>>;

pub trait QueryTermDefaultContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn queryPrimary(&self) -> Option<Rc<QueryPrimaryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> QueryTermDefaultContextAttrs<'input> for QueryTermDefaultContext<'input>{}

pub struct QueryTermDefaultContextExt<'input>{
	base:QueryTermContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{QueryTermDefaultContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for QueryTermDefaultContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryTermDefaultContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_queryTermDefault(self);
	}
}

impl<'input> CustomRuleContext<'input> for QueryTermDefaultContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryTerm }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryTerm }
}

impl<'input> Borrow<QueryTermContextExt<'input>> for QueryTermDefaultContext<'input>{
	fn borrow(&self) -> &QueryTermContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QueryTermContextExt<'input>> for QueryTermDefaultContext<'input>{
	fn borrow_mut(&mut self) -> &mut QueryTermContextExt<'input> { &mut self.base }
}

impl<'input> QueryTermContextAttrs<'input> for QueryTermDefaultContext<'input> {}

impl<'input> QueryTermDefaultContextExt<'input>{
	fn new(ctx: &dyn QueryTermContextAttrs<'input>) -> Rc<QueryTermContextAll<'input>>  {
		Rc::new(
			QueryTermContextAll::QueryTermDefaultContext(
				BaseParserRuleContext::copy_from(ctx,QueryTermDefaultContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SetOperationContext<'input> = BaseParserRuleContext<'input,SetOperationContextExt<'input>>;

pub trait SetOperationContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn queryTerm_all(&self) ->  Vec<Rc<QueryTermContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn queryTerm(&self, i: usize) -> Option<Rc<QueryTermContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token INTERSECT
	/// Returns `None` if there is no child corresponding to token INTERSECT
	fn INTERSECT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTERSECT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token UNION
	/// Returns `None` if there is no child corresponding to token UNION
	fn UNION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(UNION, 0)
	}
	/// Retrieves first TerminalNode corresponding to token EXCEPT
	/// Returns `None` if there is no child corresponding to token EXCEPT
	fn EXCEPT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXCEPT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SETMINUS
	/// Returns `None` if there is no child corresponding to token SETMINUS
	fn SETMINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SETMINUS, 0)
	}
	fn setQuantifier(&self) -> Option<Rc<SetQuantifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SetOperationContextAttrs<'input> for SetOperationContext<'input>{}

pub struct SetOperationContextExt<'input>{
	base:QueryTermContextExt<'input>,
	pub left: Option<Rc<QueryTermContextAll<'input>>>,
	pub operator: Option<TokenType<'input>>,
	pub right: Option<Rc<QueryTermContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SetOperationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SetOperationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetOperationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_setOperation(self);
	}
}

impl<'input> CustomRuleContext<'input> for SetOperationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryTerm }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryTerm }
}

impl<'input> Borrow<QueryTermContextExt<'input>> for SetOperationContext<'input>{
	fn borrow(&self) -> &QueryTermContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QueryTermContextExt<'input>> for SetOperationContext<'input>{
	fn borrow_mut(&mut self) -> &mut QueryTermContextExt<'input> { &mut self.base }
}

impl<'input> QueryTermContextAttrs<'input> for SetOperationContext<'input> {}

impl<'input> SetOperationContextExt<'input>{
	fn new(ctx: &dyn QueryTermContextAttrs<'input>) -> Rc<QueryTermContextAll<'input>>  {
		Rc::new(
			QueryTermContextAll::SetOperationContext(
				BaseParserRuleContext::copy_from(ctx,SetOperationContextExt{
					operator:None, 
        			left:None, right:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn  queryTerm(&mut self,)
	-> Result<Rc<QueryTermContextAll<'input>>,ANTLRError> {
		self.queryTerm_rec(0)
	}

	fn queryTerm_rec(&mut self, _p: isize)
	-> Result<Rc<QueryTermContextAll<'input>>,ANTLRError> {
		let recog = self;
		let _parentctx = recog.ctx.take();
		let _parentState = recog.base.get_state();
		let mut _localctx = QueryTermContextExt::new(_parentctx.clone(), recog.base.get_state());
		recog.base.enter_recursion_rule(_localctx.clone(), 84, RULE_queryTerm, _p);
	    let mut _localctx: Rc<QueryTermContextAll> = _localctx;
        let mut _prevctx = _localctx.clone();
		let _startState = 84;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {
			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			{
			let mut tmp = QueryTermDefaultContextExt::new(&**_localctx);
			recog.ctx = Some(tmp.clone());
			_localctx = tmp;
			_prevctx = _localctx.clone();


			/*InvokeRule queryPrimary*/
			recog.base.set_state(1611);
			recog.queryPrimary()?;

			}

			let tmp = recog.input.lt(-1).cloned();
			recog.ctx.as_ref().unwrap().set_stop(tmp);
			recog.base.set_state(1636);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(185,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					recog.trigger_exit_rule_event();
					_prevctx = _localctx.clone();
					{
					recog.base.set_state(1634);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(184,&mut recog.base)? {
						1 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = SetOperationContextExt::new(&**QueryTermContextExt::new(_parentctx.clone(), _parentState));
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_queryTerm);
							_localctx = tmp;
							recog.base.set_state(1613);
							if !({recog.precpred(None, 3)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 3)".to_owned()), None))?;
							}
							recog.base.set_state(1614);
							if !({recog.legacy_setops_precedence_enbled}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.,legacy_setops_precedence_enbled".to_owned()), None))?;
							}
							recog.base.set_state(1615);
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
							ctx.operator = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
							_la = recog.base.input.la(1);
							if { !(_la==EXCEPT || _la==INTERSECT || _la==SETMINUS || _la==UNION) } {
								let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
								if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
								ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							recog.base.set_state(1617);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==ALL || _la==DISTINCT {
								{
								/*InvokeRule setQuantifier*/
								recog.base.set_state(1616);
								recog.setQuantifier()?;

								}
							}

							/*InvokeRule queryTerm*/
							recog.base.set_state(1619);
							let tmp = recog.queryTerm_rec(4)?;
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						2 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = SetOperationContextExt::new(&**QueryTermContextExt::new(_parentctx.clone(), _parentState));
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_queryTerm);
							_localctx = tmp;
							recog.base.set_state(1620);
							if !({recog.precpred(None, 2)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 2)".to_owned()), None))?;
							}
							recog.base.set_state(1621);
							if !({!recog.legacy_setops_precedence_enbled}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("!recog.,legacy_setops_precedence_enbled".to_owned()), None))?;
							}
							recog.base.set_state(1622);
							let tmp = recog.base.match_token(INTERSECT,&mut recog.err_handler)?;
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
							ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							recog.base.set_state(1624);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==ALL || _la==DISTINCT {
								{
								/*InvokeRule setQuantifier*/
								recog.base.set_state(1623);
								recog.setQuantifier()?;

								}
							}

							/*InvokeRule queryTerm*/
							recog.base.set_state(1626);
							let tmp = recog.queryTerm_rec(3)?;
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						3 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = SetOperationContextExt::new(&**QueryTermContextExt::new(_parentctx.clone(), _parentState));
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_queryTerm);
							_localctx = tmp;
							recog.base.set_state(1627);
							if !({recog.precpred(None, 1)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 1)".to_owned()), None))?;
							}
							recog.base.set_state(1628);
							if !({!recog.legacy_setops_precedence_enbled}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("!recog.,legacy_setops_precedence_enbled".to_owned()), None))?;
							}
							recog.base.set_state(1629);
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
							ctx.operator = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
							_la = recog.base.input.la(1);
							if { !(_la==EXCEPT || _la==SETMINUS || _la==UNION) } {
								let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
								if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
								ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							recog.base.set_state(1631);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==ALL || _la==DISTINCT {
								{
								/*InvokeRule setQuantifier*/
								recog.base.set_state(1630);
								recog.setQuantifier()?;

								}
							}

							/*InvokeRule queryTerm*/
							recog.base.set_state(1633);
							let tmp = recog.queryTerm_rec(2)?;
							if let QueryTermContextAll::SetOperationContext(ctx) = cast_mut::<_,QueryTermContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					} 
				}
				recog.base.set_state(1638);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(185,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_) => {},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re)=>{
			//_localctx.exception = re;
			recog.err_handler.report_error(&mut recog.base, re);
	        recog.err_handler.recover(&mut recog.base, re)?;}
		}
		recog.base.unroll_recursion_context(_parentctx);

		Ok(_localctx)
	}
}
//------------------- queryPrimary ----------------
#[derive(Debug)]
pub enum QueryPrimaryContextAll<'input>{
	SubqueryContext(SubqueryContext<'input>),
	QueryPrimaryDefaultContext(QueryPrimaryDefaultContext<'input>),
	InlineTableDefault1Context(InlineTableDefault1Context<'input>),
	FromStmtContext(FromStmtContext<'input>),
	TableContext(TableContext<'input>),
Error(QueryPrimaryContext<'input>)
}
crate::tid!{QueryPrimaryContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for QueryPrimaryContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for QueryPrimaryContextAll<'input>{}

impl<'input> Deref for QueryPrimaryContextAll<'input>{
	type Target = dyn QueryPrimaryContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use QueryPrimaryContextAll::*;
		match self{
			SubqueryContext(inner) => inner,
			QueryPrimaryDefaultContext(inner) => inner,
			InlineTableDefault1Context(inner) => inner,
			FromStmtContext(inner) => inner,
			TableContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryPrimaryContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type QueryPrimaryContext<'input> = BaseParserRuleContext<'input,QueryPrimaryContextExt<'input>>;

#[derive(Clone)]
pub struct QueryPrimaryContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QueryPrimaryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryPrimaryContext<'input>{
}

impl<'input> CustomRuleContext<'input> for QueryPrimaryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryPrimary }
}
crate::tid!{QueryPrimaryContextExt<'a>}

impl<'input> QueryPrimaryContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QueryPrimaryContextAll<'input>> {
		Rc::new(
		QueryPrimaryContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QueryPrimaryContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait QueryPrimaryContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QueryPrimaryContextExt<'input>>{


}

impl<'input> QueryPrimaryContextAttrs<'input> for QueryPrimaryContext<'input>{}

pub type SubqueryContext<'input> = BaseParserRuleContext<'input,SubqueryContextExt<'input>>;

pub trait SubqueryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SubqueryContextAttrs<'input> for SubqueryContext<'input>{}

pub struct SubqueryContextExt<'input>{
	base:QueryPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SubqueryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SubqueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SubqueryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_subquery(self);
	}
}

impl<'input> CustomRuleContext<'input> for SubqueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryPrimary }
}

impl<'input> Borrow<QueryPrimaryContextExt<'input>> for SubqueryContext<'input>{
	fn borrow(&self) -> &QueryPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QueryPrimaryContextExt<'input>> for SubqueryContext<'input>{
	fn borrow_mut(&mut self) -> &mut QueryPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> QueryPrimaryContextAttrs<'input> for SubqueryContext<'input> {}

impl<'input> SubqueryContextExt<'input>{
	fn new(ctx: &dyn QueryPrimaryContextAttrs<'input>) -> Rc<QueryPrimaryContextAll<'input>>  {
		Rc::new(
			QueryPrimaryContextAll::SubqueryContext(
				BaseParserRuleContext::copy_from(ctx,SubqueryContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type QueryPrimaryDefaultContext<'input> = BaseParserRuleContext<'input,QueryPrimaryDefaultContextExt<'input>>;

pub trait QueryPrimaryDefaultContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn querySpecification(&self) -> Option<Rc<QuerySpecificationContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> QueryPrimaryDefaultContextAttrs<'input> for QueryPrimaryDefaultContext<'input>{}

pub struct QueryPrimaryDefaultContextExt<'input>{
	base:QueryPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{QueryPrimaryDefaultContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for QueryPrimaryDefaultContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QueryPrimaryDefaultContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_queryPrimaryDefault(self);
	}
}

impl<'input> CustomRuleContext<'input> for QueryPrimaryDefaultContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryPrimary }
}

impl<'input> Borrow<QueryPrimaryContextExt<'input>> for QueryPrimaryDefaultContext<'input>{
	fn borrow(&self) -> &QueryPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QueryPrimaryContextExt<'input>> for QueryPrimaryDefaultContext<'input>{
	fn borrow_mut(&mut self) -> &mut QueryPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> QueryPrimaryContextAttrs<'input> for QueryPrimaryDefaultContext<'input> {}

impl<'input> QueryPrimaryDefaultContextExt<'input>{
	fn new(ctx: &dyn QueryPrimaryContextAttrs<'input>) -> Rc<QueryPrimaryContextAll<'input>>  {
		Rc::new(
			QueryPrimaryContextAll::QueryPrimaryDefaultContext(
				BaseParserRuleContext::copy_from(ctx,QueryPrimaryDefaultContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type InlineTableDefault1Context<'input> = BaseParserRuleContext<'input,InlineTableDefault1ContextExt<'input>>;

pub trait InlineTableDefault1ContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn inlineTable(&self) -> Option<Rc<InlineTableContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> InlineTableDefault1ContextAttrs<'input> for InlineTableDefault1Context<'input>{}

pub struct InlineTableDefault1ContextExt<'input>{
	base:QueryPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{InlineTableDefault1ContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for InlineTableDefault1Context<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InlineTableDefault1Context<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_inlineTableDefault1(self);
	}
}

impl<'input> CustomRuleContext<'input> for InlineTableDefault1ContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryPrimary }
}

impl<'input> Borrow<QueryPrimaryContextExt<'input>> for InlineTableDefault1Context<'input>{
	fn borrow(&self) -> &QueryPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QueryPrimaryContextExt<'input>> for InlineTableDefault1Context<'input>{
	fn borrow_mut(&mut self) -> &mut QueryPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> QueryPrimaryContextAttrs<'input> for InlineTableDefault1Context<'input> {}

impl<'input> InlineTableDefault1ContextExt<'input>{
	fn new(ctx: &dyn QueryPrimaryContextAttrs<'input>) -> Rc<QueryPrimaryContextAll<'input>>  {
		Rc::new(
			QueryPrimaryContextAll::InlineTableDefault1Context(
				BaseParserRuleContext::copy_from(ctx,InlineTableDefault1ContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type FromStmtContext<'input> = BaseParserRuleContext<'input,FromStmtContextExt<'input>>;

pub trait FromStmtContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn fromStatement(&self) -> Option<Rc<FromStatementContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> FromStmtContextAttrs<'input> for FromStmtContext<'input>{}

pub struct FromStmtContextExt<'input>{
	base:QueryPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{FromStmtContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for FromStmtContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FromStmtContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_fromStmt(self);
	}
}

impl<'input> CustomRuleContext<'input> for FromStmtContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryPrimary }
}

impl<'input> Borrow<QueryPrimaryContextExt<'input>> for FromStmtContext<'input>{
	fn borrow(&self) -> &QueryPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QueryPrimaryContextExt<'input>> for FromStmtContext<'input>{
	fn borrow_mut(&mut self) -> &mut QueryPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> QueryPrimaryContextAttrs<'input> for FromStmtContext<'input> {}

impl<'input> FromStmtContextExt<'input>{
	fn new(ctx: &dyn QueryPrimaryContextAttrs<'input>) -> Rc<QueryPrimaryContextAll<'input>>  {
		Rc::new(
			QueryPrimaryContextAll::FromStmtContext(
				BaseParserRuleContext::copy_from(ctx,FromStmtContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type TableContext<'input> = BaseParserRuleContext<'input,TableContextExt<'input>>;

pub trait TableContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token TABLE
	/// Returns `None` if there is no child corresponding to token TABLE
	fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TABLE, 0)
	}
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> TableContextAttrs<'input> for TableContext<'input>{}

pub struct TableContextExt<'input>{
	base:QueryPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{TableContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TableContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_table(self);
	}
}

impl<'input> CustomRuleContext<'input> for TableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_queryPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_queryPrimary }
}

impl<'input> Borrow<QueryPrimaryContextExt<'input>> for TableContext<'input>{
	fn borrow(&self) -> &QueryPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QueryPrimaryContextExt<'input>> for TableContext<'input>{
	fn borrow_mut(&mut self) -> &mut QueryPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> QueryPrimaryContextAttrs<'input> for TableContext<'input> {}

impl<'input> TableContextExt<'input>{
	fn new(ctx: &dyn QueryPrimaryContextAttrs<'input>) -> Rc<QueryPrimaryContextAll<'input>>  {
		Rc::new(
			QueryPrimaryContextAll::TableContext(
				BaseParserRuleContext::copy_from(ctx,TableContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn queryPrimary(&mut self,)
	-> Result<Rc<QueryPrimaryContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QueryPrimaryContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 86, RULE_queryPrimary);
        let mut _localctx: Rc<QueryPrimaryContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1648);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 MAP | REDUCE | SELECT 
				=> {
					let tmp = QueryPrimaryDefaultContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					/*InvokeRule querySpecification*/
					recog.base.set_state(1639);
					recog.querySpecification()?;

					}
				}

			 FROM 
				=> {
					let tmp = FromStmtContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule fromStatement*/
					recog.base.set_state(1640);
					recog.fromStatement()?;

					}
				}

			 TABLE 
				=> {
					let tmp = TableContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(1641);
					recog.base.match_token(TABLE,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(1642);
					recog.multipartIdentifier()?;

					}
				}

			 VALUES 
				=> {
					let tmp = InlineTableDefault1ContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					/*InvokeRule inlineTable*/
					recog.base.set_state(1643);
					recog.inlineTable()?;

					}
				}

			 T__0 
				=> {
					let tmp = SubqueryContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 5);
					_localctx = tmp;
					{
					recog.base.set_state(1644);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule query*/
					recog.base.set_state(1645);
					recog.query()?;

					recog.base.set_state(1646);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- sortItem ----------------
pub type SortItemContextAll<'input> = SortItemContext<'input>;


pub type SortItemContext<'input> = BaseParserRuleContext<'input,SortItemContextExt<'input>>;

#[derive(Clone)]
pub struct SortItemContextExt<'input>{
	pub ordering: Option<TokenType<'input>>,
	pub nullOrder: Option<TokenType<'input>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SortItemContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SortItemContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_sortItem(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_sortItem(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SortItemContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_sortItem }
	//fn type_rule_index() -> usize where Self: Sized { RULE_sortItem }
}
crate::tid!{SortItemContextExt<'a>}

impl<'input> SortItemContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SortItemContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SortItemContextExt{
				ordering: None, nullOrder: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait SortItemContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SortItemContextExt<'input>>{

fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token NULLS
/// Returns `None` if there is no child corresponding to token NULLS
fn NULLS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULLS, 0)
}
/// Retrieves first TerminalNode corresponding to token ASC
/// Returns `None` if there is no child corresponding to token ASC
fn ASC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ASC, 0)
}
/// Retrieves first TerminalNode corresponding to token DESC
/// Returns `None` if there is no child corresponding to token DESC
fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DESC, 0)
}
/// Retrieves first TerminalNode corresponding to token LAST
/// Returns `None` if there is no child corresponding to token LAST
fn LAST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LAST, 0)
}
/// Retrieves first TerminalNode corresponding to token FIRST
/// Returns `None` if there is no child corresponding to token FIRST
fn FIRST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FIRST, 0)
}

}

impl<'input> SortItemContextAttrs<'input> for SortItemContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn sortItem(&mut self,)
	-> Result<Rc<SortItemContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SortItemContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 88, RULE_sortItem);
        let mut _localctx: Rc<SortItemContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule expression*/
			recog.base.set_state(1650);
			recog.expression()?;

			recog.base.set_state(1652);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(187,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1651);
					 cast_mut::<_,SortItemContext >(&mut _localctx).ordering = recog.base.input.lt(1).cloned();
					 
					_la = recog.base.input.la(1);
					if { !(_la==ASC || _la==DESC) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						 cast_mut::<_,SortItemContext >(&mut _localctx).ordering = Some(tmp.clone());
						  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(1656);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(188,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1654);
					recog.base.match_token(NULLS,&mut recog.err_handler)?;

					recog.base.set_state(1655);
					 cast_mut::<_,SortItemContext >(&mut _localctx).nullOrder = recog.base.input.lt(1).cloned();
					 
					_la = recog.base.input.la(1);
					if { !(_la==FIRST || _la==LAST) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						 cast_mut::<_,SortItemContext >(&mut _localctx).nullOrder = Some(tmp.clone());
						  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- fromStatement ----------------
pub type FromStatementContextAll<'input> = FromStatementContext<'input>;


pub type FromStatementContext<'input> = BaseParserRuleContext<'input,FromStatementContextExt<'input>>;

#[derive(Clone)]
pub struct FromStatementContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for FromStatementContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FromStatementContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_fromStatement(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_fromStatement(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for FromStatementContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_fromStatement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_fromStatement }
}
crate::tid!{FromStatementContextExt<'a>}

impl<'input> FromStatementContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<FromStatementContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,FromStatementContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait FromStatementContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<FromStatementContextExt<'input>>{

fn fromClause(&self) -> Option<Rc<FromClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn fromStatementBody_all(&self) ->  Vec<Rc<FromStatementBodyContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn fromStatementBody(&self, i: usize) -> Option<Rc<FromStatementBodyContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> FromStatementContextAttrs<'input> for FromStatementContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn fromStatement(&mut self,)
	-> Result<Rc<FromStatementContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = FromStatementContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 90, RULE_fromStatement);
        let mut _localctx: Rc<FromStatementContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule fromClause*/
			recog.base.set_state(1658);
			recog.fromClause()?;

			recog.base.set_state(1660); 
			recog.err_handler.sync(&mut recog.base)?;
			_alt = 1;
			loop {
				match _alt {
				    x if x == 1=>
					{
					{
					/*InvokeRule fromStatementBody*/
					recog.base.set_state(1659);
					recog.fromStatementBody()?;

					}
					}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
				}
				recog.base.set_state(1662); 
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(189,&mut recog.base)?;
				if _alt==2 || _alt==INVALID_ALT { break }
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- fromStatementBody ----------------
pub type FromStatementBodyContextAll<'input> = FromStatementBodyContext<'input>;


pub type FromStatementBodyContext<'input> = BaseParserRuleContext<'input,FromStatementBodyContextExt<'input>>;

#[derive(Clone)]
pub struct FromStatementBodyContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for FromStatementBodyContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FromStatementBodyContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_fromStatementBody(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_fromStatementBody(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for FromStatementBodyContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_fromStatementBody }
	//fn type_rule_index() -> usize where Self: Sized { RULE_fromStatementBody }
}
crate::tid!{FromStatementBodyContextExt<'a>}

impl<'input> FromStatementBodyContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<FromStatementBodyContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,FromStatementBodyContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait FromStatementBodyContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<FromStatementBodyContextExt<'input>>{

fn transformClause(&self) -> Option<Rc<TransformClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn queryOrganization(&self) -> Option<Rc<QueryOrganizationContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn whereClause(&self) -> Option<Rc<WhereClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn selectClause(&self) -> Option<Rc<SelectClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn lateralView_all(&self) ->  Vec<Rc<LateralViewContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn lateralView(&self, i: usize) -> Option<Rc<LateralViewContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn aggregationClause(&self) -> Option<Rc<AggregationClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn havingClause(&self) -> Option<Rc<HavingClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn windowClause(&self) -> Option<Rc<WindowClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> FromStatementBodyContextAttrs<'input> for FromStatementBodyContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn fromStatementBody(&mut self,)
	-> Result<Rc<FromStatementBodyContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = FromStatementBodyContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 92, RULE_fromStatementBody);
        let mut _localctx: Rc<FromStatementBodyContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			recog.base.set_state(1691);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(196,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule transformClause*/
					recog.base.set_state(1664);
					recog.transformClause()?;

					recog.base.set_state(1666);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(190,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule whereClause*/
							recog.base.set_state(1665);
							recog.whereClause()?;

							}
						}

						_ => {}
					}
					/*InvokeRule queryOrganization*/
					recog.base.set_state(1668);
					recog.queryOrganization()?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					/*InvokeRule selectClause*/
					recog.base.set_state(1670);
					recog.selectClause()?;

					recog.base.set_state(1674);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(191,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							/*InvokeRule lateralView*/
							recog.base.set_state(1671);
							recog.lateralView()?;

							}
							} 
						}
						recog.base.set_state(1676);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(191,&mut recog.base)?;
					}
					recog.base.set_state(1678);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(192,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule whereClause*/
							recog.base.set_state(1677);
							recog.whereClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1681);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(193,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule aggregationClause*/
							recog.base.set_state(1680);
							recog.aggregationClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1684);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(194,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule havingClause*/
							recog.base.set_state(1683);
							recog.havingClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1687);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(195,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule windowClause*/
							recog.base.set_state(1686);
							recog.windowClause()?;

							}
						}

						_ => {}
					}
					/*InvokeRule queryOrganization*/
					recog.base.set_state(1689);
					recog.queryOrganization()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- querySpecification ----------------
#[derive(Debug)]
pub enum QuerySpecificationContextAll<'input>{
	RegularQuerySpecificationContext(RegularQuerySpecificationContext<'input>),
	TransformQuerySpecificationContext(TransformQuerySpecificationContext<'input>),
Error(QuerySpecificationContext<'input>)
}
crate::tid!{QuerySpecificationContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for QuerySpecificationContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for QuerySpecificationContextAll<'input>{}

impl<'input> Deref for QuerySpecificationContextAll<'input>{
	type Target = dyn QuerySpecificationContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use QuerySpecificationContextAll::*;
		match self{
			RegularQuerySpecificationContext(inner) => inner,
			TransformQuerySpecificationContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QuerySpecificationContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type QuerySpecificationContext<'input> = BaseParserRuleContext<'input,QuerySpecificationContextExt<'input>>;

#[derive(Clone)]
pub struct QuerySpecificationContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QuerySpecificationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QuerySpecificationContext<'input>{
}

impl<'input> CustomRuleContext<'input> for QuerySpecificationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_querySpecification }
	//fn type_rule_index() -> usize where Self: Sized { RULE_querySpecification }
}
crate::tid!{QuerySpecificationContextExt<'a>}

impl<'input> QuerySpecificationContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QuerySpecificationContextAll<'input>> {
		Rc::new(
		QuerySpecificationContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QuerySpecificationContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait QuerySpecificationContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QuerySpecificationContextExt<'input>>{


}

impl<'input> QuerySpecificationContextAttrs<'input> for QuerySpecificationContext<'input>{}

pub type RegularQuerySpecificationContext<'input> = BaseParserRuleContext<'input,RegularQuerySpecificationContextExt<'input>>;

pub trait RegularQuerySpecificationContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn selectClause(&self) -> Option<Rc<SelectClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn fromClause(&self) -> Option<Rc<FromClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn lateralView_all(&self) ->  Vec<Rc<LateralViewContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn lateralView(&self, i: usize) -> Option<Rc<LateralViewContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn whereClause(&self) -> Option<Rc<WhereClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn aggregationClause(&self) -> Option<Rc<AggregationClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn havingClause(&self) -> Option<Rc<HavingClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn windowClause(&self) -> Option<Rc<WindowClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> RegularQuerySpecificationContextAttrs<'input> for RegularQuerySpecificationContext<'input>{}

pub struct RegularQuerySpecificationContextExt<'input>{
	base:QuerySpecificationContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RegularQuerySpecificationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RegularQuerySpecificationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RegularQuerySpecificationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_regularQuerySpecification(self);
	}
}

impl<'input> CustomRuleContext<'input> for RegularQuerySpecificationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_querySpecification }
	//fn type_rule_index() -> usize where Self: Sized { RULE_querySpecification }
}

impl<'input> Borrow<QuerySpecificationContextExt<'input>> for RegularQuerySpecificationContext<'input>{
	fn borrow(&self) -> &QuerySpecificationContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QuerySpecificationContextExt<'input>> for RegularQuerySpecificationContext<'input>{
	fn borrow_mut(&mut self) -> &mut QuerySpecificationContextExt<'input> { &mut self.base }
}

impl<'input> QuerySpecificationContextAttrs<'input> for RegularQuerySpecificationContext<'input> {}

impl<'input> RegularQuerySpecificationContextExt<'input>{
	fn new(ctx: &dyn QuerySpecificationContextAttrs<'input>) -> Rc<QuerySpecificationContextAll<'input>>  {
		Rc::new(
			QuerySpecificationContextAll::RegularQuerySpecificationContext(
				BaseParserRuleContext::copy_from(ctx,RegularQuerySpecificationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type TransformQuerySpecificationContext<'input> = BaseParserRuleContext<'input,TransformQuerySpecificationContextExt<'input>>;

pub trait TransformQuerySpecificationContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn transformClause(&self) -> Option<Rc<TransformClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn fromClause(&self) -> Option<Rc<FromClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn whereClause(&self) -> Option<Rc<WhereClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> TransformQuerySpecificationContextAttrs<'input> for TransformQuerySpecificationContext<'input>{}

pub struct TransformQuerySpecificationContextExt<'input>{
	base:QuerySpecificationContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{TransformQuerySpecificationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TransformQuerySpecificationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TransformQuerySpecificationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_transformQuerySpecification(self);
	}
}

impl<'input> CustomRuleContext<'input> for TransformQuerySpecificationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_querySpecification }
	//fn type_rule_index() -> usize where Self: Sized { RULE_querySpecification }
}

impl<'input> Borrow<QuerySpecificationContextExt<'input>> for TransformQuerySpecificationContext<'input>{
	fn borrow(&self) -> &QuerySpecificationContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<QuerySpecificationContextExt<'input>> for TransformQuerySpecificationContext<'input>{
	fn borrow_mut(&mut self) -> &mut QuerySpecificationContextExt<'input> { &mut self.base }
}

impl<'input> QuerySpecificationContextAttrs<'input> for TransformQuerySpecificationContext<'input> {}

impl<'input> TransformQuerySpecificationContextExt<'input>{
	fn new(ctx: &dyn QuerySpecificationContextAttrs<'input>) -> Rc<QuerySpecificationContextAll<'input>>  {
		Rc::new(
			QuerySpecificationContextAll::TransformQuerySpecificationContext(
				BaseParserRuleContext::copy_from(ctx,TransformQuerySpecificationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn querySpecification(&mut self,)
	-> Result<Rc<QuerySpecificationContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QuerySpecificationContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 94, RULE_querySpecification);
        let mut _localctx: Rc<QuerySpecificationContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			recog.base.set_state(1722);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(205,&mut recog.base)? {
				1 =>{
					let tmp = TransformQuerySpecificationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					/*InvokeRule transformClause*/
					recog.base.set_state(1693);
					recog.transformClause()?;

					recog.base.set_state(1695);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(197,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule fromClause*/
							recog.base.set_state(1694);
							recog.fromClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1698);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(198,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule whereClause*/
							recog.base.set_state(1697);
							recog.whereClause()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				2 =>{
					let tmp = RegularQuerySpecificationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule selectClause*/
					recog.base.set_state(1700);
					recog.selectClause()?;

					recog.base.set_state(1702);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(199,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule fromClause*/
							recog.base.set_state(1701);
							recog.fromClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1707);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(200,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							/*InvokeRule lateralView*/
							recog.base.set_state(1704);
							recog.lateralView()?;

							}
							} 
						}
						recog.base.set_state(1709);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(200,&mut recog.base)?;
					}
					recog.base.set_state(1711);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(201,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule whereClause*/
							recog.base.set_state(1710);
							recog.whereClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1714);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(202,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule aggregationClause*/
							recog.base.set_state(1713);
							recog.aggregationClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1717);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(203,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule havingClause*/
							recog.base.set_state(1716);
							recog.havingClause()?;

							}
						}

						_ => {}
					}
					recog.base.set_state(1720);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(204,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule windowClause*/
							recog.base.set_state(1719);
							recog.windowClause()?;

							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- transformClause ----------------
pub type TransformClauseContextAll<'input> = TransformClauseContext<'input>;


pub type TransformClauseContext<'input> = BaseParserRuleContext<'input,TransformClauseContextExt<'input>>;

#[derive(Clone)]
pub struct TransformClauseContextExt<'input>{
	pub kind: Option<TokenType<'input>>,
	pub inRowFormat: Option<Rc<RowFormatContextAll<'input>>>,
	pub recordWriter: Option<TokenType<'input>>,
	pub script: Option<TokenType<'input>>,
	pub outRowFormat: Option<Rc<RowFormatContextAll<'input>>>,
	pub recordReader: Option<TokenType<'input>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TransformClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TransformClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_transformClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_transformClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TransformClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_transformClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_transformClause }
}
crate::tid!{TransformClauseContextExt<'a>}

impl<'input> TransformClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TransformClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TransformClauseContextExt{
				kind: None, recordWriter: None, script: None, recordReader: None, 
				inRowFormat: None, outRowFormat: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait TransformClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TransformClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token USING
/// Returns `None` if there is no child corresponding to token USING
fn USING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(USING, 0)
}
/// Retrieves all `TerminalNode`s corresponding to token STRING in current rule
fn STRING_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
	self.children_of_type()
}
/// Retrieves 'i's TerminalNode corresponding to token STRING, starting from 0.
/// Returns `None` if number of children corresponding to token STRING is less or equal than `i`.
fn STRING(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, i)
}
/// Retrieves first TerminalNode corresponding to token SELECT
/// Returns `None` if there is no child corresponding to token SELECT
fn SELECT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SELECT, 0)
}
fn namedExpressionSeq(&self) -> Option<Rc<NamedExpressionSeqContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token TRANSFORM
/// Returns `None` if there is no child corresponding to token TRANSFORM
fn TRANSFORM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSFORM, 0)
}
/// Retrieves first TerminalNode corresponding to token MAP
/// Returns `None` if there is no child corresponding to token MAP
fn MAP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MAP, 0)
}
/// Retrieves first TerminalNode corresponding to token REDUCE
/// Returns `None` if there is no child corresponding to token REDUCE
fn REDUCE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REDUCE, 0)
}
/// Retrieves first TerminalNode corresponding to token RECORDWRITER
/// Returns `None` if there is no child corresponding to token RECORDWRITER
fn RECORDWRITER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECORDWRITER, 0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
/// Retrieves first TerminalNode corresponding to token RECORDREADER
/// Returns `None` if there is no child corresponding to token RECORDREADER
fn RECORDREADER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECORDREADER, 0)
}
fn rowFormat_all(&self) ->  Vec<Rc<RowFormatContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn rowFormat(&self, i: usize) -> Option<Rc<RowFormatContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn identifierSeq(&self) -> Option<Rc<IdentifierSeqContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn colTypeList(&self) -> Option<Rc<ColTypeListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> TransformClauseContextAttrs<'input> for TransformClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn transformClause(&mut self,)
	-> Result<Rc<TransformClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TransformClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 96, RULE_transformClause);
        let mut _localctx: Rc<TransformClauseContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1734);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 SELECT 
				=> {
					{
					recog.base.set_state(1724);
					recog.base.match_token(SELECT,&mut recog.err_handler)?;

					recog.base.set_state(1725);
					let tmp = recog.base.match_token(TRANSFORM,&mut recog.err_handler)?;
					 cast_mut::<_,TransformClauseContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					recog.base.set_state(1726);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule namedExpressionSeq*/
					recog.base.set_state(1727);
					recog.namedExpressionSeq()?;

					recog.base.set_state(1728);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

			 MAP 
				=> {
					{
					recog.base.set_state(1730);
					let tmp = recog.base.match_token(MAP,&mut recog.err_handler)?;
					 cast_mut::<_,TransformClauseContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					/*InvokeRule namedExpressionSeq*/
					recog.base.set_state(1731);
					recog.namedExpressionSeq()?;

					}
				}

			 REDUCE 
				=> {
					{
					recog.base.set_state(1732);
					let tmp = recog.base.match_token(REDUCE,&mut recog.err_handler)?;
					 cast_mut::<_,TransformClauseContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					/*InvokeRule namedExpressionSeq*/
					recog.base.set_state(1733);
					recog.namedExpressionSeq()?;

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			recog.base.set_state(1737);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==ROW {
				{
				/*InvokeRule rowFormat*/
				recog.base.set_state(1736);
				let tmp = recog.rowFormat()?;
				 cast_mut::<_,TransformClauseContext >(&mut _localctx).inRowFormat = Some(tmp.clone());
				  

				}
			}

			recog.base.set_state(1741);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==RECORDWRITER {
				{
				recog.base.set_state(1739);
				recog.base.match_token(RECORDWRITER,&mut recog.err_handler)?;

				recog.base.set_state(1740);
				let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
				 cast_mut::<_,TransformClauseContext >(&mut _localctx).recordWriter = Some(tmp.clone());
				  

				}
			}

			recog.base.set_state(1743);
			recog.base.match_token(USING,&mut recog.err_handler)?;

			recog.base.set_state(1744);
			let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
			 cast_mut::<_,TransformClauseContext >(&mut _localctx).script = Some(tmp.clone());
			  

			recog.base.set_state(1757);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(211,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1745);
					recog.base.match_token(AS,&mut recog.err_handler)?;

					recog.base.set_state(1755);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(210,&mut recog.base)? {
						1 =>{
							{
							/*InvokeRule identifierSeq*/
							recog.base.set_state(1746);
							recog.identifierSeq()?;

							}
						}
					,
						2 =>{
							{
							/*InvokeRule colTypeList*/
							recog.base.set_state(1747);
							recog.colTypeList()?;

							}
						}
					,
						3 =>{
							{
							{
							recog.base.set_state(1748);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							recog.base.set_state(1751);
							recog.err_handler.sync(&mut recog.base)?;
							match  recog.interpreter.adaptive_predict(209,&mut recog.base)? {
								1 =>{
									{
									/*InvokeRule identifierSeq*/
									recog.base.set_state(1749);
									recog.identifierSeq()?;

									}
								}
							,
								2 =>{
									{
									/*InvokeRule colTypeList*/
									recog.base.set_state(1750);
									recog.colTypeList()?;

									}
								}

								_ => {}
							}
							recog.base.set_state(1753);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(1760);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(212,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule rowFormat*/
					recog.base.set_state(1759);
					let tmp = recog.rowFormat()?;
					 cast_mut::<_,TransformClauseContext >(&mut _localctx).outRowFormat = Some(tmp.clone());
					  

					}
				}

				_ => {}
			}
			recog.base.set_state(1764);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(213,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1762);
					recog.base.match_token(RECORDREADER,&mut recog.err_handler)?;

					recog.base.set_state(1763);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					 cast_mut::<_,TransformClauseContext >(&mut _localctx).recordReader = Some(tmp.clone());
					  

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- selectClause ----------------
pub type SelectClauseContextAll<'input> = SelectClauseContext<'input>;


pub type SelectClauseContext<'input> = BaseParserRuleContext<'input,SelectClauseContextExt<'input>>;

#[derive(Clone)]
pub struct SelectClauseContextExt<'input>{
	pub hint: Option<Rc<HintContextAll<'input>>>,
	pub hints:Vec<Rc<HintContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SelectClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SelectClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_selectClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_selectClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SelectClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_selectClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_selectClause }
}
crate::tid!{SelectClauseContextExt<'a>}

impl<'input> SelectClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SelectClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SelectClauseContextExt{
				hint: None, 
				hints: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait SelectClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SelectClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token SELECT
/// Returns `None` if there is no child corresponding to token SELECT
fn SELECT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SELECT, 0)
}
fn namedExpressionSeq(&self) -> Option<Rc<NamedExpressionSeqContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn setQuantifier(&self) -> Option<Rc<SetQuantifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn hint_all(&self) ->  Vec<Rc<HintContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn hint(&self, i: usize) -> Option<Rc<HintContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> SelectClauseContextAttrs<'input> for SelectClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn selectClause(&mut self,)
	-> Result<Rc<SelectClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SelectClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 98, RULE_selectClause);
        let mut _localctx: Rc<SelectClauseContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1766);
			recog.base.match_token(SELECT,&mut recog.err_handler)?;

			recog.base.set_state(1770);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(214,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					/*InvokeRule hint*/
					recog.base.set_state(1767);
					let tmp = recog.hint()?;
					 cast_mut::<_,SelectClauseContext >(&mut _localctx).hint = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,SelectClauseContext >(&mut _localctx).hint.clone().unwrap()
					 ;
					 cast_mut::<_,SelectClauseContext >(&mut _localctx).hints.push(temp);
					  
					}
					} 
				}
				recog.base.set_state(1772);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(214,&mut recog.base)?;
			}
			recog.base.set_state(1774);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(215,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule setQuantifier*/
					recog.base.set_state(1773);
					recog.setQuantifier()?;

					}
				}

				_ => {}
			}
			/*InvokeRule namedExpressionSeq*/
			recog.base.set_state(1776);
			recog.namedExpressionSeq()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- setClause ----------------
pub type SetClauseContextAll<'input> = SetClauseContext<'input>;


pub type SetClauseContext<'input> = BaseParserRuleContext<'input,SetClauseContextExt<'input>>;

#[derive(Clone)]
pub struct SetClauseContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SetClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_setClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_setClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SetClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_setClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_setClause }
}
crate::tid!{SetClauseContextExt<'a>}

impl<'input> SetClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SetClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SetClauseContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SetClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SetClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token SET
/// Returns `None` if there is no child corresponding to token SET
fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SET, 0)
}
fn assignmentList(&self) -> Option<Rc<AssignmentListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> SetClauseContextAttrs<'input> for SetClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn setClause(&mut self,)
	-> Result<Rc<SetClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SetClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 100, RULE_setClause);
        let mut _localctx: Rc<SetClauseContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1778);
			recog.base.match_token(SET,&mut recog.err_handler)?;

			/*InvokeRule assignmentList*/
			recog.base.set_state(1779);
			recog.assignmentList()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- matchedClause ----------------
pub type MatchedClauseContextAll<'input> = MatchedClauseContext<'input>;


pub type MatchedClauseContext<'input> = BaseParserRuleContext<'input,MatchedClauseContextExt<'input>>;

#[derive(Clone)]
pub struct MatchedClauseContextExt<'input>{
	pub matchedCond: Option<Rc<BooleanExpressionContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for MatchedClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MatchedClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_matchedClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_matchedClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for MatchedClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_matchedClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_matchedClause }
}
crate::tid!{MatchedClauseContextExt<'a>}

impl<'input> MatchedClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<MatchedClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,MatchedClauseContextExt{
				matchedCond: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait MatchedClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<MatchedClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token WHEN
/// Returns `None` if there is no child corresponding to token WHEN
fn WHEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WHEN, 0)
}
/// Retrieves first TerminalNode corresponding to token MATCHED
/// Returns `None` if there is no child corresponding to token MATCHED
fn MATCHED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MATCHED, 0)
}
/// Retrieves first TerminalNode corresponding to token THEN
/// Returns `None` if there is no child corresponding to token THEN
fn THEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(THEN, 0)
}
fn matchedAction(&self) -> Option<Rc<MatchedActionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token AND
/// Returns `None` if there is no child corresponding to token AND
fn AND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AND, 0)
}
fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> MatchedClauseContextAttrs<'input> for MatchedClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn matchedClause(&mut self,)
	-> Result<Rc<MatchedClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = MatchedClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 102, RULE_matchedClause);
        let mut _localctx: Rc<MatchedClauseContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1781);
			recog.base.match_token(WHEN,&mut recog.err_handler)?;

			recog.base.set_state(1782);
			recog.base.match_token(MATCHED,&mut recog.err_handler)?;

			recog.base.set_state(1785);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==AND {
				{
				recog.base.set_state(1783);
				recog.base.match_token(AND,&mut recog.err_handler)?;

				/*InvokeRule booleanExpression*/
				recog.base.set_state(1784);
				let tmp = recog.booleanExpression_rec(0)?;
				 cast_mut::<_,MatchedClauseContext >(&mut _localctx).matchedCond = Some(tmp.clone());
				  

				}
			}

			recog.base.set_state(1787);
			recog.base.match_token(THEN,&mut recog.err_handler)?;

			/*InvokeRule matchedAction*/
			recog.base.set_state(1788);
			recog.matchedAction()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- notMatchedClause ----------------
pub type NotMatchedClauseContextAll<'input> = NotMatchedClauseContext<'input>;


pub type NotMatchedClauseContext<'input> = BaseParserRuleContext<'input,NotMatchedClauseContextExt<'input>>;

#[derive(Clone)]
pub struct NotMatchedClauseContextExt<'input>{
	pub notMatchedCond: Option<Rc<BooleanExpressionContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NotMatchedClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NotMatchedClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_notMatchedClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_notMatchedClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NotMatchedClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_notMatchedClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_notMatchedClause }
}
crate::tid!{NotMatchedClauseContextExt<'a>}

impl<'input> NotMatchedClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NotMatchedClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NotMatchedClauseContextExt{
				notMatchedCond: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait NotMatchedClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NotMatchedClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token WHEN
/// Returns `None` if there is no child corresponding to token WHEN
fn WHEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WHEN, 0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token MATCHED
/// Returns `None` if there is no child corresponding to token MATCHED
fn MATCHED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MATCHED, 0)
}
/// Retrieves first TerminalNode corresponding to token THEN
/// Returns `None` if there is no child corresponding to token THEN
fn THEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(THEN, 0)
}
fn notMatchedAction(&self) -> Option<Rc<NotMatchedActionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token AND
/// Returns `None` if there is no child corresponding to token AND
fn AND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AND, 0)
}
fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> NotMatchedClauseContextAttrs<'input> for NotMatchedClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn notMatchedClause(&mut self,)
	-> Result<Rc<NotMatchedClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NotMatchedClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 104, RULE_notMatchedClause);
        let mut _localctx: Rc<NotMatchedClauseContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1790);
			recog.base.match_token(WHEN,&mut recog.err_handler)?;

			recog.base.set_state(1791);
			recog.base.match_token(NOT,&mut recog.err_handler)?;

			recog.base.set_state(1792);
			recog.base.match_token(MATCHED,&mut recog.err_handler)?;

			recog.base.set_state(1795);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==AND {
				{
				recog.base.set_state(1793);
				recog.base.match_token(AND,&mut recog.err_handler)?;

				/*InvokeRule booleanExpression*/
				recog.base.set_state(1794);
				let tmp = recog.booleanExpression_rec(0)?;
				 cast_mut::<_,NotMatchedClauseContext >(&mut _localctx).notMatchedCond = Some(tmp.clone());
				  

				}
			}

			recog.base.set_state(1797);
			recog.base.match_token(THEN,&mut recog.err_handler)?;

			/*InvokeRule notMatchedAction*/
			recog.base.set_state(1798);
			recog.notMatchedAction()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- matchedAction ----------------
pub type MatchedActionContextAll<'input> = MatchedActionContext<'input>;


pub type MatchedActionContext<'input> = BaseParserRuleContext<'input,MatchedActionContextExt<'input>>;

#[derive(Clone)]
pub struct MatchedActionContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for MatchedActionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MatchedActionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_matchedAction(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_matchedAction(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for MatchedActionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_matchedAction }
	//fn type_rule_index() -> usize where Self: Sized { RULE_matchedAction }
}
crate::tid!{MatchedActionContextExt<'a>}

impl<'input> MatchedActionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<MatchedActionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,MatchedActionContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait MatchedActionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<MatchedActionContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token DELETE
/// Returns `None` if there is no child corresponding to token DELETE
fn DELETE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DELETE, 0)
}
/// Retrieves first TerminalNode corresponding to token UPDATE
/// Returns `None` if there is no child corresponding to token UPDATE
fn UPDATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UPDATE, 0)
}
/// Retrieves first TerminalNode corresponding to token SET
/// Returns `None` if there is no child corresponding to token SET
fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SET, 0)
}
/// Retrieves first TerminalNode corresponding to token ASTERISK
/// Returns `None` if there is no child corresponding to token ASTERISK
fn ASTERISK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ASTERISK, 0)
}
fn assignmentList(&self) -> Option<Rc<AssignmentListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> MatchedActionContextAttrs<'input> for MatchedActionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn matchedAction(&mut self,)
	-> Result<Rc<MatchedActionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = MatchedActionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 106, RULE_matchedAction);
        let mut _localctx: Rc<MatchedActionContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1807);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(218,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(1800);
					recog.base.match_token(DELETE,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1801);
					recog.base.match_token(UPDATE,&mut recog.err_handler)?;

					recog.base.set_state(1802);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					recog.base.set_state(1803);
					recog.base.match_token(ASTERISK,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					recog.base.set_state(1804);
					recog.base.match_token(UPDATE,&mut recog.err_handler)?;

					recog.base.set_state(1805);
					recog.base.match_token(SET,&mut recog.err_handler)?;

					/*InvokeRule assignmentList*/
					recog.base.set_state(1806);
					recog.assignmentList()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- notMatchedAction ----------------
pub type NotMatchedActionContextAll<'input> = NotMatchedActionContext<'input>;


pub type NotMatchedActionContext<'input> = BaseParserRuleContext<'input,NotMatchedActionContextExt<'input>>;

#[derive(Clone)]
pub struct NotMatchedActionContextExt<'input>{
	pub columns: Option<Rc<MultipartIdentifierListContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NotMatchedActionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NotMatchedActionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_notMatchedAction(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_notMatchedAction(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NotMatchedActionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_notMatchedAction }
	//fn type_rule_index() -> usize where Self: Sized { RULE_notMatchedAction }
}
crate::tid!{NotMatchedActionContextExt<'a>}

impl<'input> NotMatchedActionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NotMatchedActionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NotMatchedActionContextExt{
				columns: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait NotMatchedActionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NotMatchedActionContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token INSERT
/// Returns `None` if there is no child corresponding to token INSERT
fn INSERT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INSERT, 0)
}
/// Retrieves first TerminalNode corresponding to token ASTERISK
/// Returns `None` if there is no child corresponding to token ASTERISK
fn ASTERISK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ASTERISK, 0)
}
/// Retrieves first TerminalNode corresponding to token VALUES
/// Returns `None` if there is no child corresponding to token VALUES
fn VALUES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VALUES, 0)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn multipartIdentifierList(&self) -> Option<Rc<MultipartIdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> NotMatchedActionContextAttrs<'input> for NotMatchedActionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn notMatchedAction(&mut self,)
	-> Result<Rc<NotMatchedActionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NotMatchedActionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 108, RULE_notMatchedAction);
        let mut _localctx: Rc<NotMatchedActionContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1827);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(220,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(1809);
					recog.base.match_token(INSERT,&mut recog.err_handler)?;

					recog.base.set_state(1810);
					recog.base.match_token(ASTERISK,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1811);
					recog.base.match_token(INSERT,&mut recog.err_handler)?;

					recog.base.set_state(1812);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule multipartIdentifierList*/
					recog.base.set_state(1813);
					let tmp = recog.multipartIdentifierList()?;
					 cast_mut::<_,NotMatchedActionContext >(&mut _localctx).columns = Some(tmp.clone());
					  

					recog.base.set_state(1814);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					recog.base.set_state(1815);
					recog.base.match_token(VALUES,&mut recog.err_handler)?;

					recog.base.set_state(1816);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(1817);
					recog.expression()?;

					recog.base.set_state(1822);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(1818);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule expression*/
						recog.base.set_state(1819);
						recog.expression()?;

						}
						}
						recog.base.set_state(1824);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(1825);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- assignmentList ----------------
pub type AssignmentListContextAll<'input> = AssignmentListContext<'input>;


pub type AssignmentListContext<'input> = BaseParserRuleContext<'input,AssignmentListContextExt<'input>>;

#[derive(Clone)]
pub struct AssignmentListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for AssignmentListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AssignmentListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_assignmentList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_assignmentList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for AssignmentListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_assignmentList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_assignmentList }
}
crate::tid!{AssignmentListContextExt<'a>}

impl<'input> AssignmentListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<AssignmentListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,AssignmentListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait AssignmentListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<AssignmentListContextExt<'input>>{

fn assignment_all(&self) ->  Vec<Rc<AssignmentContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn assignment(&self, i: usize) -> Option<Rc<AssignmentContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> AssignmentListContextAttrs<'input> for AssignmentListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn assignmentList(&mut self,)
	-> Result<Rc<AssignmentListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = AssignmentListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 110, RULE_assignmentList);
        let mut _localctx: Rc<AssignmentListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule assignment*/
			recog.base.set_state(1829);
			recog.assignment()?;

			recog.base.set_state(1834);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(1830);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule assignment*/
				recog.base.set_state(1831);
				recog.assignment()?;

				}
				}
				recog.base.set_state(1836);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- assignment ----------------
pub type AssignmentContextAll<'input> = AssignmentContext<'input>;


pub type AssignmentContext<'input> = BaseParserRuleContext<'input,AssignmentContextExt<'input>>;

#[derive(Clone)]
pub struct AssignmentContextExt<'input>{
	pub key: Option<Rc<MultipartIdentifierContextAll<'input>>>,
	pub value: Option<Rc<ExpressionContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for AssignmentContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AssignmentContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_assignment(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_assignment(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for AssignmentContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_assignment }
	//fn type_rule_index() -> usize where Self: Sized { RULE_assignment }
}
crate::tid!{AssignmentContextExt<'a>}

impl<'input> AssignmentContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<AssignmentContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,AssignmentContextExt{
				key: None, value: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait AssignmentContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<AssignmentContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token EQ
/// Returns `None` if there is no child corresponding to token EQ
fn EQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EQ, 0)
}
fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> AssignmentContextAttrs<'input> for AssignmentContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn assignment(&mut self,)
	-> Result<Rc<AssignmentContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = AssignmentContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 112, RULE_assignment);
        let mut _localctx: Rc<AssignmentContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule multipartIdentifier*/
			recog.base.set_state(1837);
			let tmp = recog.multipartIdentifier()?;
			 cast_mut::<_,AssignmentContext >(&mut _localctx).key = Some(tmp.clone());
			  

			recog.base.set_state(1838);
			recog.base.match_token(EQ,&mut recog.err_handler)?;

			/*InvokeRule expression*/
			recog.base.set_state(1839);
			let tmp = recog.expression()?;
			 cast_mut::<_,AssignmentContext >(&mut _localctx).value = Some(tmp.clone());
			  

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- whereClause ----------------
pub type WhereClauseContextAll<'input> = WhereClauseContext<'input>;


pub type WhereClauseContext<'input> = BaseParserRuleContext<'input,WhereClauseContextExt<'input>>;

#[derive(Clone)]
pub struct WhereClauseContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for WhereClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WhereClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_whereClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_whereClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for WhereClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_whereClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_whereClause }
}
crate::tid!{WhereClauseContextExt<'a>}

impl<'input> WhereClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<WhereClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,WhereClauseContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait WhereClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<WhereClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token WHERE
/// Returns `None` if there is no child corresponding to token WHERE
fn WHERE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WHERE, 0)
}
fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> WhereClauseContextAttrs<'input> for WhereClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn whereClause(&mut self,)
	-> Result<Rc<WhereClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = WhereClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 114, RULE_whereClause);
        let mut _localctx: Rc<WhereClauseContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1841);
			recog.base.match_token(WHERE,&mut recog.err_handler)?;

			/*InvokeRule booleanExpression*/
			recog.base.set_state(1842);
			recog.booleanExpression_rec(0)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- havingClause ----------------
pub type HavingClauseContextAll<'input> = HavingClauseContext<'input>;


pub type HavingClauseContext<'input> = BaseParserRuleContext<'input,HavingClauseContextExt<'input>>;

#[derive(Clone)]
pub struct HavingClauseContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for HavingClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for HavingClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_havingClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_havingClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for HavingClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_havingClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_havingClause }
}
crate::tid!{HavingClauseContextExt<'a>}

impl<'input> HavingClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<HavingClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,HavingClauseContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait HavingClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<HavingClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token HAVING
/// Returns `None` if there is no child corresponding to token HAVING
fn HAVING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(HAVING, 0)
}
fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> HavingClauseContextAttrs<'input> for HavingClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn havingClause(&mut self,)
	-> Result<Rc<HavingClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = HavingClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 116, RULE_havingClause);
        let mut _localctx: Rc<HavingClauseContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1844);
			recog.base.match_token(HAVING,&mut recog.err_handler)?;

			/*InvokeRule booleanExpression*/
			recog.base.set_state(1845);
			recog.booleanExpression_rec(0)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- hint ----------------
pub type HintContextAll<'input> = HintContext<'input>;


pub type HintContext<'input> = BaseParserRuleContext<'input,HintContextExt<'input>>;

#[derive(Clone)]
pub struct HintContextExt<'input>{
	pub hintStatement: Option<Rc<HintStatementContextAll<'input>>>,
	pub hintStatements:Vec<Rc<HintStatementContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for HintContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for HintContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_hint(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_hint(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for HintContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_hint }
	//fn type_rule_index() -> usize where Self: Sized { RULE_hint }
}
crate::tid!{HintContextExt<'a>}

impl<'input> HintContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<HintContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,HintContextExt{
				hintStatement: None, 
				hintStatements: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait HintContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<HintContextExt<'input>>{

fn hintStatement_all(&self) ->  Vec<Rc<HintStatementContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn hintStatement(&self, i: usize) -> Option<Rc<HintStatementContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> HintContextAttrs<'input> for HintContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn hint(&mut self,)
	-> Result<Rc<HintContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = HintContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 118, RULE_hint);
        let mut _localctx: Rc<HintContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1847);
			recog.base.match_token(T__4,&mut recog.err_handler)?;

			/*InvokeRule hintStatement*/
			recog.base.set_state(1848);
			let tmp = recog.hintStatement()?;
			 cast_mut::<_,HintContext >(&mut _localctx).hintStatement = Some(tmp.clone());
			  

			let temp =  cast_mut::<_,HintContext >(&mut _localctx).hintStatement.clone().unwrap()
			 ;
			 cast_mut::<_,HintContext >(&mut _localctx).hintStatements.push(temp);
			  
			recog.base.set_state(1855);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(223,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(1850);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(222,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1849);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule hintStatement*/
					recog.base.set_state(1852);
					let tmp = recog.hintStatement()?;
					 cast_mut::<_,HintContext >(&mut _localctx).hintStatement = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,HintContext >(&mut _localctx).hintStatement.clone().unwrap()
					 ;
					 cast_mut::<_,HintContext >(&mut _localctx).hintStatements.push(temp);
					  
					}
					} 
				}
				recog.base.set_state(1857);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(223,&mut recog.base)?;
			}
			recog.base.set_state(1858);
			recog.base.match_token(T__5,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- hintStatement ----------------
pub type HintStatementContextAll<'input> = HintStatementContext<'input>;


pub type HintStatementContext<'input> = BaseParserRuleContext<'input,HintStatementContextExt<'input>>;

#[derive(Clone)]
pub struct HintStatementContextExt<'input>{
	pub hintName: Option<Rc<IdentifierContextAll<'input>>>,
	pub primaryExpression: Option<Rc<PrimaryExpressionContextAll<'input>>>,
	pub parameters:Vec<Rc<PrimaryExpressionContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for HintStatementContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for HintStatementContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_hintStatement(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_hintStatement(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for HintStatementContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_hintStatement }
	//fn type_rule_index() -> usize where Self: Sized { RULE_hintStatement }
}
crate::tid!{HintStatementContextExt<'a>}

impl<'input> HintStatementContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<HintStatementContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,HintStatementContextExt{
				hintName: None, primaryExpression: None, 
				parameters: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait HintStatementContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<HintStatementContextExt<'input>>{

fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn primaryExpression_all(&self) ->  Vec<Rc<PrimaryExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn primaryExpression(&self, i: usize) -> Option<Rc<PrimaryExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> HintStatementContextAttrs<'input> for HintStatementContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn hintStatement(&mut self,)
	-> Result<Rc<HintStatementContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = HintStatementContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 120, RULE_hintStatement);
        let mut _localctx: Rc<HintStatementContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1873);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(225,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule identifier*/
					recog.base.set_state(1860);
					let tmp = recog.identifier()?;
					 cast_mut::<_,HintStatementContext >(&mut _localctx).hintName = Some(tmp.clone());
					  

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					/*InvokeRule identifier*/
					recog.base.set_state(1861);
					let tmp = recog.identifier()?;
					 cast_mut::<_,HintStatementContext >(&mut _localctx).hintName = Some(tmp.clone());
					  

					recog.base.set_state(1862);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule primaryExpression*/
					recog.base.set_state(1863);
					let tmp = recog.primaryExpression_rec(0)?;
					 cast_mut::<_,HintStatementContext >(&mut _localctx).primaryExpression = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,HintStatementContext >(&mut _localctx).primaryExpression.clone().unwrap()
					 ;
					 cast_mut::<_,HintStatementContext >(&mut _localctx).parameters.push(temp);
					  
					recog.base.set_state(1868);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(1864);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule primaryExpression*/
						recog.base.set_state(1865);
						let tmp = recog.primaryExpression_rec(0)?;
						 cast_mut::<_,HintStatementContext >(&mut _localctx).primaryExpression = Some(tmp.clone());
						  

						let temp =  cast_mut::<_,HintStatementContext >(&mut _localctx).primaryExpression.clone().unwrap()
						 ;
						 cast_mut::<_,HintStatementContext >(&mut _localctx).parameters.push(temp);
						  
						}
						}
						recog.base.set_state(1870);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(1871);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- fromClause ----------------
pub type FromClauseContextAll<'input> = FromClauseContext<'input>;


pub type FromClauseContext<'input> = BaseParserRuleContext<'input,FromClauseContextExt<'input>>;

#[derive(Clone)]
pub struct FromClauseContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for FromClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FromClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_fromClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_fromClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for FromClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_fromClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_fromClause }
}
crate::tid!{FromClauseContextExt<'a>}

impl<'input> FromClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<FromClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,FromClauseContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait FromClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<FromClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token FROM
/// Returns `None` if there is no child corresponding to token FROM
fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FROM, 0)
}
fn relation_all(&self) ->  Vec<Rc<RelationContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn relation(&self, i: usize) -> Option<Rc<RelationContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn lateralView_all(&self) ->  Vec<Rc<LateralViewContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn lateralView(&self, i: usize) -> Option<Rc<LateralViewContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn pivotClause(&self) -> Option<Rc<PivotClauseContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> FromClauseContextAttrs<'input> for FromClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn fromClause(&mut self,)
	-> Result<Rc<FromClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = FromClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 122, RULE_fromClause);
        let mut _localctx: Rc<FromClauseContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1875);
			recog.base.match_token(FROM,&mut recog.err_handler)?;

			/*InvokeRule relation*/
			recog.base.set_state(1876);
			recog.relation()?;

			recog.base.set_state(1881);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(226,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(1877);
					recog.base.match_token(T__2,&mut recog.err_handler)?;

					/*InvokeRule relation*/
					recog.base.set_state(1878);
					recog.relation()?;

					}
					} 
				}
				recog.base.set_state(1883);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(226,&mut recog.base)?;
			}
			recog.base.set_state(1887);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(227,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					/*InvokeRule lateralView*/
					recog.base.set_state(1884);
					recog.lateralView()?;

					}
					} 
				}
				recog.base.set_state(1889);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(227,&mut recog.base)?;
			}
			recog.base.set_state(1891);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(228,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule pivotClause*/
					recog.base.set_state(1890);
					recog.pivotClause()?;

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- aggregationClause ----------------
pub type AggregationClauseContextAll<'input> = AggregationClauseContext<'input>;


pub type AggregationClauseContext<'input> = BaseParserRuleContext<'input,AggregationClauseContextExt<'input>>;

#[derive(Clone)]
pub struct AggregationClauseContextExt<'input>{
	pub expression: Option<Rc<ExpressionContextAll<'input>>>,
	pub groupingExpressions:Vec<Rc<ExpressionContextAll<'input>>>,
	pub kind: Option<TokenType<'input>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for AggregationClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AggregationClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_aggregationClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_aggregationClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for AggregationClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_aggregationClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_aggregationClause }
}
crate::tid!{AggregationClauseContextExt<'a>}

impl<'input> AggregationClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<AggregationClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,AggregationClauseContextExt{
				kind: None, 
				expression: None, 
				groupingExpressions: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait AggregationClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<AggregationClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token GROUP
/// Returns `None` if there is no child corresponding to token GROUP
fn GROUP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GROUP, 0)
}
/// Retrieves first TerminalNode corresponding to token BY
/// Returns `None` if there is no child corresponding to token BY
fn BY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, 0)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token WITH
/// Returns `None` if there is no child corresponding to token WITH
fn WITH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WITH, 0)
}
/// Retrieves first TerminalNode corresponding to token SETS
/// Returns `None` if there is no child corresponding to token SETS
fn SETS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SETS, 0)
}
fn groupingSet_all(&self) ->  Vec<Rc<GroupingSetContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn groupingSet(&self, i: usize) -> Option<Rc<GroupingSetContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token ROLLUP
/// Returns `None` if there is no child corresponding to token ROLLUP
fn ROLLUP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLLUP, 0)
}
/// Retrieves first TerminalNode corresponding to token CUBE
/// Returns `None` if there is no child corresponding to token CUBE
fn CUBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CUBE, 0)
}
/// Retrieves first TerminalNode corresponding to token GROUPING
/// Returns `None` if there is no child corresponding to token GROUPING
fn GROUPING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GROUPING, 0)
}

}

impl<'input> AggregationClauseContextAttrs<'input> for AggregationClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn aggregationClause(&mut self,)
	-> Result<Rc<AggregationClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = AggregationClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 124, RULE_aggregationClause);
        let mut _localctx: Rc<AggregationClauseContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			recog.base.set_state(1937);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(233,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(1893);
					recog.base.match_token(GROUP,&mut recog.err_handler)?;

					recog.base.set_state(1894);
					recog.base.match_token(BY,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(1895);
					let tmp = recog.expression()?;
					 cast_mut::<_,AggregationClauseContext >(&mut _localctx).expression = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,AggregationClauseContext >(&mut _localctx).expression.clone().unwrap()
					 ;
					 cast_mut::<_,AggregationClauseContext >(&mut _localctx).groupingExpressions.push(temp);
					  
					recog.base.set_state(1900);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(229,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							recog.base.set_state(1896);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							/*InvokeRule expression*/
							recog.base.set_state(1897);
							let tmp = recog.expression()?;
							 cast_mut::<_,AggregationClauseContext >(&mut _localctx).expression = Some(tmp.clone());
							  

							let temp =  cast_mut::<_,AggregationClauseContext >(&mut _localctx).expression.clone().unwrap()
							 ;
							 cast_mut::<_,AggregationClauseContext >(&mut _localctx).groupingExpressions.push(temp);
							  
							}
							} 
						}
						recog.base.set_state(1902);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(229,&mut recog.base)?;
					}
					recog.base.set_state(1920);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(231,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1903);
							recog.base.match_token(WITH,&mut recog.err_handler)?;

							recog.base.set_state(1904);
							let tmp = recog.base.match_token(ROLLUP,&mut recog.err_handler)?;
							 cast_mut::<_,AggregationClauseContext >(&mut _localctx).kind = Some(tmp.clone());
							  

							}
						}

						x if x == 2=>{
							{
							recog.base.set_state(1905);
							recog.base.match_token(WITH,&mut recog.err_handler)?;

							recog.base.set_state(1906);
							let tmp = recog.base.match_token(CUBE,&mut recog.err_handler)?;
							 cast_mut::<_,AggregationClauseContext >(&mut _localctx).kind = Some(tmp.clone());
							  

							}
						}

						x if x == 3=>{
							{
							recog.base.set_state(1907);
							let tmp = recog.base.match_token(GROUPING,&mut recog.err_handler)?;
							 cast_mut::<_,AggregationClauseContext >(&mut _localctx).kind = Some(tmp.clone());
							  

							recog.base.set_state(1908);
							recog.base.match_token(SETS,&mut recog.err_handler)?;

							recog.base.set_state(1909);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							/*InvokeRule groupingSet*/
							recog.base.set_state(1910);
							recog.groupingSet()?;

							recog.base.set_state(1915);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							while _la==T__2 {
								{
								{
								recog.base.set_state(1911);
								recog.base.match_token(T__2,&mut recog.err_handler)?;

								/*InvokeRule groupingSet*/
								recog.base.set_state(1912);
								recog.groupingSet()?;

								}
								}
								recog.base.set_state(1917);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
							}
							recog.base.set_state(1918);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1922);
					recog.base.match_token(GROUP,&mut recog.err_handler)?;

					recog.base.set_state(1923);
					recog.base.match_token(BY,&mut recog.err_handler)?;

					recog.base.set_state(1924);
					let tmp = recog.base.match_token(GROUPING,&mut recog.err_handler)?;
					 cast_mut::<_,AggregationClauseContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					recog.base.set_state(1925);
					recog.base.match_token(SETS,&mut recog.err_handler)?;

					recog.base.set_state(1926);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule groupingSet*/
					recog.base.set_state(1927);
					recog.groupingSet()?;

					recog.base.set_state(1932);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(1928);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule groupingSet*/
						recog.base.set_state(1929);
						recog.groupingSet()?;

						}
						}
						recog.base.set_state(1934);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(1935);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- groupingSet ----------------
pub type GroupingSetContextAll<'input> = GroupingSetContext<'input>;


pub type GroupingSetContext<'input> = BaseParserRuleContext<'input,GroupingSetContextExt<'input>>;

#[derive(Clone)]
pub struct GroupingSetContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for GroupingSetContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for GroupingSetContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_groupingSet(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_groupingSet(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for GroupingSetContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_groupingSet }
	//fn type_rule_index() -> usize where Self: Sized { RULE_groupingSet }
}
crate::tid!{GroupingSetContextExt<'a>}

impl<'input> GroupingSetContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<GroupingSetContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,GroupingSetContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait GroupingSetContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<GroupingSetContextExt<'input>>{

fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> GroupingSetContextAttrs<'input> for GroupingSetContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn groupingSet(&mut self,)
	-> Result<Rc<GroupingSetContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = GroupingSetContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 126, RULE_groupingSet);
        let mut _localctx: Rc<GroupingSetContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1952);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(236,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(1939);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					recog.base.set_state(1948);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(235,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule expression*/
							recog.base.set_state(1940);
							recog.expression()?;

							recog.base.set_state(1945);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							while _la==T__2 {
								{
								{
								recog.base.set_state(1941);
								recog.base.match_token(T__2,&mut recog.err_handler)?;

								/*InvokeRule expression*/
								recog.base.set_state(1942);
								recog.expression()?;

								}
								}
								recog.base.set_state(1947);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
							}
							}
						}

						_ => {}
					}
					recog.base.set_state(1950);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					/*InvokeRule expression*/
					recog.base.set_state(1951);
					recog.expression()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- pivotClause ----------------
pub type PivotClauseContextAll<'input> = PivotClauseContext<'input>;


pub type PivotClauseContext<'input> = BaseParserRuleContext<'input,PivotClauseContextExt<'input>>;

#[derive(Clone)]
pub struct PivotClauseContextExt<'input>{
	pub aggregates: Option<Rc<NamedExpressionSeqContextAll<'input>>>,
	pub pivotValue: Option<Rc<PivotValueContextAll<'input>>>,
	pub pivotValues:Vec<Rc<PivotValueContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PivotClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PivotClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_pivotClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_pivotClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PivotClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_pivotClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_pivotClause }
}
crate::tid!{PivotClauseContextExt<'a>}

impl<'input> PivotClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PivotClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PivotClauseContextExt{
				aggregates: None, pivotValue: None, 
				pivotValues: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait PivotClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PivotClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token PIVOT
/// Returns `None` if there is no child corresponding to token PIVOT
fn PIVOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PIVOT, 0)
}
/// Retrieves first TerminalNode corresponding to token FOR
/// Returns `None` if there is no child corresponding to token FOR
fn FOR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FOR, 0)
}
fn pivotColumn(&self) -> Option<Rc<PivotColumnContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token IN
/// Returns `None` if there is no child corresponding to token IN
fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IN, 0)
}
fn namedExpressionSeq(&self) -> Option<Rc<NamedExpressionSeqContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn pivotValue_all(&self) ->  Vec<Rc<PivotValueContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn pivotValue(&self, i: usize) -> Option<Rc<PivotValueContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> PivotClauseContextAttrs<'input> for PivotClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn pivotClause(&mut self,)
	-> Result<Rc<PivotClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PivotClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 128, RULE_pivotClause);
        let mut _localctx: Rc<PivotClauseContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1954);
			recog.base.match_token(PIVOT,&mut recog.err_handler)?;

			recog.base.set_state(1955);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule namedExpressionSeq*/
			recog.base.set_state(1956);
			let tmp = recog.namedExpressionSeq()?;
			 cast_mut::<_,PivotClauseContext >(&mut _localctx).aggregates = Some(tmp.clone());
			  

			recog.base.set_state(1957);
			recog.base.match_token(FOR,&mut recog.err_handler)?;

			/*InvokeRule pivotColumn*/
			recog.base.set_state(1958);
			recog.pivotColumn()?;

			recog.base.set_state(1959);
			recog.base.match_token(IN,&mut recog.err_handler)?;

			recog.base.set_state(1960);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule pivotValue*/
			recog.base.set_state(1961);
			let tmp = recog.pivotValue()?;
			 cast_mut::<_,PivotClauseContext >(&mut _localctx).pivotValue = Some(tmp.clone());
			  

			let temp =  cast_mut::<_,PivotClauseContext >(&mut _localctx).pivotValue.clone().unwrap()
			 ;
			 cast_mut::<_,PivotClauseContext >(&mut _localctx).pivotValues.push(temp);
			  
			recog.base.set_state(1966);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(1962);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule pivotValue*/
				recog.base.set_state(1963);
				let tmp = recog.pivotValue()?;
				 cast_mut::<_,PivotClauseContext >(&mut _localctx).pivotValue = Some(tmp.clone());
				  

				let temp =  cast_mut::<_,PivotClauseContext >(&mut _localctx).pivotValue.clone().unwrap()
				 ;
				 cast_mut::<_,PivotClauseContext >(&mut _localctx).pivotValues.push(temp);
				  
				}
				}
				recog.base.set_state(1968);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(1969);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			recog.base.set_state(1970);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- pivotColumn ----------------
pub type PivotColumnContextAll<'input> = PivotColumnContext<'input>;


pub type PivotColumnContext<'input> = BaseParserRuleContext<'input,PivotColumnContextExt<'input>>;

#[derive(Clone)]
pub struct PivotColumnContextExt<'input>{
	pub identifier: Option<Rc<IdentifierContextAll<'input>>>,
	pub identifiers:Vec<Rc<IdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PivotColumnContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PivotColumnContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_pivotColumn(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_pivotColumn(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PivotColumnContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_pivotColumn }
	//fn type_rule_index() -> usize where Self: Sized { RULE_pivotColumn }
}
crate::tid!{PivotColumnContextExt<'a>}

impl<'input> PivotColumnContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PivotColumnContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PivotColumnContextExt{
				identifier: None, 
				identifiers: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait PivotColumnContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PivotColumnContextExt<'input>>{

fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> PivotColumnContextAttrs<'input> for PivotColumnContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn pivotColumn(&mut self,)
	-> Result<Rc<PivotColumnContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PivotColumnContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 130, RULE_pivotColumn);
        let mut _localctx: Rc<PivotColumnContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(1984);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(239,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule identifier*/
					recog.base.set_state(1972);
					let tmp = recog.identifier()?;
					 cast_mut::<_,PivotColumnContext >(&mut _localctx).identifier = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,PivotColumnContext >(&mut _localctx).identifier.clone().unwrap()
					 ;
					 cast_mut::<_,PivotColumnContext >(&mut _localctx).identifiers.push(temp);
					  
					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(1973);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule identifier*/
					recog.base.set_state(1974);
					let tmp = recog.identifier()?;
					 cast_mut::<_,PivotColumnContext >(&mut _localctx).identifier = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,PivotColumnContext >(&mut _localctx).identifier.clone().unwrap()
					 ;
					 cast_mut::<_,PivotColumnContext >(&mut _localctx).identifiers.push(temp);
					  
					recog.base.set_state(1979);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(1975);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule identifier*/
						recog.base.set_state(1976);
						let tmp = recog.identifier()?;
						 cast_mut::<_,PivotColumnContext >(&mut _localctx).identifier = Some(tmp.clone());
						  

						let temp =  cast_mut::<_,PivotColumnContext >(&mut _localctx).identifier.clone().unwrap()
						 ;
						 cast_mut::<_,PivotColumnContext >(&mut _localctx).identifiers.push(temp);
						  
						}
						}
						recog.base.set_state(1981);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(1982);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- pivotValue ----------------
pub type PivotValueContextAll<'input> = PivotValueContext<'input>;


pub type PivotValueContext<'input> = BaseParserRuleContext<'input,PivotValueContextExt<'input>>;

#[derive(Clone)]
pub struct PivotValueContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PivotValueContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PivotValueContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_pivotValue(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_pivotValue(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PivotValueContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_pivotValue }
	//fn type_rule_index() -> usize where Self: Sized { RULE_pivotValue }
}
crate::tid!{PivotValueContextExt<'a>}

impl<'input> PivotValueContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PivotValueContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PivotValueContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait PivotValueContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PivotValueContextExt<'input>>{

fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}

}

impl<'input> PivotValueContextAttrs<'input> for PivotValueContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn pivotValue(&mut self,)
	-> Result<Rc<PivotValueContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PivotValueContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 132, RULE_pivotValue);
        let mut _localctx: Rc<PivotValueContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule expression*/
			recog.base.set_state(1986);
			recog.expression()?;

			recog.base.set_state(1991);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(241,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1988);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(240,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(1987);
							recog.base.match_token(AS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule identifier*/
					recog.base.set_state(1990);
					recog.identifier()?;

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- lateralView ----------------
pub type LateralViewContextAll<'input> = LateralViewContext<'input>;


pub type LateralViewContext<'input> = BaseParserRuleContext<'input,LateralViewContextExt<'input>>;

#[derive(Clone)]
pub struct LateralViewContextExt<'input>{
	pub tblName: Option<Rc<IdentifierContextAll<'input>>>,
	pub identifier: Option<Rc<IdentifierContextAll<'input>>>,
	pub colName:Vec<Rc<IdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for LateralViewContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LateralViewContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_lateralView(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_lateralView(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for LateralViewContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_lateralView }
	//fn type_rule_index() -> usize where Self: Sized { RULE_lateralView }
}
crate::tid!{LateralViewContextExt<'a>}

impl<'input> LateralViewContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<LateralViewContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,LateralViewContextExt{
				tblName: None, identifier: None, 
				colName: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait LateralViewContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<LateralViewContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token LATERAL
/// Returns `None` if there is no child corresponding to token LATERAL
fn LATERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LATERAL, 0)
}
/// Retrieves first TerminalNode corresponding to token VIEW
/// Returns `None` if there is no child corresponding to token VIEW
fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VIEW, 0)
}
fn qualifiedName(&self) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token OUTER
/// Returns `None` if there is no child corresponding to token OUTER
fn OUTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OUTER, 0)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}

}

impl<'input> LateralViewContextAttrs<'input> for LateralViewContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn lateralView(&mut self,)
	-> Result<Rc<LateralViewContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = LateralViewContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 134, RULE_lateralView);
        let mut _localctx: Rc<LateralViewContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(1993);
			recog.base.match_token(LATERAL,&mut recog.err_handler)?;

			recog.base.set_state(1994);
			recog.base.match_token(VIEW,&mut recog.err_handler)?;

			recog.base.set_state(1996);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(242,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(1995);
					recog.base.match_token(OUTER,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			/*InvokeRule qualifiedName*/
			recog.base.set_state(1998);
			recog.qualifiedName()?;

			recog.base.set_state(1999);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			recog.base.set_state(2008);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(244,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule expression*/
					recog.base.set_state(2000);
					recog.expression()?;

					recog.base.set_state(2005);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(2001);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule expression*/
						recog.base.set_state(2002);
						recog.expression()?;

						}
						}
						recog.base.set_state(2007);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(2010);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			/*InvokeRule identifier*/
			recog.base.set_state(2011);
			let tmp = recog.identifier()?;
			 cast_mut::<_,LateralViewContext >(&mut _localctx).tblName = Some(tmp.clone());
			  

			recog.base.set_state(2023);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(247,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(2013);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(245,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2012);
							recog.base.match_token(AS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule identifier*/
					recog.base.set_state(2015);
					let tmp = recog.identifier()?;
					 cast_mut::<_,LateralViewContext >(&mut _localctx).identifier = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,LateralViewContext >(&mut _localctx).identifier.clone().unwrap()
					 ;
					 cast_mut::<_,LateralViewContext >(&mut _localctx).colName.push(temp);
					  
					recog.base.set_state(2020);
					recog.err_handler.sync(&mut recog.base)?;
					_alt = recog.interpreter.adaptive_predict(246,&mut recog.base)?;
					while { _alt!=2 && _alt!=INVALID_ALT } {
						if _alt==1 {
							{
							{
							recog.base.set_state(2016);
							recog.base.match_token(T__2,&mut recog.err_handler)?;

							/*InvokeRule identifier*/
							recog.base.set_state(2017);
							let tmp = recog.identifier()?;
							 cast_mut::<_,LateralViewContext >(&mut _localctx).identifier = Some(tmp.clone());
							  

							let temp =  cast_mut::<_,LateralViewContext >(&mut _localctx).identifier.clone().unwrap()
							 ;
							 cast_mut::<_,LateralViewContext >(&mut _localctx).colName.push(temp);
							  
							}
							} 
						}
						recog.base.set_state(2022);
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(246,&mut recog.base)?;
					}
					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- setQuantifier ----------------
pub type SetQuantifierContextAll<'input> = SetQuantifierContext<'input>;


pub type SetQuantifierContext<'input> = BaseParserRuleContext<'input,SetQuantifierContextExt<'input>>;

#[derive(Clone)]
pub struct SetQuantifierContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SetQuantifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SetQuantifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_setQuantifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_setQuantifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SetQuantifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_setQuantifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_setQuantifier }
}
crate::tid!{SetQuantifierContextExt<'a>}

impl<'input> SetQuantifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SetQuantifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SetQuantifierContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SetQuantifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SetQuantifierContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token DISTINCT
/// Returns `None` if there is no child corresponding to token DISTINCT
fn DISTINCT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DISTINCT, 0)
}
/// Retrieves first TerminalNode corresponding to token ALL
/// Returns `None` if there is no child corresponding to token ALL
fn ALL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ALL, 0)
}

}

impl<'input> SetQuantifierContextAttrs<'input> for SetQuantifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn setQuantifier(&mut self,)
	-> Result<Rc<SetQuantifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SetQuantifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 136, RULE_setQuantifier);
        let mut _localctx: Rc<SetQuantifierContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2025);
			_la = recog.base.input.la(1);
			if { !(_la==ALL || _la==DISTINCT) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- relation ----------------
pub type RelationContextAll<'input> = RelationContext<'input>;


pub type RelationContext<'input> = BaseParserRuleContext<'input,RelationContextExt<'input>>;

#[derive(Clone)]
pub struct RelationContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for RelationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RelationContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_relation(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_relation(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for RelationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_relation }
	//fn type_rule_index() -> usize where Self: Sized { RULE_relation }
}
crate::tid!{RelationContextExt<'a>}

impl<'input> RelationContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<RelationContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,RelationContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait RelationContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<RelationContextExt<'input>>{

fn relationPrimary(&self) -> Option<Rc<RelationPrimaryContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn joinRelation_all(&self) ->  Vec<Rc<JoinRelationContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn joinRelation(&self, i: usize) -> Option<Rc<JoinRelationContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> RelationContextAttrs<'input> for RelationContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn relation(&mut self,)
	-> Result<Rc<RelationContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = RelationContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 138, RULE_relation);
        let mut _localctx: Rc<RelationContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule relationPrimary*/
			recog.base.set_state(2027);
			recog.relationPrimary()?;

			recog.base.set_state(2031);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(248,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					/*InvokeRule joinRelation*/
					recog.base.set_state(2028);
					recog.joinRelation()?;

					}
					} 
				}
				recog.base.set_state(2033);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(248,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- joinRelation ----------------
pub type JoinRelationContextAll<'input> = JoinRelationContext<'input>;


pub type JoinRelationContext<'input> = BaseParserRuleContext<'input,JoinRelationContextExt<'input>>;

#[derive(Clone)]
pub struct JoinRelationContextExt<'input>{
	pub right: Option<Rc<RelationPrimaryContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for JoinRelationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for JoinRelationContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_joinRelation(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_joinRelation(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for JoinRelationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_joinRelation }
	//fn type_rule_index() -> usize where Self: Sized { RULE_joinRelation }
}
crate::tid!{JoinRelationContextExt<'a>}

impl<'input> JoinRelationContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<JoinRelationContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,JoinRelationContextExt{
				right: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait JoinRelationContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<JoinRelationContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token JOIN
/// Returns `None` if there is no child corresponding to token JOIN
fn JOIN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(JOIN, 0)
}
fn relationPrimary(&self) -> Option<Rc<RelationPrimaryContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn joinType(&self) -> Option<Rc<JoinTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn joinCriteria(&self) -> Option<Rc<JoinCriteriaContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token NATURAL
/// Returns `None` if there is no child corresponding to token NATURAL
fn NATURAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NATURAL, 0)
}

}

impl<'input> JoinRelationContextAttrs<'input> for JoinRelationContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn joinRelation(&mut self,)
	-> Result<Rc<JoinRelationContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = JoinRelationContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 140, RULE_joinRelation);
        let mut _localctx: Rc<JoinRelationContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2045);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 ANTI | CROSS | FULL | INNER | JOIN | LEFT | RIGHT | SEMI 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					{
					/*InvokeRule joinType*/
					recog.base.set_state(2034);
					recog.joinType()?;

					}
					recog.base.set_state(2035);
					recog.base.match_token(JOIN,&mut recog.err_handler)?;

					/*InvokeRule relationPrimary*/
					recog.base.set_state(2036);
					let tmp = recog.relationPrimary()?;
					 cast_mut::<_,JoinRelationContext >(&mut _localctx).right = Some(tmp.clone());
					  

					recog.base.set_state(2038);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(249,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule joinCriteria*/
							recog.base.set_state(2037);
							recog.joinCriteria()?;

							}
						}

						_ => {}
					}
					}
				}

			 NATURAL 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2040);
					recog.base.match_token(NATURAL,&mut recog.err_handler)?;

					/*InvokeRule joinType*/
					recog.base.set_state(2041);
					recog.joinType()?;

					recog.base.set_state(2042);
					recog.base.match_token(JOIN,&mut recog.err_handler)?;

					/*InvokeRule relationPrimary*/
					recog.base.set_state(2043);
					let tmp = recog.relationPrimary()?;
					 cast_mut::<_,JoinRelationContext >(&mut _localctx).right = Some(tmp.clone());
					  

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- joinType ----------------
pub type JoinTypeContextAll<'input> = JoinTypeContext<'input>;


pub type JoinTypeContext<'input> = BaseParserRuleContext<'input,JoinTypeContextExt<'input>>;

#[derive(Clone)]
pub struct JoinTypeContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for JoinTypeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for JoinTypeContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_joinType(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_joinType(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for JoinTypeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_joinType }
	//fn type_rule_index() -> usize where Self: Sized { RULE_joinType }
}
crate::tid!{JoinTypeContextExt<'a>}

impl<'input> JoinTypeContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<JoinTypeContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,JoinTypeContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait JoinTypeContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<JoinTypeContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token INNER
/// Returns `None` if there is no child corresponding to token INNER
fn INNER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INNER, 0)
}
/// Retrieves first TerminalNode corresponding to token CROSS
/// Returns `None` if there is no child corresponding to token CROSS
fn CROSS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CROSS, 0)
}
/// Retrieves first TerminalNode corresponding to token LEFT
/// Returns `None` if there is no child corresponding to token LEFT
fn LEFT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LEFT, 0)
}
/// Retrieves first TerminalNode corresponding to token OUTER
/// Returns `None` if there is no child corresponding to token OUTER
fn OUTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OUTER, 0)
}
/// Retrieves first TerminalNode corresponding to token SEMI
/// Returns `None` if there is no child corresponding to token SEMI
fn SEMI(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SEMI, 0)
}
/// Retrieves first TerminalNode corresponding to token RIGHT
/// Returns `None` if there is no child corresponding to token RIGHT
fn RIGHT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RIGHT, 0)
}
/// Retrieves first TerminalNode corresponding to token FULL
/// Returns `None` if there is no child corresponding to token FULL
fn FULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FULL, 0)
}
/// Retrieves first TerminalNode corresponding to token ANTI
/// Returns `None` if there is no child corresponding to token ANTI
fn ANTI(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ANTI, 0)
}

}

impl<'input> JoinTypeContextAttrs<'input> for JoinTypeContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn joinType(&mut self,)
	-> Result<Rc<JoinTypeContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = JoinTypeContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 142, RULE_joinType);
        let mut _localctx: Rc<JoinTypeContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2071);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(257,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(2048);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==INNER {
						{
						recog.base.set_state(2047);
						recog.base.match_token(INNER,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2050);
					recog.base.match_token(CROSS,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					recog.base.set_state(2051);
					recog.base.match_token(LEFT,&mut recog.err_handler)?;

					recog.base.set_state(2053);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OUTER {
						{
						recog.base.set_state(2052);
						recog.base.match_token(OUTER,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				4 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					recog.base.set_state(2056);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LEFT {
						{
						recog.base.set_state(2055);
						recog.base.match_token(LEFT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2058);
					recog.base.match_token(SEMI,&mut recog.err_handler)?;

					}
				}
			,
				5 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 5);
					recog.base.enter_outer_alt(None, 5);
					{
					recog.base.set_state(2059);
					recog.base.match_token(RIGHT,&mut recog.err_handler)?;

					recog.base.set_state(2061);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OUTER {
						{
						recog.base.set_state(2060);
						recog.base.match_token(OUTER,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				6 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 6);
					recog.base.enter_outer_alt(None, 6);
					{
					recog.base.set_state(2063);
					recog.base.match_token(FULL,&mut recog.err_handler)?;

					recog.base.set_state(2065);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==OUTER {
						{
						recog.base.set_state(2064);
						recog.base.match_token(OUTER,&mut recog.err_handler)?;

						}
					}

					}
				}
			,
				7 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 7);
					recog.base.enter_outer_alt(None, 7);
					{
					recog.base.set_state(2068);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==LEFT {
						{
						recog.base.set_state(2067);
						recog.base.match_token(LEFT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2070);
					recog.base.match_token(ANTI,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- joinCriteria ----------------
pub type JoinCriteriaContextAll<'input> = JoinCriteriaContext<'input>;


pub type JoinCriteriaContext<'input> = BaseParserRuleContext<'input,JoinCriteriaContextExt<'input>>;

#[derive(Clone)]
pub struct JoinCriteriaContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for JoinCriteriaContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for JoinCriteriaContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_joinCriteria(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_joinCriteria(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for JoinCriteriaContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_joinCriteria }
	//fn type_rule_index() -> usize where Self: Sized { RULE_joinCriteria }
}
crate::tid!{JoinCriteriaContextExt<'a>}

impl<'input> JoinCriteriaContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<JoinCriteriaContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,JoinCriteriaContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait JoinCriteriaContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<JoinCriteriaContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token ON
/// Returns `None` if there is no child corresponding to token ON
fn ON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ON, 0)
}
fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token USING
/// Returns `None` if there is no child corresponding to token USING
fn USING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(USING, 0)
}
fn identifierList(&self) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> JoinCriteriaContextAttrs<'input> for JoinCriteriaContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn joinCriteria(&mut self,)
	-> Result<Rc<JoinCriteriaContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = JoinCriteriaContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 144, RULE_joinCriteria);
        let mut _localctx: Rc<JoinCriteriaContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2077);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 ON 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(2073);
					recog.base.match_token(ON,&mut recog.err_handler)?;

					/*InvokeRule booleanExpression*/
					recog.base.set_state(2074);
					recog.booleanExpression_rec(0)?;

					}
				}

			 USING 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2075);
					recog.base.match_token(USING,&mut recog.err_handler)?;

					/*InvokeRule identifierList*/
					recog.base.set_state(2076);
					recog.identifierList()?;

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- sample ----------------
pub type SampleContextAll<'input> = SampleContext<'input>;


pub type SampleContext<'input> = BaseParserRuleContext<'input,SampleContextExt<'input>>;

#[derive(Clone)]
pub struct SampleContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SampleContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SampleContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_sample(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_sample(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for SampleContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_sample }
	//fn type_rule_index() -> usize where Self: Sized { RULE_sample }
}
crate::tid!{SampleContextExt<'a>}

impl<'input> SampleContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SampleContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SampleContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait SampleContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SampleContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token TABLESAMPLE
/// Returns `None` if there is no child corresponding to token TABLESAMPLE
fn TABLESAMPLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLESAMPLE, 0)
}
fn sampleMethod(&self) -> Option<Rc<SampleMethodContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> SampleContextAttrs<'input> for SampleContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn sample(&mut self,)
	-> Result<Rc<SampleContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SampleContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 146, RULE_sample);
        let mut _localctx: Rc<SampleContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2079);
			recog.base.match_token(TABLESAMPLE,&mut recog.err_handler)?;

			recog.base.set_state(2080);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			recog.base.set_state(2082);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(259,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule sampleMethod*/
					recog.base.set_state(2081);
					recog.sampleMethod()?;

					}
				}

				_ => {}
			}
			recog.base.set_state(2084);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- sampleMethod ----------------
#[derive(Debug)]
pub enum SampleMethodContextAll<'input>{
	SampleByRowsContext(SampleByRowsContext<'input>),
	SampleByPercentileContext(SampleByPercentileContext<'input>),
	SampleByBucketContext(SampleByBucketContext<'input>),
	SampleByBytesContext(SampleByBytesContext<'input>),
Error(SampleMethodContext<'input>)
}
crate::tid!{SampleMethodContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for SampleMethodContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for SampleMethodContextAll<'input>{}

impl<'input> Deref for SampleMethodContextAll<'input>{
	type Target = dyn SampleMethodContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use SampleMethodContextAll::*;
		match self{
			SampleByRowsContext(inner) => inner,
			SampleByPercentileContext(inner) => inner,
			SampleByBucketContext(inner) => inner,
			SampleByBytesContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SampleMethodContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type SampleMethodContext<'input> = BaseParserRuleContext<'input,SampleMethodContextExt<'input>>;

#[derive(Clone)]
pub struct SampleMethodContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for SampleMethodContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SampleMethodContext<'input>{
}

impl<'input> CustomRuleContext<'input> for SampleMethodContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_sampleMethod }
	//fn type_rule_index() -> usize where Self: Sized { RULE_sampleMethod }
}
crate::tid!{SampleMethodContextExt<'a>}

impl<'input> SampleMethodContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<SampleMethodContextAll<'input>> {
		Rc::new(
		SampleMethodContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,SampleMethodContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait SampleMethodContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<SampleMethodContextExt<'input>>{


}

impl<'input> SampleMethodContextAttrs<'input> for SampleMethodContext<'input>{}

pub type SampleByRowsContext<'input> = BaseParserRuleContext<'input,SampleByRowsContextExt<'input>>;

pub trait SampleByRowsContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token ROWS
	/// Returns `None` if there is no child corresponding to token ROWS
	fn ROWS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ROWS, 0)
	}
}

impl<'input> SampleByRowsContextAttrs<'input> for SampleByRowsContext<'input>{}

pub struct SampleByRowsContextExt<'input>{
	base:SampleMethodContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SampleByRowsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SampleByRowsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SampleByRowsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_sampleByRows(self);
	}
}

impl<'input> CustomRuleContext<'input> for SampleByRowsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_sampleMethod }
	//fn type_rule_index() -> usize where Self: Sized { RULE_sampleMethod }
}

impl<'input> Borrow<SampleMethodContextExt<'input>> for SampleByRowsContext<'input>{
	fn borrow(&self) -> &SampleMethodContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<SampleMethodContextExt<'input>> for SampleByRowsContext<'input>{
	fn borrow_mut(&mut self) -> &mut SampleMethodContextExt<'input> { &mut self.base }
}

impl<'input> SampleMethodContextAttrs<'input> for SampleByRowsContext<'input> {}

impl<'input> SampleByRowsContextExt<'input>{
	fn new(ctx: &dyn SampleMethodContextAttrs<'input>) -> Rc<SampleMethodContextAll<'input>>  {
		Rc::new(
			SampleMethodContextAll::SampleByRowsContext(
				BaseParserRuleContext::copy_from(ctx,SampleByRowsContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SampleByPercentileContext<'input> = BaseParserRuleContext<'input,SampleByPercentileContextExt<'input>>;

pub trait SampleByPercentileContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token PERCENTLIT
	/// Returns `None` if there is no child corresponding to token PERCENTLIT
	fn PERCENTLIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PERCENTLIT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token INTEGER_VALUE
	/// Returns `None` if there is no child corresponding to token INTEGER_VALUE
	fn INTEGER_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTEGER_VALUE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DECIMAL_VALUE
	/// Returns `None` if there is no child corresponding to token DECIMAL_VALUE
	fn DECIMAL_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DECIMAL_VALUE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> SampleByPercentileContextAttrs<'input> for SampleByPercentileContext<'input>{}

pub struct SampleByPercentileContextExt<'input>{
	base:SampleMethodContextExt<'input>,
	pub negativeSign: Option<TokenType<'input>>,
	pub percentage: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SampleByPercentileContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SampleByPercentileContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SampleByPercentileContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_sampleByPercentile(self);
	}
}

impl<'input> CustomRuleContext<'input> for SampleByPercentileContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_sampleMethod }
	//fn type_rule_index() -> usize where Self: Sized { RULE_sampleMethod }
}

impl<'input> Borrow<SampleMethodContextExt<'input>> for SampleByPercentileContext<'input>{
	fn borrow(&self) -> &SampleMethodContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<SampleMethodContextExt<'input>> for SampleByPercentileContext<'input>{
	fn borrow_mut(&mut self) -> &mut SampleMethodContextExt<'input> { &mut self.base }
}

impl<'input> SampleMethodContextAttrs<'input> for SampleByPercentileContext<'input> {}

impl<'input> SampleByPercentileContextExt<'input>{
	fn new(ctx: &dyn SampleMethodContextAttrs<'input>) -> Rc<SampleMethodContextAll<'input>>  {
		Rc::new(
			SampleMethodContextAll::SampleByPercentileContext(
				BaseParserRuleContext::copy_from(ctx,SampleByPercentileContextExt{
					negativeSign:None, percentage:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SampleByBucketContext<'input> = BaseParserRuleContext<'input,SampleByBucketContextExt<'input>>;

pub trait SampleByBucketContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token OUT
	/// Returns `None` if there is no child corresponding to token OUT
	fn OUT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OUT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OF
	/// Returns `None` if there is no child corresponding to token OF
	fn OF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OF, 0)
	}
	/// Retrieves first TerminalNode corresponding to token BUCKET
	/// Returns `None` if there is no child corresponding to token BUCKET
	fn BUCKET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(BUCKET, 0)
	}
	/// Retrieves all `TerminalNode`s corresponding to token INTEGER_VALUE in current rule
	fn INTEGER_VALUE_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token INTEGER_VALUE, starting from 0.
	/// Returns `None` if number of children corresponding to token INTEGER_VALUE is less or equal than `i`.
	fn INTEGER_VALUE(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTEGER_VALUE, i)
	}
	/// Retrieves first TerminalNode corresponding to token ON
	/// Returns `None` if there is no child corresponding to token ON
	fn ON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ON, 0)
	}
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn qualifiedName(&self) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SampleByBucketContextAttrs<'input> for SampleByBucketContext<'input>{}

pub struct SampleByBucketContextExt<'input>{
	base:SampleMethodContextExt<'input>,
	pub sampleType: Option<TokenType<'input>>,
	pub numerator: Option<TokenType<'input>>,
	pub denominator: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SampleByBucketContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SampleByBucketContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SampleByBucketContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_sampleByBucket(self);
	}
}

impl<'input> CustomRuleContext<'input> for SampleByBucketContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_sampleMethod }
	//fn type_rule_index() -> usize where Self: Sized { RULE_sampleMethod }
}

impl<'input> Borrow<SampleMethodContextExt<'input>> for SampleByBucketContext<'input>{
	fn borrow(&self) -> &SampleMethodContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<SampleMethodContextExt<'input>> for SampleByBucketContext<'input>{
	fn borrow_mut(&mut self) -> &mut SampleMethodContextExt<'input> { &mut self.base }
}

impl<'input> SampleMethodContextAttrs<'input> for SampleByBucketContext<'input> {}

impl<'input> SampleByBucketContextExt<'input>{
	fn new(ctx: &dyn SampleMethodContextAttrs<'input>) -> Rc<SampleMethodContextAll<'input>>  {
		Rc::new(
			SampleMethodContextAll::SampleByBucketContext(
				BaseParserRuleContext::copy_from(ctx,SampleByBucketContextExt{
					sampleType:None, numerator:None, denominator:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SampleByBytesContext<'input> = BaseParserRuleContext<'input,SampleByBytesContextExt<'input>>;

pub trait SampleByBytesContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SampleByBytesContextAttrs<'input> for SampleByBytesContext<'input>{}

pub struct SampleByBytesContextExt<'input>{
	base:SampleMethodContextExt<'input>,
	pub bytes: Option<Rc<ExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SampleByBytesContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SampleByBytesContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SampleByBytesContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_sampleByBytes(self);
	}
}

impl<'input> CustomRuleContext<'input> for SampleByBytesContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_sampleMethod }
	//fn type_rule_index() -> usize where Self: Sized { RULE_sampleMethod }
}

impl<'input> Borrow<SampleMethodContextExt<'input>> for SampleByBytesContext<'input>{
	fn borrow(&self) -> &SampleMethodContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<SampleMethodContextExt<'input>> for SampleByBytesContext<'input>{
	fn borrow_mut(&mut self) -> &mut SampleMethodContextExt<'input> { &mut self.base }
}

impl<'input> SampleMethodContextAttrs<'input> for SampleByBytesContext<'input> {}

impl<'input> SampleByBytesContextExt<'input>{
	fn new(ctx: &dyn SampleMethodContextAttrs<'input>) -> Rc<SampleMethodContextAll<'input>>  {
		Rc::new(
			SampleMethodContextAll::SampleByBytesContext(
				BaseParserRuleContext::copy_from(ctx,SampleByBytesContextExt{
        			bytes:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn sampleMethod(&mut self,)
	-> Result<Rc<SampleMethodContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = SampleMethodContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 148, RULE_sampleMethod);
        let mut _localctx: Rc<SampleMethodContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2110);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(263,&mut recog.base)? {
				1 =>{
					let tmp = SampleByPercentileContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(2087);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2086);
						let tmp = recog.base.match_token(MINUS,&mut recog.err_handler)?;
						if let SampleMethodContextAll::SampleByPercentileContext(ctx) = cast_mut::<_,SampleMethodContextAll >(&mut _localctx){
						ctx.negativeSign = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					recog.base.set_state(2089);
					if let SampleMethodContextAll::SampleByPercentileContext(ctx) = cast_mut::<_,SampleMethodContextAll >(&mut _localctx){
					ctx.percentage = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
					_la = recog.base.input.la(1);
					if { !(_la==INTEGER_VALUE || _la==DECIMAL_VALUE) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						if let SampleMethodContextAll::SampleByPercentileContext(ctx) = cast_mut::<_,SampleMethodContextAll >(&mut _localctx){
						ctx.percentage = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(2090);
					recog.base.match_token(PERCENTLIT,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					let tmp = SampleByRowsContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule expression*/
					recog.base.set_state(2091);
					recog.expression()?;

					recog.base.set_state(2092);
					recog.base.match_token(ROWS,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					let tmp = SampleByBucketContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(2094);
					let tmp = recog.base.match_token(BUCKET,&mut recog.err_handler)?;
					if let SampleMethodContextAll::SampleByBucketContext(ctx) = cast_mut::<_,SampleMethodContextAll >(&mut _localctx){
					ctx.sampleType = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2095);
					let tmp = recog.base.match_token(INTEGER_VALUE,&mut recog.err_handler)?;
					if let SampleMethodContextAll::SampleByBucketContext(ctx) = cast_mut::<_,SampleMethodContextAll >(&mut _localctx){
					ctx.numerator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2096);
					recog.base.match_token(OUT,&mut recog.err_handler)?;

					recog.base.set_state(2097);
					recog.base.match_token(OF,&mut recog.err_handler)?;

					recog.base.set_state(2098);
					let tmp = recog.base.match_token(INTEGER_VALUE,&mut recog.err_handler)?;
					if let SampleMethodContextAll::SampleByBucketContext(ctx) = cast_mut::<_,SampleMethodContextAll >(&mut _localctx){
					ctx.denominator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2107);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==ON {
						{
						recog.base.set_state(2099);
						recog.base.match_token(ON,&mut recog.err_handler)?;

						recog.base.set_state(2105);
						recog.err_handler.sync(&mut recog.base)?;
						match  recog.interpreter.adaptive_predict(261,&mut recog.base)? {
							1 =>{
								{
								/*InvokeRule identifier*/
								recog.base.set_state(2100);
								recog.identifier()?;

								}
							}
						,
							2 =>{
								{
								/*InvokeRule qualifiedName*/
								recog.base.set_state(2101);
								recog.qualifiedName()?;

								recog.base.set_state(2102);
								recog.base.match_token(T__0,&mut recog.err_handler)?;

								recog.base.set_state(2103);
								recog.base.match_token(T__1,&mut recog.err_handler)?;

								}
							}

							_ => {}
						}
						}
					}

					}
				}
			,
				4 =>{
					let tmp = SampleByBytesContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					/*InvokeRule expression*/
					recog.base.set_state(2109);
					let tmp = recog.expression()?;
					if let SampleMethodContextAll::SampleByBytesContext(ctx) = cast_mut::<_,SampleMethodContextAll >(&mut _localctx){
					ctx.bytes = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- identifierList ----------------
pub type IdentifierListContextAll<'input> = IdentifierListContext<'input>;


pub type IdentifierListContext<'input> = BaseParserRuleContext<'input,IdentifierListContextExt<'input>>;

#[derive(Clone)]
pub struct IdentifierListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for IdentifierListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IdentifierListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_identifierList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_identifierList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for IdentifierListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_identifierList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_identifierList }
}
crate::tid!{IdentifierListContextExt<'a>}

impl<'input> IdentifierListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<IdentifierListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,IdentifierListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait IdentifierListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<IdentifierListContextExt<'input>>{

fn identifierSeq(&self) -> Option<Rc<IdentifierSeqContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> IdentifierListContextAttrs<'input> for IdentifierListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn identifierList(&mut self,)
	-> Result<Rc<IdentifierListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = IdentifierListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 150, RULE_identifierList);
        let mut _localctx: Rc<IdentifierListContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2112);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule identifierSeq*/
			recog.base.set_state(2113);
			recog.identifierSeq()?;

			recog.base.set_state(2114);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- identifierSeq ----------------
pub type IdentifierSeqContextAll<'input> = IdentifierSeqContext<'input>;


pub type IdentifierSeqContext<'input> = BaseParserRuleContext<'input,IdentifierSeqContextExt<'input>>;

#[derive(Clone)]
pub struct IdentifierSeqContextExt<'input>{
	pub errorCapturingIdentifier: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
	pub ident:Vec<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for IdentifierSeqContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IdentifierSeqContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_identifierSeq(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_identifierSeq(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for IdentifierSeqContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_identifierSeq }
	//fn type_rule_index() -> usize where Self: Sized { RULE_identifierSeq }
}
crate::tid!{IdentifierSeqContextExt<'a>}

impl<'input> IdentifierSeqContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<IdentifierSeqContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,IdentifierSeqContextExt{
				errorCapturingIdentifier: None, 
				ident: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait IdentifierSeqContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<IdentifierSeqContextExt<'input>>{

fn errorCapturingIdentifier_all(&self) ->  Vec<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn errorCapturingIdentifier(&self, i: usize) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> IdentifierSeqContextAttrs<'input> for IdentifierSeqContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn identifierSeq(&mut self,)
	-> Result<Rc<IdentifierSeqContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = IdentifierSeqContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 152, RULE_identifierSeq);
        let mut _localctx: Rc<IdentifierSeqContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(2116);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,IdentifierSeqContext >(&mut _localctx).errorCapturingIdentifier = Some(tmp.clone());
			  

			let temp =  cast_mut::<_,IdentifierSeqContext >(&mut _localctx).errorCapturingIdentifier.clone().unwrap()
			 ;
			 cast_mut::<_,IdentifierSeqContext >(&mut _localctx).ident.push(temp);
			  
			recog.base.set_state(2121);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(264,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(2117);
					recog.base.match_token(T__2,&mut recog.err_handler)?;

					/*InvokeRule errorCapturingIdentifier*/
					recog.base.set_state(2118);
					let tmp = recog.errorCapturingIdentifier()?;
					 cast_mut::<_,IdentifierSeqContext >(&mut _localctx).errorCapturingIdentifier = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,IdentifierSeqContext >(&mut _localctx).errorCapturingIdentifier.clone().unwrap()
					 ;
					 cast_mut::<_,IdentifierSeqContext >(&mut _localctx).ident.push(temp);
					  
					}
					} 
				}
				recog.base.set_state(2123);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(264,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- orderedIdentifierList ----------------
pub type OrderedIdentifierListContextAll<'input> = OrderedIdentifierListContext<'input>;


pub type OrderedIdentifierListContext<'input> = BaseParserRuleContext<'input,OrderedIdentifierListContextExt<'input>>;

#[derive(Clone)]
pub struct OrderedIdentifierListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for OrderedIdentifierListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for OrderedIdentifierListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_orderedIdentifierList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_orderedIdentifierList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for OrderedIdentifierListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_orderedIdentifierList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_orderedIdentifierList }
}
crate::tid!{OrderedIdentifierListContextExt<'a>}

impl<'input> OrderedIdentifierListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<OrderedIdentifierListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,OrderedIdentifierListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait OrderedIdentifierListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<OrderedIdentifierListContextExt<'input>>{

fn orderedIdentifier_all(&self) ->  Vec<Rc<OrderedIdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn orderedIdentifier(&self, i: usize) -> Option<Rc<OrderedIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> OrderedIdentifierListContextAttrs<'input> for OrderedIdentifierListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn orderedIdentifierList(&mut self,)
	-> Result<Rc<OrderedIdentifierListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = OrderedIdentifierListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 154, RULE_orderedIdentifierList);
        let mut _localctx: Rc<OrderedIdentifierListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2124);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule orderedIdentifier*/
			recog.base.set_state(2125);
			recog.orderedIdentifier()?;

			recog.base.set_state(2130);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(2126);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule orderedIdentifier*/
				recog.base.set_state(2127);
				recog.orderedIdentifier()?;

				}
				}
				recog.base.set_state(2132);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(2133);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- orderedIdentifier ----------------
pub type OrderedIdentifierContextAll<'input> = OrderedIdentifierContext<'input>;


pub type OrderedIdentifierContext<'input> = BaseParserRuleContext<'input,OrderedIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct OrderedIdentifierContextExt<'input>{
	pub ident: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
	pub ordering: Option<TokenType<'input>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for OrderedIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for OrderedIdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_orderedIdentifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_orderedIdentifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for OrderedIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_orderedIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_orderedIdentifier }
}
crate::tid!{OrderedIdentifierContextExt<'a>}

impl<'input> OrderedIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<OrderedIdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,OrderedIdentifierContextExt{
				ordering: None, 
				ident: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait OrderedIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<OrderedIdentifierContextExt<'input>>{

fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token ASC
/// Returns `None` if there is no child corresponding to token ASC
fn ASC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ASC, 0)
}
/// Retrieves first TerminalNode corresponding to token DESC
/// Returns `None` if there is no child corresponding to token DESC
fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DESC, 0)
}

}

impl<'input> OrderedIdentifierContextAttrs<'input> for OrderedIdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn orderedIdentifier(&mut self,)
	-> Result<Rc<OrderedIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = OrderedIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 156, RULE_orderedIdentifier);
        let mut _localctx: Rc<OrderedIdentifierContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(2135);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,OrderedIdentifierContext >(&mut _localctx).ident = Some(tmp.clone());
			  

			recog.base.set_state(2137);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==ASC || _la==DESC {
				{
				recog.base.set_state(2136);
				 cast_mut::<_,OrderedIdentifierContext >(&mut _localctx).ordering = recog.base.input.lt(1).cloned();
				 
				_la = recog.base.input.la(1);
				if { !(_la==ASC || _la==DESC) } {
					let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
					 cast_mut::<_,OrderedIdentifierContext >(&mut _localctx).ordering = Some(tmp.clone());
					  

				}
				else {
					if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
					recog.err_handler.report_match(&mut recog.base);
					recog.base.consume(&mut recog.err_handler);
				}
				}
			}

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- identifierCommentList ----------------
pub type IdentifierCommentListContextAll<'input> = IdentifierCommentListContext<'input>;


pub type IdentifierCommentListContext<'input> = BaseParserRuleContext<'input,IdentifierCommentListContextExt<'input>>;

#[derive(Clone)]
pub struct IdentifierCommentListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for IdentifierCommentListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IdentifierCommentListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_identifierCommentList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_identifierCommentList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for IdentifierCommentListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_identifierCommentList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_identifierCommentList }
}
crate::tid!{IdentifierCommentListContextExt<'a>}

impl<'input> IdentifierCommentListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<IdentifierCommentListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,IdentifierCommentListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait IdentifierCommentListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<IdentifierCommentListContextExt<'input>>{

fn identifierComment_all(&self) ->  Vec<Rc<IdentifierCommentContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifierComment(&self, i: usize) -> Option<Rc<IdentifierCommentContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> IdentifierCommentListContextAttrs<'input> for IdentifierCommentListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn identifierCommentList(&mut self,)
	-> Result<Rc<IdentifierCommentListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = IdentifierCommentListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 158, RULE_identifierCommentList);
        let mut _localctx: Rc<IdentifierCommentListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2139);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule identifierComment*/
			recog.base.set_state(2140);
			recog.identifierComment()?;

			recog.base.set_state(2145);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(2141);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule identifierComment*/
				recog.base.set_state(2142);
				recog.identifierComment()?;

				}
				}
				recog.base.set_state(2147);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(2148);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- identifierComment ----------------
pub type IdentifierCommentContextAll<'input> = IdentifierCommentContext<'input>;


pub type IdentifierCommentContext<'input> = BaseParserRuleContext<'input,IdentifierCommentContextExt<'input>>;

#[derive(Clone)]
pub struct IdentifierCommentContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for IdentifierCommentContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IdentifierCommentContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_identifierComment(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_identifierComment(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for IdentifierCommentContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_identifierComment }
	//fn type_rule_index() -> usize where Self: Sized { RULE_identifierComment }
}
crate::tid!{IdentifierCommentContextExt<'a>}

impl<'input> IdentifierCommentContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<IdentifierCommentContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,IdentifierCommentContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait IdentifierCommentContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<IdentifierCommentContextExt<'input>>{

fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn commentSpec(&self) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> IdentifierCommentContextAttrs<'input> for IdentifierCommentContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn identifierComment(&mut self,)
	-> Result<Rc<IdentifierCommentContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = IdentifierCommentContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 160, RULE_identifierComment);
        let mut _localctx: Rc<IdentifierCommentContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule identifier*/
			recog.base.set_state(2150);
			recog.identifier()?;

			recog.base.set_state(2152);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==COMMENT {
				{
				/*InvokeRule commentSpec*/
				recog.base.set_state(2151);
				recog.commentSpec()?;

				}
			}

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- relationPrimary ----------------
#[derive(Debug)]
pub enum RelationPrimaryContextAll<'input>{
	TableValuedFunctionContext(TableValuedFunctionContext<'input>),
	InlineTableDefault2Context(InlineTableDefault2Context<'input>),
	AliasedRelationContext(AliasedRelationContext<'input>),
	AliasedQueryContext(AliasedQueryContext<'input>),
	TableNameContext(TableNameContext<'input>),
Error(RelationPrimaryContext<'input>)
}
crate::tid!{RelationPrimaryContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for RelationPrimaryContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for RelationPrimaryContextAll<'input>{}

impl<'input> Deref for RelationPrimaryContextAll<'input>{
	type Target = dyn RelationPrimaryContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use RelationPrimaryContextAll::*;
		match self{
			TableValuedFunctionContext(inner) => inner,
			InlineTableDefault2Context(inner) => inner,
			AliasedRelationContext(inner) => inner,
			AliasedQueryContext(inner) => inner,
			TableNameContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RelationPrimaryContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type RelationPrimaryContext<'input> = BaseParserRuleContext<'input,RelationPrimaryContextExt<'input>>;

#[derive(Clone)]
pub struct RelationPrimaryContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for RelationPrimaryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RelationPrimaryContext<'input>{
}

impl<'input> CustomRuleContext<'input> for RelationPrimaryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_relationPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_relationPrimary }
}
crate::tid!{RelationPrimaryContextExt<'a>}

impl<'input> RelationPrimaryContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<RelationPrimaryContextAll<'input>> {
		Rc::new(
		RelationPrimaryContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,RelationPrimaryContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait RelationPrimaryContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<RelationPrimaryContextExt<'input>>{


}

impl<'input> RelationPrimaryContextAttrs<'input> for RelationPrimaryContext<'input>{}

pub type TableValuedFunctionContext<'input> = BaseParserRuleContext<'input,TableValuedFunctionContextExt<'input>>;

pub trait TableValuedFunctionContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn functionTable(&self) -> Option<Rc<FunctionTableContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> TableValuedFunctionContextAttrs<'input> for TableValuedFunctionContext<'input>{}

pub struct TableValuedFunctionContextExt<'input>{
	base:RelationPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{TableValuedFunctionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TableValuedFunctionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TableValuedFunctionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_tableValuedFunction(self);
	}
}

impl<'input> CustomRuleContext<'input> for TableValuedFunctionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_relationPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_relationPrimary }
}

impl<'input> Borrow<RelationPrimaryContextExt<'input>> for TableValuedFunctionContext<'input>{
	fn borrow(&self) -> &RelationPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<RelationPrimaryContextExt<'input>> for TableValuedFunctionContext<'input>{
	fn borrow_mut(&mut self) -> &mut RelationPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> RelationPrimaryContextAttrs<'input> for TableValuedFunctionContext<'input> {}

impl<'input> TableValuedFunctionContextExt<'input>{
	fn new(ctx: &dyn RelationPrimaryContextAttrs<'input>) -> Rc<RelationPrimaryContextAll<'input>>  {
		Rc::new(
			RelationPrimaryContextAll::TableValuedFunctionContext(
				BaseParserRuleContext::copy_from(ctx,TableValuedFunctionContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type InlineTableDefault2Context<'input> = BaseParserRuleContext<'input,InlineTableDefault2ContextExt<'input>>;

pub trait InlineTableDefault2ContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn inlineTable(&self) -> Option<Rc<InlineTableContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> InlineTableDefault2ContextAttrs<'input> for InlineTableDefault2Context<'input>{}

pub struct InlineTableDefault2ContextExt<'input>{
	base:RelationPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{InlineTableDefault2ContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for InlineTableDefault2Context<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InlineTableDefault2Context<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_inlineTableDefault2(self);
	}
}

impl<'input> CustomRuleContext<'input> for InlineTableDefault2ContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_relationPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_relationPrimary }
}

impl<'input> Borrow<RelationPrimaryContextExt<'input>> for InlineTableDefault2Context<'input>{
	fn borrow(&self) -> &RelationPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<RelationPrimaryContextExt<'input>> for InlineTableDefault2Context<'input>{
	fn borrow_mut(&mut self) -> &mut RelationPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> RelationPrimaryContextAttrs<'input> for InlineTableDefault2Context<'input> {}

impl<'input> InlineTableDefault2ContextExt<'input>{
	fn new(ctx: &dyn RelationPrimaryContextAttrs<'input>) -> Rc<RelationPrimaryContextAll<'input>>  {
		Rc::new(
			RelationPrimaryContextAll::InlineTableDefault2Context(
				BaseParserRuleContext::copy_from(ctx,InlineTableDefault2ContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type AliasedRelationContext<'input> = BaseParserRuleContext<'input,AliasedRelationContextExt<'input>>;

pub trait AliasedRelationContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn relation(&self) -> Option<Rc<RelationContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableAlias(&self) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn sample(&self) -> Option<Rc<SampleContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> AliasedRelationContextAttrs<'input> for AliasedRelationContext<'input>{}

pub struct AliasedRelationContextExt<'input>{
	base:RelationPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{AliasedRelationContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for AliasedRelationContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AliasedRelationContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_aliasedRelation(self);
	}
}

impl<'input> CustomRuleContext<'input> for AliasedRelationContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_relationPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_relationPrimary }
}

impl<'input> Borrow<RelationPrimaryContextExt<'input>> for AliasedRelationContext<'input>{
	fn borrow(&self) -> &RelationPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<RelationPrimaryContextExt<'input>> for AliasedRelationContext<'input>{
	fn borrow_mut(&mut self) -> &mut RelationPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> RelationPrimaryContextAttrs<'input> for AliasedRelationContext<'input> {}

impl<'input> AliasedRelationContextExt<'input>{
	fn new(ctx: &dyn RelationPrimaryContextAttrs<'input>) -> Rc<RelationPrimaryContextAll<'input>>  {
		Rc::new(
			RelationPrimaryContextAll::AliasedRelationContext(
				BaseParserRuleContext::copy_from(ctx,AliasedRelationContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type AliasedQueryContext<'input> = BaseParserRuleContext<'input,AliasedQueryContextExt<'input>>;

pub trait AliasedQueryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableAlias(&self) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn sample(&self) -> Option<Rc<SampleContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> AliasedQueryContextAttrs<'input> for AliasedQueryContext<'input>{}

pub struct AliasedQueryContextExt<'input>{
	base:RelationPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{AliasedQueryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for AliasedQueryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AliasedQueryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_aliasedQuery(self);
	}
}

impl<'input> CustomRuleContext<'input> for AliasedQueryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_relationPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_relationPrimary }
}

impl<'input> Borrow<RelationPrimaryContextExt<'input>> for AliasedQueryContext<'input>{
	fn borrow(&self) -> &RelationPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<RelationPrimaryContextExt<'input>> for AliasedQueryContext<'input>{
	fn borrow_mut(&mut self) -> &mut RelationPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> RelationPrimaryContextAttrs<'input> for AliasedQueryContext<'input> {}

impl<'input> AliasedQueryContextExt<'input>{
	fn new(ctx: &dyn RelationPrimaryContextAttrs<'input>) -> Rc<RelationPrimaryContextAll<'input>>  {
		Rc::new(
			RelationPrimaryContextAll::AliasedQueryContext(
				BaseParserRuleContext::copy_from(ctx,AliasedQueryContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type TableNameContext<'input> = BaseParserRuleContext<'input,TableNameContextExt<'input>>;

pub trait TableNameContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn tableAlias(&self) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn sample(&self) -> Option<Rc<SampleContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> TableNameContextAttrs<'input> for TableNameContext<'input>{}

pub struct TableNameContextExt<'input>{
	base:RelationPrimaryContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{TableNameContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TableNameContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TableNameContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_tableName(self);
	}
}

impl<'input> CustomRuleContext<'input> for TableNameContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_relationPrimary }
	//fn type_rule_index() -> usize where Self: Sized { RULE_relationPrimary }
}

impl<'input> Borrow<RelationPrimaryContextExt<'input>> for TableNameContext<'input>{
	fn borrow(&self) -> &RelationPrimaryContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<RelationPrimaryContextExt<'input>> for TableNameContext<'input>{
	fn borrow_mut(&mut self) -> &mut RelationPrimaryContextExt<'input> { &mut self.base }
}

impl<'input> RelationPrimaryContextAttrs<'input> for TableNameContext<'input> {}

impl<'input> TableNameContextExt<'input>{
	fn new(ctx: &dyn RelationPrimaryContextAttrs<'input>) -> Rc<RelationPrimaryContextAll<'input>>  {
		Rc::new(
			RelationPrimaryContextAll::TableNameContext(
				BaseParserRuleContext::copy_from(ctx,TableNameContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn relationPrimary(&mut self,)
	-> Result<Rc<RelationPrimaryContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = RelationPrimaryContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 162, RULE_relationPrimary);
        let mut _localctx: Rc<RelationPrimaryContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2178);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(272,&mut recog.base)? {
				1 =>{
					let tmp = TableNameContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					/*InvokeRule multipartIdentifier*/
					recog.base.set_state(2154);
					recog.multipartIdentifier()?;

					recog.base.set_state(2156);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(269,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule sample*/
							recog.base.set_state(2155);
							recog.sample()?;

							}
						}

						_ => {}
					}
					/*InvokeRule tableAlias*/
					recog.base.set_state(2158);
					recog.tableAlias()?;

					}
				}
			,
				2 =>{
					let tmp = AliasedQueryContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					recog.base.set_state(2160);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule query*/
					recog.base.set_state(2161);
					recog.query()?;

					recog.base.set_state(2162);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					recog.base.set_state(2164);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(270,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule sample*/
							recog.base.set_state(2163);
							recog.sample()?;

							}
						}

						_ => {}
					}
					/*InvokeRule tableAlias*/
					recog.base.set_state(2166);
					recog.tableAlias()?;

					}
				}
			,
				3 =>{
					let tmp = AliasedRelationContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(2168);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule relation*/
					recog.base.set_state(2169);
					recog.relation()?;

					recog.base.set_state(2170);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					recog.base.set_state(2172);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(271,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule sample*/
							recog.base.set_state(2171);
							recog.sample()?;

							}
						}

						_ => {}
					}
					/*InvokeRule tableAlias*/
					recog.base.set_state(2174);
					recog.tableAlias()?;

					}
				}
			,
				4 =>{
					let tmp = InlineTableDefault2ContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					/*InvokeRule inlineTable*/
					recog.base.set_state(2176);
					recog.inlineTable()?;

					}
				}
			,
				5 =>{
					let tmp = TableValuedFunctionContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 5);
					_localctx = tmp;
					{
					/*InvokeRule functionTable*/
					recog.base.set_state(2177);
					recog.functionTable()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- inlineTable ----------------
pub type InlineTableContextAll<'input> = InlineTableContext<'input>;


pub type InlineTableContext<'input> = BaseParserRuleContext<'input,InlineTableContextExt<'input>>;

#[derive(Clone)]
pub struct InlineTableContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for InlineTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for InlineTableContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_inlineTable(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_inlineTable(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for InlineTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_inlineTable }
	//fn type_rule_index() -> usize where Self: Sized { RULE_inlineTable }
}
crate::tid!{InlineTableContextExt<'a>}

impl<'input> InlineTableContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<InlineTableContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,InlineTableContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait InlineTableContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<InlineTableContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token VALUES
/// Returns `None` if there is no child corresponding to token VALUES
fn VALUES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VALUES, 0)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn tableAlias(&self) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> InlineTableContextAttrs<'input> for InlineTableContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn inlineTable(&mut self,)
	-> Result<Rc<InlineTableContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = InlineTableContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 164, RULE_inlineTable);
        let mut _localctx: Rc<InlineTableContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2180);
			recog.base.match_token(VALUES,&mut recog.err_handler)?;

			/*InvokeRule expression*/
			recog.base.set_state(2181);
			recog.expression()?;

			recog.base.set_state(2186);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(273,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(2182);
					recog.base.match_token(T__2,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2183);
					recog.expression()?;

					}
					} 
				}
				recog.base.set_state(2188);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(273,&mut recog.base)?;
			}
			/*InvokeRule tableAlias*/
			recog.base.set_state(2189);
			recog.tableAlias()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- functionTable ----------------
pub type FunctionTableContextAll<'input> = FunctionTableContext<'input>;


pub type FunctionTableContext<'input> = BaseParserRuleContext<'input,FunctionTableContextExt<'input>>;

#[derive(Clone)]
pub struct FunctionTableContextExt<'input>{
	pub funcName: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for FunctionTableContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FunctionTableContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_functionTable(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_functionTable(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for FunctionTableContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_functionTable }
	//fn type_rule_index() -> usize where Self: Sized { RULE_functionTable }
}
crate::tid!{FunctionTableContextExt<'a>}

impl<'input> FunctionTableContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<FunctionTableContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,FunctionTableContextExt{
				funcName: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait FunctionTableContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<FunctionTableContextExt<'input>>{

fn tableAlias(&self) -> Option<Rc<TableAliasContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> FunctionTableContextAttrs<'input> for FunctionTableContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn functionTable(&mut self,)
	-> Result<Rc<FunctionTableContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = FunctionTableContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 166, RULE_functionTable);
        let mut _localctx: Rc<FunctionTableContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(2191);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,FunctionTableContext >(&mut _localctx).funcName = Some(tmp.clone());
			  

			recog.base.set_state(2192);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			recog.base.set_state(2201);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(275,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule expression*/
					recog.base.set_state(2193);
					recog.expression()?;

					recog.base.set_state(2198);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(2194);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule expression*/
						recog.base.set_state(2195);
						recog.expression()?;

						}
						}
						recog.base.set_state(2200);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					}
				}

				_ => {}
			}
			recog.base.set_state(2203);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			/*InvokeRule tableAlias*/
			recog.base.set_state(2204);
			recog.tableAlias()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- tableAlias ----------------
pub type TableAliasContextAll<'input> = TableAliasContext<'input>;


pub type TableAliasContext<'input> = BaseParserRuleContext<'input,TableAliasContextExt<'input>>;

#[derive(Clone)]
pub struct TableAliasContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TableAliasContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TableAliasContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_tableAlias(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_tableAlias(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TableAliasContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_tableAlias }
	//fn type_rule_index() -> usize where Self: Sized { RULE_tableAlias }
}
crate::tid!{TableAliasContextExt<'a>}

impl<'input> TableAliasContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TableAliasContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TableAliasContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait TableAliasContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TableAliasContextExt<'input>>{

fn strictIdentifier(&self) -> Option<Rc<StrictIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
fn identifierList(&self) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> TableAliasContextAttrs<'input> for TableAliasContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn tableAlias(&mut self,)
	-> Result<Rc<TableAliasContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TableAliasContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 168, RULE_tableAlias);
        let mut _localctx: Rc<TableAliasContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2213);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(278,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(2207);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(276,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2206);
							recog.base.match_token(AS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					/*InvokeRule strictIdentifier*/
					recog.base.set_state(2209);
					recog.strictIdentifier()?;

					recog.base.set_state(2211);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(277,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule identifierList*/
							recog.base.set_state(2210);
							recog.identifierList()?;

							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- rowFormat ----------------
#[derive(Debug)]
pub enum RowFormatContextAll<'input>{
	RowFormatSerdeContext(RowFormatSerdeContext<'input>),
	RowFormatDelimitedContext(RowFormatDelimitedContext<'input>),
Error(RowFormatContext<'input>)
}
crate::tid!{RowFormatContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for RowFormatContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for RowFormatContextAll<'input>{}

impl<'input> Deref for RowFormatContextAll<'input>{
	type Target = dyn RowFormatContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use RowFormatContextAll::*;
		match self{
			RowFormatSerdeContext(inner) => inner,
			RowFormatDelimitedContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RowFormatContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type RowFormatContext<'input> = BaseParserRuleContext<'input,RowFormatContextExt<'input>>;

#[derive(Clone)]
pub struct RowFormatContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for RowFormatContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RowFormatContext<'input>{
}

impl<'input> CustomRuleContext<'input> for RowFormatContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_rowFormat }
	//fn type_rule_index() -> usize where Self: Sized { RULE_rowFormat }
}
crate::tid!{RowFormatContextExt<'a>}

impl<'input> RowFormatContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<RowFormatContextAll<'input>> {
		Rc::new(
		RowFormatContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,RowFormatContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait RowFormatContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<RowFormatContextExt<'input>>{


}

impl<'input> RowFormatContextAttrs<'input> for RowFormatContext<'input>{}

pub type RowFormatSerdeContext<'input> = BaseParserRuleContext<'input,RowFormatSerdeContextExt<'input>>;

pub trait RowFormatSerdeContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ROW
	/// Returns `None` if there is no child corresponding to token ROW
	fn ROW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ROW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FORMAT
	/// Returns `None` if there is no child corresponding to token FORMAT
	fn FORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FORMAT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SERDE
	/// Returns `None` if there is no child corresponding to token SERDE
	fn SERDE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SERDE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token WITH
	/// Returns `None` if there is no child corresponding to token WITH
	fn WITH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(WITH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SERDEPROPERTIES
	/// Returns `None` if there is no child corresponding to token SERDEPROPERTIES
	fn SERDEPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SERDEPROPERTIES, 0)
	}
	fn tablePropertyList(&self) -> Option<Rc<TablePropertyListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> RowFormatSerdeContextAttrs<'input> for RowFormatSerdeContext<'input>{}

pub struct RowFormatSerdeContextExt<'input>{
	base:RowFormatContextExt<'input>,
	pub name: Option<TokenType<'input>>,
	pub props: Option<Rc<TablePropertyListContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{RowFormatSerdeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RowFormatSerdeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RowFormatSerdeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_rowFormatSerde(self);
	}
}

impl<'input> CustomRuleContext<'input> for RowFormatSerdeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_rowFormat }
	//fn type_rule_index() -> usize where Self: Sized { RULE_rowFormat }
}

impl<'input> Borrow<RowFormatContextExt<'input>> for RowFormatSerdeContext<'input>{
	fn borrow(&self) -> &RowFormatContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<RowFormatContextExt<'input>> for RowFormatSerdeContext<'input>{
	fn borrow_mut(&mut self) -> &mut RowFormatContextExt<'input> { &mut self.base }
}

impl<'input> RowFormatContextAttrs<'input> for RowFormatSerdeContext<'input> {}

impl<'input> RowFormatSerdeContextExt<'input>{
	fn new(ctx: &dyn RowFormatContextAttrs<'input>) -> Rc<RowFormatContextAll<'input>>  {
		Rc::new(
			RowFormatContextAll::RowFormatSerdeContext(
				BaseParserRuleContext::copy_from(ctx,RowFormatSerdeContextExt{
					name:None, 
        			props:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RowFormatDelimitedContext<'input> = BaseParserRuleContext<'input,RowFormatDelimitedContextExt<'input>>;

pub trait RowFormatDelimitedContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ROW
	/// Returns `None` if there is no child corresponding to token ROW
	fn ROW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ROW, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FORMAT
	/// Returns `None` if there is no child corresponding to token FORMAT
	fn FORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FORMAT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DELIMITED
	/// Returns `None` if there is no child corresponding to token DELIMITED
	fn DELIMITED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DELIMITED, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FIELDS
	/// Returns `None` if there is no child corresponding to token FIELDS
	fn FIELDS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FIELDS, 0)
	}
	/// Retrieves all `TerminalNode`s corresponding to token TERMINATED in current rule
	fn TERMINATED_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token TERMINATED, starting from 0.
	/// Returns `None` if number of children corresponding to token TERMINATED is less or equal than `i`.
	fn TERMINATED(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TERMINATED, i)
	}
	/// Retrieves all `TerminalNode`s corresponding to token BY in current rule
	fn BY_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token BY, starting from 0.
	/// Returns `None` if number of children corresponding to token BY is less or equal than `i`.
	fn BY(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(BY, i)
	}
	/// Retrieves first TerminalNode corresponding to token COLLECTION
	/// Returns `None` if there is no child corresponding to token COLLECTION
	fn COLLECTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(COLLECTION, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ITEMS
	/// Returns `None` if there is no child corresponding to token ITEMS
	fn ITEMS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ITEMS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MAP
	/// Returns `None` if there is no child corresponding to token MAP
	fn MAP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MAP, 0)
	}
	/// Retrieves first TerminalNode corresponding to token KEYS
	/// Returns `None` if there is no child corresponding to token KEYS
	fn KEYS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(KEYS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LINES
	/// Returns `None` if there is no child corresponding to token LINES
	fn LINES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LINES, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NULL
	/// Returns `None` if there is no child corresponding to token NULL
	fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NULL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DEFINED
	/// Returns `None` if there is no child corresponding to token DEFINED
	fn DEFINED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DEFINED, 0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
	/// Retrieves all `TerminalNode`s corresponding to token STRING in current rule
	fn STRING_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token STRING, starting from 0.
	/// Returns `None` if number of children corresponding to token STRING is less or equal than `i`.
	fn STRING(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, i)
	}
	/// Retrieves first TerminalNode corresponding to token ESCAPED
	/// Returns `None` if there is no child corresponding to token ESCAPED
	fn ESCAPED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ESCAPED, 0)
	}
}

impl<'input> RowFormatDelimitedContextAttrs<'input> for RowFormatDelimitedContext<'input>{}

pub struct RowFormatDelimitedContextExt<'input>{
	base:RowFormatContextExt<'input>,
	pub fieldsTerminatedBy: Option<TokenType<'input>>,
	pub escapedBy: Option<TokenType<'input>>,
	pub collectionItemsTerminatedBy: Option<TokenType<'input>>,
	pub keysTerminatedBy: Option<TokenType<'input>>,
	pub linesSeparatedBy: Option<TokenType<'input>>,
	pub nullDefinedAs: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{RowFormatDelimitedContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RowFormatDelimitedContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RowFormatDelimitedContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_rowFormatDelimited(self);
	}
}

impl<'input> CustomRuleContext<'input> for RowFormatDelimitedContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_rowFormat }
	//fn type_rule_index() -> usize where Self: Sized { RULE_rowFormat }
}

impl<'input> Borrow<RowFormatContextExt<'input>> for RowFormatDelimitedContext<'input>{
	fn borrow(&self) -> &RowFormatContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<RowFormatContextExt<'input>> for RowFormatDelimitedContext<'input>{
	fn borrow_mut(&mut self) -> &mut RowFormatContextExt<'input> { &mut self.base }
}

impl<'input> RowFormatContextAttrs<'input> for RowFormatDelimitedContext<'input> {}

impl<'input> RowFormatDelimitedContextExt<'input>{
	fn new(ctx: &dyn RowFormatContextAttrs<'input>) -> Rc<RowFormatContextAll<'input>>  {
		Rc::new(
			RowFormatContextAll::RowFormatDelimitedContext(
				BaseParserRuleContext::copy_from(ctx,RowFormatDelimitedContextExt{
					fieldsTerminatedBy:None, escapedBy:None, collectionItemsTerminatedBy:None, keysTerminatedBy:None, linesSeparatedBy:None, nullDefinedAs:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn rowFormat(&mut self,)
	-> Result<Rc<RowFormatContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = RowFormatContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 170, RULE_rowFormat);
        let mut _localctx: Rc<RowFormatContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2264);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(286,&mut recog.base)? {
				1 =>{
					let tmp = RowFormatSerdeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(2215);
					recog.base.match_token(ROW,&mut recog.err_handler)?;

					recog.base.set_state(2216);
					recog.base.match_token(FORMAT,&mut recog.err_handler)?;

					recog.base.set_state(2217);
					recog.base.match_token(SERDE,&mut recog.err_handler)?;

					recog.base.set_state(2218);
					let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
					if let RowFormatContextAll::RowFormatSerdeContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
					ctx.name = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2222);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(279,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2219);
							recog.base.match_token(WITH,&mut recog.err_handler)?;

							recog.base.set_state(2220);
							recog.base.match_token(SERDEPROPERTIES,&mut recog.err_handler)?;

							/*InvokeRule tablePropertyList*/
							recog.base.set_state(2221);
							let tmp = recog.tablePropertyList()?;
							if let RowFormatContextAll::RowFormatSerdeContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
							ctx.props = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					}
				}
			,
				2 =>{
					let tmp = RowFormatDelimitedContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					recog.base.set_state(2224);
					recog.base.match_token(ROW,&mut recog.err_handler)?;

					recog.base.set_state(2225);
					recog.base.match_token(FORMAT,&mut recog.err_handler)?;

					recog.base.set_state(2226);
					recog.base.match_token(DELIMITED,&mut recog.err_handler)?;

					recog.base.set_state(2236);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(281,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2227);
							recog.base.match_token(FIELDS,&mut recog.err_handler)?;

							recog.base.set_state(2228);
							recog.base.match_token(TERMINATED,&mut recog.err_handler)?;

							recog.base.set_state(2229);
							recog.base.match_token(BY,&mut recog.err_handler)?;

							recog.base.set_state(2230);
							let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
							if let RowFormatContextAll::RowFormatDelimitedContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
							ctx.fieldsTerminatedBy = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							recog.base.set_state(2234);
							recog.err_handler.sync(&mut recog.base)?;
							match  recog.interpreter.adaptive_predict(280,&mut recog.base)? {
								x if x == 1=>{
									{
									recog.base.set_state(2231);
									recog.base.match_token(ESCAPED,&mut recog.err_handler)?;

									recog.base.set_state(2232);
									recog.base.match_token(BY,&mut recog.err_handler)?;

									recog.base.set_state(2233);
									let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
									if let RowFormatContextAll::RowFormatDelimitedContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
									ctx.escapedBy = Some(tmp.clone()); } else {unreachable!("cant cast");}  

									}
								}

								_ => {}
							}
							}
						}

						_ => {}
					}
					recog.base.set_state(2243);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(282,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2238);
							recog.base.match_token(COLLECTION,&mut recog.err_handler)?;

							recog.base.set_state(2239);
							recog.base.match_token(ITEMS,&mut recog.err_handler)?;

							recog.base.set_state(2240);
							recog.base.match_token(TERMINATED,&mut recog.err_handler)?;

							recog.base.set_state(2241);
							recog.base.match_token(BY,&mut recog.err_handler)?;

							recog.base.set_state(2242);
							let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
							if let RowFormatContextAll::RowFormatDelimitedContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
							ctx.collectionItemsTerminatedBy = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					recog.base.set_state(2250);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(283,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2245);
							recog.base.match_token(MAP,&mut recog.err_handler)?;

							recog.base.set_state(2246);
							recog.base.match_token(KEYS,&mut recog.err_handler)?;

							recog.base.set_state(2247);
							recog.base.match_token(TERMINATED,&mut recog.err_handler)?;

							recog.base.set_state(2248);
							recog.base.match_token(BY,&mut recog.err_handler)?;

							recog.base.set_state(2249);
							let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
							if let RowFormatContextAll::RowFormatDelimitedContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
							ctx.keysTerminatedBy = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					recog.base.set_state(2256);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(284,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2252);
							recog.base.match_token(LINES,&mut recog.err_handler)?;

							recog.base.set_state(2253);
							recog.base.match_token(TERMINATED,&mut recog.err_handler)?;

							recog.base.set_state(2254);
							recog.base.match_token(BY,&mut recog.err_handler)?;

							recog.base.set_state(2255);
							let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
							if let RowFormatContextAll::RowFormatDelimitedContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
							ctx.linesSeparatedBy = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					recog.base.set_state(2262);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(285,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2258);
							recog.base.match_token(NULL,&mut recog.err_handler)?;

							recog.base.set_state(2259);
							recog.base.match_token(DEFINED,&mut recog.err_handler)?;

							recog.base.set_state(2260);
							recog.base.match_token(AS,&mut recog.err_handler)?;

							recog.base.set_state(2261);
							let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
							if let RowFormatContextAll::RowFormatDelimitedContext(ctx) = cast_mut::<_,RowFormatContextAll >(&mut _localctx){
							ctx.nullDefinedAs = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- multipartIdentifierList ----------------
pub type MultipartIdentifierListContextAll<'input> = MultipartIdentifierListContext<'input>;


pub type MultipartIdentifierListContext<'input> = BaseParserRuleContext<'input,MultipartIdentifierListContextExt<'input>>;

#[derive(Clone)]
pub struct MultipartIdentifierListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for MultipartIdentifierListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MultipartIdentifierListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_multipartIdentifierList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_multipartIdentifierList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for MultipartIdentifierListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_multipartIdentifierList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_multipartIdentifierList }
}
crate::tid!{MultipartIdentifierListContextExt<'a>}

impl<'input> MultipartIdentifierListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<MultipartIdentifierListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,MultipartIdentifierListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait MultipartIdentifierListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<MultipartIdentifierListContextExt<'input>>{

fn multipartIdentifier_all(&self) ->  Vec<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn multipartIdentifier(&self, i: usize) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> MultipartIdentifierListContextAttrs<'input> for MultipartIdentifierListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn multipartIdentifierList(&mut self,)
	-> Result<Rc<MultipartIdentifierListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = MultipartIdentifierListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 172, RULE_multipartIdentifierList);
        let mut _localctx: Rc<MultipartIdentifierListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule multipartIdentifier*/
			recog.base.set_state(2266);
			recog.multipartIdentifier()?;

			recog.base.set_state(2271);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(2267);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule multipartIdentifier*/
				recog.base.set_state(2268);
				recog.multipartIdentifier()?;

				}
				}
				recog.base.set_state(2273);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- multipartIdentifier ----------------
pub type MultipartIdentifierContextAll<'input> = MultipartIdentifierContext<'input>;


pub type MultipartIdentifierContext<'input> = BaseParserRuleContext<'input,MultipartIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct MultipartIdentifierContextExt<'input>{
	pub errorCapturingIdentifier: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
	pub parts:Vec<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for MultipartIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MultipartIdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_multipartIdentifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_multipartIdentifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for MultipartIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_multipartIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_multipartIdentifier }
}
crate::tid!{MultipartIdentifierContextExt<'a>}

impl<'input> MultipartIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<MultipartIdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,MultipartIdentifierContextExt{
				errorCapturingIdentifier: None, 
				parts: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait MultipartIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<MultipartIdentifierContextExt<'input>>{

fn errorCapturingIdentifier_all(&self) ->  Vec<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn errorCapturingIdentifier(&self, i: usize) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> MultipartIdentifierContextAttrs<'input> for MultipartIdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn multipartIdentifier(&mut self,)
	-> Result<Rc<MultipartIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = MultipartIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 174, RULE_multipartIdentifier);
        let mut _localctx: Rc<MultipartIdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(2274);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,MultipartIdentifierContext >(&mut _localctx).errorCapturingIdentifier = Some(tmp.clone());
			  

			let temp =  cast_mut::<_,MultipartIdentifierContext >(&mut _localctx).errorCapturingIdentifier.clone().unwrap()
			 ;
			 cast_mut::<_,MultipartIdentifierContext >(&mut _localctx).parts.push(temp);
			  
			recog.base.set_state(2279);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(288,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(2275);
					recog.base.match_token(T__3,&mut recog.err_handler)?;

					/*InvokeRule errorCapturingIdentifier*/
					recog.base.set_state(2276);
					let tmp = recog.errorCapturingIdentifier()?;
					 cast_mut::<_,MultipartIdentifierContext >(&mut _localctx).errorCapturingIdentifier = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,MultipartIdentifierContext >(&mut _localctx).errorCapturingIdentifier.clone().unwrap()
					 ;
					 cast_mut::<_,MultipartIdentifierContext >(&mut _localctx).parts.push(temp);
					  
					}
					} 
				}
				recog.base.set_state(2281);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(288,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- tableIdentifier ----------------
pub type TableIdentifierContextAll<'input> = TableIdentifierContext<'input>;


pub type TableIdentifierContext<'input> = BaseParserRuleContext<'input,TableIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct TableIdentifierContextExt<'input>{
	pub db: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
	pub table: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TableIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TableIdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_tableIdentifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_tableIdentifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TableIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_tableIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_tableIdentifier }
}
crate::tid!{TableIdentifierContextExt<'a>}

impl<'input> TableIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TableIdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TableIdentifierContextExt{
				db: None, table: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait TableIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TableIdentifierContextExt<'input>>{

fn errorCapturingIdentifier_all(&self) ->  Vec<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn errorCapturingIdentifier(&self, i: usize) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> TableIdentifierContextAttrs<'input> for TableIdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn tableIdentifier(&mut self,)
	-> Result<Rc<TableIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TableIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 176, RULE_tableIdentifier);
        let mut _localctx: Rc<TableIdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2285);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(289,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule errorCapturingIdentifier*/
					recog.base.set_state(2282);
					let tmp = recog.errorCapturingIdentifier()?;
					 cast_mut::<_,TableIdentifierContext >(&mut _localctx).db = Some(tmp.clone());
					  

					recog.base.set_state(2283);
					recog.base.match_token(T__3,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(2287);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,TableIdentifierContext >(&mut _localctx).table = Some(tmp.clone());
			  

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- namedExpression ----------------
pub type NamedExpressionContextAll<'input> = NamedExpressionContext<'input>;


pub type NamedExpressionContext<'input> = BaseParserRuleContext<'input,NamedExpressionContextExt<'input>>;

#[derive(Clone)]
pub struct NamedExpressionContextExt<'input>{
	pub name: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NamedExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NamedExpressionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_namedExpression(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_namedExpression(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NamedExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_namedExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_namedExpression }
}
crate::tid!{NamedExpressionContextExt<'a>}

impl<'input> NamedExpressionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NamedExpressionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NamedExpressionContextExt{
				name: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait NamedExpressionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NamedExpressionContextExt<'input>>{

fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn identifierList(&self) -> Option<Rc<IdentifierListContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> NamedExpressionContextAttrs<'input> for NamedExpressionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn namedExpression(&mut self,)
	-> Result<Rc<NamedExpressionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NamedExpressionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 178, RULE_namedExpression);
        let mut _localctx: Rc<NamedExpressionContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule expression*/
			recog.base.set_state(2289);
			recog.expression()?;

			recog.base.set_state(2297);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(292,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(2291);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(290,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2290);
							recog.base.match_token(AS,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					recog.base.set_state(2295);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(291,&mut recog.base)? {
						1 =>{
							{
							/*InvokeRule errorCapturingIdentifier*/
							recog.base.set_state(2293);
							let tmp = recog.errorCapturingIdentifier()?;
							 cast_mut::<_,NamedExpressionContext >(&mut _localctx).name = Some(tmp.clone());
							  

							}
						}
					,
						2 =>{
							{
							/*InvokeRule identifierList*/
							recog.base.set_state(2294);
							recog.identifierList()?;

							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- namedExpressionSeq ----------------
pub type NamedExpressionSeqContextAll<'input> = NamedExpressionSeqContext<'input>;


pub type NamedExpressionSeqContext<'input> = BaseParserRuleContext<'input,NamedExpressionSeqContextExt<'input>>;

#[derive(Clone)]
pub struct NamedExpressionSeqContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NamedExpressionSeqContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NamedExpressionSeqContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_namedExpressionSeq(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_namedExpressionSeq(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NamedExpressionSeqContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_namedExpressionSeq }
	//fn type_rule_index() -> usize where Self: Sized { RULE_namedExpressionSeq }
}
crate::tid!{NamedExpressionSeqContextExt<'a>}

impl<'input> NamedExpressionSeqContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NamedExpressionSeqContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NamedExpressionSeqContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait NamedExpressionSeqContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NamedExpressionSeqContextExt<'input>>{

fn namedExpression_all(&self) ->  Vec<Rc<NamedExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn namedExpression(&self, i: usize) -> Option<Rc<NamedExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> NamedExpressionSeqContextAttrs<'input> for NamedExpressionSeqContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn namedExpressionSeq(&mut self,)
	-> Result<Rc<NamedExpressionSeqContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NamedExpressionSeqContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 180, RULE_namedExpressionSeq);
        let mut _localctx: Rc<NamedExpressionSeqContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule namedExpression*/
			recog.base.set_state(2299);
			recog.namedExpression()?;

			recog.base.set_state(2304);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(293,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(2300);
					recog.base.match_token(T__2,&mut recog.err_handler)?;

					/*InvokeRule namedExpression*/
					recog.base.set_state(2301);
					recog.namedExpression()?;

					}
					} 
				}
				recog.base.set_state(2306);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(293,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- transformList ----------------
pub type TransformListContextAll<'input> = TransformListContext<'input>;


pub type TransformListContext<'input> = BaseParserRuleContext<'input,TransformListContextExt<'input>>;

#[derive(Clone)]
pub struct TransformListContextExt<'input>{
	pub transform: Option<Rc<TransformContextAll<'input>>>,
	pub transforms:Vec<Rc<TransformContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TransformListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TransformListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_transformList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_transformList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TransformListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_transformList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_transformList }
}
crate::tid!{TransformListContextExt<'a>}

impl<'input> TransformListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TransformListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TransformListContextExt{
				transform: None, 
				transforms: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait TransformListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TransformListContextExt<'input>>{

fn transform_all(&self) ->  Vec<Rc<TransformContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn transform(&self, i: usize) -> Option<Rc<TransformContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> TransformListContextAttrs<'input> for TransformListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn transformList(&mut self,)
	-> Result<Rc<TransformListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TransformListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 182, RULE_transformList);
        let mut _localctx: Rc<TransformListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2307);
			recog.base.match_token(T__0,&mut recog.err_handler)?;

			/*InvokeRule transform*/
			recog.base.set_state(2308);
			let tmp = recog.transform()?;
			 cast_mut::<_,TransformListContext >(&mut _localctx).transform = Some(tmp.clone());
			  

			let temp =  cast_mut::<_,TransformListContext >(&mut _localctx).transform.clone().unwrap()
			 ;
			 cast_mut::<_,TransformListContext >(&mut _localctx).transforms.push(temp);
			  
			recog.base.set_state(2313);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(2309);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule transform*/
				recog.base.set_state(2310);
				let tmp = recog.transform()?;
				 cast_mut::<_,TransformListContext >(&mut _localctx).transform = Some(tmp.clone());
				  

				let temp =  cast_mut::<_,TransformListContext >(&mut _localctx).transform.clone().unwrap()
				 ;
				 cast_mut::<_,TransformListContext >(&mut _localctx).transforms.push(temp);
				  
				}
				}
				recog.base.set_state(2315);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			recog.base.set_state(2316);
			recog.base.match_token(T__1,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- transform ----------------
#[derive(Debug)]
pub enum TransformContextAll<'input>{
	IdentityTransformContext(IdentityTransformContext<'input>),
	ApplyTransformContext(ApplyTransformContext<'input>),
Error(TransformContext<'input>)
}
crate::tid!{TransformContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for TransformContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for TransformContextAll<'input>{}

impl<'input> Deref for TransformContextAll<'input>{
	type Target = dyn TransformContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use TransformContextAll::*;
		match self{
			IdentityTransformContext(inner) => inner,
			ApplyTransformContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TransformContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type TransformContext<'input> = BaseParserRuleContext<'input,TransformContextExt<'input>>;

#[derive(Clone)]
pub struct TransformContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TransformContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TransformContext<'input>{
}

impl<'input> CustomRuleContext<'input> for TransformContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_transform }
	//fn type_rule_index() -> usize where Self: Sized { RULE_transform }
}
crate::tid!{TransformContextExt<'a>}

impl<'input> TransformContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TransformContextAll<'input>> {
		Rc::new(
		TransformContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TransformContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait TransformContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TransformContextExt<'input>>{


}

impl<'input> TransformContextAttrs<'input> for TransformContext<'input>{}

pub type IdentityTransformContext<'input> = BaseParserRuleContext<'input,IdentityTransformContextExt<'input>>;

pub trait IdentityTransformContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn qualifiedName(&self) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> IdentityTransformContextAttrs<'input> for IdentityTransformContext<'input>{}

pub struct IdentityTransformContextExt<'input>{
	base:TransformContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{IdentityTransformContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for IdentityTransformContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IdentityTransformContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_identityTransform(self);
	}
}

impl<'input> CustomRuleContext<'input> for IdentityTransformContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_transform }
	//fn type_rule_index() -> usize where Self: Sized { RULE_transform }
}

impl<'input> Borrow<TransformContextExt<'input>> for IdentityTransformContext<'input>{
	fn borrow(&self) -> &TransformContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<TransformContextExt<'input>> for IdentityTransformContext<'input>{
	fn borrow_mut(&mut self) -> &mut TransformContextExt<'input> { &mut self.base }
}

impl<'input> TransformContextAttrs<'input> for IdentityTransformContext<'input> {}

impl<'input> IdentityTransformContextExt<'input>{
	fn new(ctx: &dyn TransformContextAttrs<'input>) -> Rc<TransformContextAll<'input>>  {
		Rc::new(
			TransformContextAll::IdentityTransformContext(
				BaseParserRuleContext::copy_from(ctx,IdentityTransformContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ApplyTransformContext<'input> = BaseParserRuleContext<'input,ApplyTransformContextExt<'input>>;

pub trait ApplyTransformContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn transformArgument_all(&self) ->  Vec<Rc<TransformArgumentContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn transformArgument(&self, i: usize) -> Option<Rc<TransformArgumentContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> ApplyTransformContextAttrs<'input> for ApplyTransformContext<'input>{}

pub struct ApplyTransformContextExt<'input>{
	base:TransformContextExt<'input>,
	pub transformName: Option<Rc<IdentifierContextAll<'input>>>,
	pub transformArgument: Option<Rc<TransformArgumentContextAll<'input>>>,
	pub argument:Vec<Rc<TransformArgumentContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ApplyTransformContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ApplyTransformContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ApplyTransformContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_applyTransform(self);
	}
}

impl<'input> CustomRuleContext<'input> for ApplyTransformContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_transform }
	//fn type_rule_index() -> usize where Self: Sized { RULE_transform }
}

impl<'input> Borrow<TransformContextExt<'input>> for ApplyTransformContext<'input>{
	fn borrow(&self) -> &TransformContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<TransformContextExt<'input>> for ApplyTransformContext<'input>{
	fn borrow_mut(&mut self) -> &mut TransformContextExt<'input> { &mut self.base }
}

impl<'input> TransformContextAttrs<'input> for ApplyTransformContext<'input> {}

impl<'input> ApplyTransformContextExt<'input>{
	fn new(ctx: &dyn TransformContextAttrs<'input>) -> Rc<TransformContextAll<'input>>  {
		Rc::new(
			TransformContextAll::ApplyTransformContext(
				BaseParserRuleContext::copy_from(ctx,ApplyTransformContextExt{
        			transformName:None, transformArgument:None, 
        			argument:Vec::new(), 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn transform(&mut self,)
	-> Result<Rc<TransformContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TransformContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 184, RULE_transform);
        let mut _localctx: Rc<TransformContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2331);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(296,&mut recog.base)? {
				1 =>{
					let tmp = IdentityTransformContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					/*InvokeRule qualifiedName*/
					recog.base.set_state(2318);
					recog.qualifiedName()?;

					}
				}
			,
				2 =>{
					let tmp = ApplyTransformContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule identifier*/
					recog.base.set_state(2319);
					let tmp = recog.identifier()?;
					if let TransformContextAll::ApplyTransformContext(ctx) = cast_mut::<_,TransformContextAll >(&mut _localctx){
					ctx.transformName = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2320);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule transformArgument*/
					recog.base.set_state(2321);
					let tmp = recog.transformArgument()?;
					if let TransformContextAll::ApplyTransformContext(ctx) = cast_mut::<_,TransformContextAll >(&mut _localctx){
					ctx.transformArgument = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					let temp = if let TransformContextAll::ApplyTransformContext(ctx) = cast_mut::<_,TransformContextAll >(&mut _localctx){
					ctx.transformArgument.clone().unwrap() } else {unreachable!("cant cast");} ;
					if let TransformContextAll::ApplyTransformContext(ctx) = cast_mut::<_,TransformContextAll >(&mut _localctx){
					ctx.argument.push(temp); } else {unreachable!("cant cast");}  
					recog.base.set_state(2326);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(2322);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule transformArgument*/
						recog.base.set_state(2323);
						let tmp = recog.transformArgument()?;
						if let TransformContextAll::ApplyTransformContext(ctx) = cast_mut::<_,TransformContextAll >(&mut _localctx){
						ctx.transformArgument = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						let temp = if let TransformContextAll::ApplyTransformContext(ctx) = cast_mut::<_,TransformContextAll >(&mut _localctx){
						ctx.transformArgument.clone().unwrap() } else {unreachable!("cant cast");} ;
						if let TransformContextAll::ApplyTransformContext(ctx) = cast_mut::<_,TransformContextAll >(&mut _localctx){
						ctx.argument.push(temp); } else {unreachable!("cant cast");}  
						}
						}
						recog.base.set_state(2328);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(2329);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- transformArgument ----------------
pub type TransformArgumentContextAll<'input> = TransformArgumentContext<'input>;


pub type TransformArgumentContext<'input> = BaseParserRuleContext<'input,TransformArgumentContextExt<'input>>;

#[derive(Clone)]
pub struct TransformArgumentContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for TransformArgumentContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TransformArgumentContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_transformArgument(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_transformArgument(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for TransformArgumentContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_transformArgument }
	//fn type_rule_index() -> usize where Self: Sized { RULE_transformArgument }
}
crate::tid!{TransformArgumentContextExt<'a>}

impl<'input> TransformArgumentContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<TransformArgumentContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,TransformArgumentContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait TransformArgumentContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<TransformArgumentContextExt<'input>>{

fn qualifiedName(&self) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn constant(&self) -> Option<Rc<ConstantContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> TransformArgumentContextAttrs<'input> for TransformArgumentContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn transformArgument(&mut self,)
	-> Result<Rc<TransformArgumentContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = TransformArgumentContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 186, RULE_transformArgument);
        let mut _localctx: Rc<TransformArgumentContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2335);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(297,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule qualifiedName*/
					recog.base.set_state(2333);
					recog.qualifiedName()?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					/*InvokeRule constant*/
					recog.base.set_state(2334);
					recog.constant()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- expression ----------------
pub type ExpressionContextAll<'input> = ExpressionContext<'input>;


pub type ExpressionContext<'input> = BaseParserRuleContext<'input,ExpressionContextExt<'input>>;

#[derive(Clone)]
pub struct ExpressionContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ExpressionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_expression(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_expression(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_expression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_expression }
}
crate::tid!{ExpressionContextExt<'a>}

impl<'input> ExpressionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ExpressionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ExpressionContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ExpressionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ExpressionContextExt<'input>>{

fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ExpressionContextAttrs<'input> for ExpressionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn expression(&mut self,)
	-> Result<Rc<ExpressionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ExpressionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 188, RULE_expression);
        let mut _localctx: Rc<ExpressionContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule booleanExpression*/
			recog.base.set_state(2337);
			recog.booleanExpression_rec(0)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- booleanExpression ----------------
#[derive(Debug)]
pub enum BooleanExpressionContextAll<'input>{
	LogicalNotContext(LogicalNotContext<'input>),
	PredicatedContext(PredicatedContext<'input>),
	ExistsContext(ExistsContext<'input>),
	LogicalBinaryContext(LogicalBinaryContext<'input>),
Error(BooleanExpressionContext<'input>)
}
crate::tid!{BooleanExpressionContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for BooleanExpressionContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for BooleanExpressionContextAll<'input>{}

impl<'input> Deref for BooleanExpressionContextAll<'input>{
	type Target = dyn BooleanExpressionContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use BooleanExpressionContextAll::*;
		match self{
			LogicalNotContext(inner) => inner,
			PredicatedContext(inner) => inner,
			ExistsContext(inner) => inner,
			LogicalBinaryContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for BooleanExpressionContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type BooleanExpressionContext<'input> = BaseParserRuleContext<'input,BooleanExpressionContextExt<'input>>;

#[derive(Clone)]
pub struct BooleanExpressionContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for BooleanExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for BooleanExpressionContext<'input>{
}

impl<'input> CustomRuleContext<'input> for BooleanExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_booleanExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_booleanExpression }
}
crate::tid!{BooleanExpressionContextExt<'a>}

impl<'input> BooleanExpressionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<BooleanExpressionContextAll<'input>> {
		Rc::new(
		BooleanExpressionContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,BooleanExpressionContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait BooleanExpressionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<BooleanExpressionContextExt<'input>>{


}

impl<'input> BooleanExpressionContextAttrs<'input> for BooleanExpressionContext<'input>{}

pub type LogicalNotContext<'input> = BaseParserRuleContext<'input,LogicalNotContextExt<'input>>;

pub trait LogicalNotContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token NOT
	/// Returns `None` if there is no child corresponding to token NOT
	fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NOT, 0)
	}
	fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> LogicalNotContextAttrs<'input> for LogicalNotContext<'input>{}

pub struct LogicalNotContextExt<'input>{
	base:BooleanExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{LogicalNotContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for LogicalNotContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LogicalNotContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_logicalNot(self);
	}
}

impl<'input> CustomRuleContext<'input> for LogicalNotContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_booleanExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_booleanExpression }
}

impl<'input> Borrow<BooleanExpressionContextExt<'input>> for LogicalNotContext<'input>{
	fn borrow(&self) -> &BooleanExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<BooleanExpressionContextExt<'input>> for LogicalNotContext<'input>{
	fn borrow_mut(&mut self) -> &mut BooleanExpressionContextExt<'input> { &mut self.base }
}

impl<'input> BooleanExpressionContextAttrs<'input> for LogicalNotContext<'input> {}

impl<'input> LogicalNotContextExt<'input>{
	fn new(ctx: &dyn BooleanExpressionContextAttrs<'input>) -> Rc<BooleanExpressionContextAll<'input>>  {
		Rc::new(
			BooleanExpressionContextAll::LogicalNotContext(
				BaseParserRuleContext::copy_from(ctx,LogicalNotContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type PredicatedContext<'input> = BaseParserRuleContext<'input,PredicatedContextExt<'input>>;

pub trait PredicatedContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn valueExpression(&self) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn predicate(&self) -> Option<Rc<PredicateContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> PredicatedContextAttrs<'input> for PredicatedContext<'input>{}

pub struct PredicatedContextExt<'input>{
	base:BooleanExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{PredicatedContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for PredicatedContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PredicatedContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_predicated(self);
	}
}

impl<'input> CustomRuleContext<'input> for PredicatedContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_booleanExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_booleanExpression }
}

impl<'input> Borrow<BooleanExpressionContextExt<'input>> for PredicatedContext<'input>{
	fn borrow(&self) -> &BooleanExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<BooleanExpressionContextExt<'input>> for PredicatedContext<'input>{
	fn borrow_mut(&mut self) -> &mut BooleanExpressionContextExt<'input> { &mut self.base }
}

impl<'input> BooleanExpressionContextAttrs<'input> for PredicatedContext<'input> {}

impl<'input> PredicatedContextExt<'input>{
	fn new(ctx: &dyn BooleanExpressionContextAttrs<'input>) -> Rc<BooleanExpressionContextAll<'input>>  {
		Rc::new(
			BooleanExpressionContextAll::PredicatedContext(
				BaseParserRuleContext::copy_from(ctx,PredicatedContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ExistsContext<'input> = BaseParserRuleContext<'input,ExistsContextExt<'input>>;

pub trait ExistsContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token EXISTS
	/// Returns `None` if there is no child corresponding to token EXISTS
	fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXISTS, 0)
	}
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ExistsContextAttrs<'input> for ExistsContext<'input>{}

pub struct ExistsContextExt<'input>{
	base:BooleanExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ExistsContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ExistsContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ExistsContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_exists(self);
	}
}

impl<'input> CustomRuleContext<'input> for ExistsContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_booleanExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_booleanExpression }
}

impl<'input> Borrow<BooleanExpressionContextExt<'input>> for ExistsContext<'input>{
	fn borrow(&self) -> &BooleanExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<BooleanExpressionContextExt<'input>> for ExistsContext<'input>{
	fn borrow_mut(&mut self) -> &mut BooleanExpressionContextExt<'input> { &mut self.base }
}

impl<'input> BooleanExpressionContextAttrs<'input> for ExistsContext<'input> {}

impl<'input> ExistsContextExt<'input>{
	fn new(ctx: &dyn BooleanExpressionContextAttrs<'input>) -> Rc<BooleanExpressionContextAll<'input>>  {
		Rc::new(
			BooleanExpressionContextAll::ExistsContext(
				BaseParserRuleContext::copy_from(ctx,ExistsContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type LogicalBinaryContext<'input> = BaseParserRuleContext<'input,LogicalBinaryContextExt<'input>>;

pub trait LogicalBinaryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn booleanExpression_all(&self) ->  Vec<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn booleanExpression(&self, i: usize) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token AND
	/// Returns `None` if there is no child corresponding to token AND
	fn AND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AND, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OR
	/// Returns `None` if there is no child corresponding to token OR
	fn OR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OR, 0)
	}
}

impl<'input> LogicalBinaryContextAttrs<'input> for LogicalBinaryContext<'input>{}

pub struct LogicalBinaryContextExt<'input>{
	base:BooleanExpressionContextExt<'input>,
	pub left: Option<Rc<BooleanExpressionContextAll<'input>>>,
	pub operator: Option<TokenType<'input>>,
	pub right: Option<Rc<BooleanExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{LogicalBinaryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for LogicalBinaryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LogicalBinaryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_logicalBinary(self);
	}
}

impl<'input> CustomRuleContext<'input> for LogicalBinaryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_booleanExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_booleanExpression }
}

impl<'input> Borrow<BooleanExpressionContextExt<'input>> for LogicalBinaryContext<'input>{
	fn borrow(&self) -> &BooleanExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<BooleanExpressionContextExt<'input>> for LogicalBinaryContext<'input>{
	fn borrow_mut(&mut self) -> &mut BooleanExpressionContextExt<'input> { &mut self.base }
}

impl<'input> BooleanExpressionContextAttrs<'input> for LogicalBinaryContext<'input> {}

impl<'input> LogicalBinaryContextExt<'input>{
	fn new(ctx: &dyn BooleanExpressionContextAttrs<'input>) -> Rc<BooleanExpressionContextAll<'input>>  {
		Rc::new(
			BooleanExpressionContextAll::LogicalBinaryContext(
				BaseParserRuleContext::copy_from(ctx,LogicalBinaryContextExt{
					operator:None, 
        			left:None, right:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn  booleanExpression(&mut self,)
	-> Result<Rc<BooleanExpressionContextAll<'input>>,ANTLRError> {
		self.booleanExpression_rec(0)
	}

	fn booleanExpression_rec(&mut self, _p: isize)
	-> Result<Rc<BooleanExpressionContextAll<'input>>,ANTLRError> {
		let recog = self;
		let _parentctx = recog.ctx.take();
		let _parentState = recog.base.get_state();
		let mut _localctx = BooleanExpressionContextExt::new(_parentctx.clone(), recog.base.get_state());
		recog.base.enter_recursion_rule(_localctx.clone(), 190, RULE_booleanExpression, _p);
	    let mut _localctx: Rc<BooleanExpressionContextAll> = _localctx;
        let mut _prevctx = _localctx.clone();
		let _startState = 190;
		let result: Result<(), ANTLRError> = (|| {
			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2351);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(299,&mut recog.base)? {
				1 =>{
					{
					let mut tmp = LogicalNotContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();


					recog.base.set_state(2340);
					recog.base.match_token(NOT,&mut recog.err_handler)?;

					/*InvokeRule booleanExpression*/
					recog.base.set_state(2341);
					recog.booleanExpression_rec(5)?;

					}
				}
			,
				2 =>{
					{
					let mut tmp = ExistsContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2342);
					recog.base.match_token(EXISTS,&mut recog.err_handler)?;

					recog.base.set_state(2343);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule query*/
					recog.base.set_state(2344);
					recog.query()?;

					recog.base.set_state(2345);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					{
					let mut tmp = PredicatedContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					/*InvokeRule valueExpression*/
					recog.base.set_state(2347);
					recog.valueExpression_rec(0)?;

					recog.base.set_state(2349);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(298,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule predicate*/
							recog.base.set_state(2348);
							recog.predicate()?;

							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}

			let tmp = recog.input.lt(-1).cloned();
			recog.ctx.as_ref().unwrap().set_stop(tmp);
			recog.base.set_state(2361);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(301,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					recog.trigger_exit_rule_event();
					_prevctx = _localctx.clone();
					{
					recog.base.set_state(2359);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(300,&mut recog.base)? {
						1 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = LogicalBinaryContextExt::new(&**BooleanExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let BooleanExpressionContextAll::LogicalBinaryContext(ctx) = cast_mut::<_,BooleanExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_booleanExpression);
							_localctx = tmp;
							recog.base.set_state(2353);
							if !({recog.precpred(None, 2)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 2)".to_owned()), None))?;
							}
							recog.base.set_state(2354);
							let tmp = recog.base.match_token(AND,&mut recog.err_handler)?;
							if let BooleanExpressionContextAll::LogicalBinaryContext(ctx) = cast_mut::<_,BooleanExpressionContextAll >(&mut _localctx){
							ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							/*InvokeRule booleanExpression*/
							recog.base.set_state(2355);
							let tmp = recog.booleanExpression_rec(3)?;
							if let BooleanExpressionContextAll::LogicalBinaryContext(ctx) = cast_mut::<_,BooleanExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						2 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = LogicalBinaryContextExt::new(&**BooleanExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let BooleanExpressionContextAll::LogicalBinaryContext(ctx) = cast_mut::<_,BooleanExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_booleanExpression);
							_localctx = tmp;
							recog.base.set_state(2356);
							if !({recog.precpred(None, 1)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 1)".to_owned()), None))?;
							}
							recog.base.set_state(2357);
							let tmp = recog.base.match_token(OR,&mut recog.err_handler)?;
							if let BooleanExpressionContextAll::LogicalBinaryContext(ctx) = cast_mut::<_,BooleanExpressionContextAll >(&mut _localctx){
							ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							/*InvokeRule booleanExpression*/
							recog.base.set_state(2358);
							let tmp = recog.booleanExpression_rec(2)?;
							if let BooleanExpressionContextAll::LogicalBinaryContext(ctx) = cast_mut::<_,BooleanExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					} 
				}
				recog.base.set_state(2363);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(301,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_) => {},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re)=>{
			//_localctx.exception = re;
			recog.err_handler.report_error(&mut recog.base, re);
	        recog.err_handler.recover(&mut recog.base, re)?;}
		}
		recog.base.unroll_recursion_context(_parentctx);

		Ok(_localctx)
	}
}
//------------------- predicate ----------------
pub type PredicateContextAll<'input> = PredicateContext<'input>;


pub type PredicateContext<'input> = BaseParserRuleContext<'input,PredicateContextExt<'input>>;

#[derive(Clone)]
pub struct PredicateContextExt<'input>{
	pub kind: Option<TokenType<'input>>,
	pub lower: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub upper: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub pattern: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub quantifier: Option<TokenType<'input>>,
	pub escapeChar: Option<TokenType<'input>>,
	pub right: Option<Rc<ValueExpressionContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PredicateContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PredicateContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_predicate(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_predicate(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PredicateContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_predicate }
	//fn type_rule_index() -> usize where Self: Sized { RULE_predicate }
}
crate::tid!{PredicateContextExt<'a>}

impl<'input> PredicateContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PredicateContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PredicateContextExt{
				kind: None, quantifier: None, escapeChar: None, 
				lower: None, upper: None, pattern: None, right: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait PredicateContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PredicateContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token AND
/// Returns `None` if there is no child corresponding to token AND
fn AND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AND, 0)
}
/// Retrieves first TerminalNode corresponding to token BETWEEN
/// Returns `None` if there is no child corresponding to token BETWEEN
fn BETWEEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BETWEEN, 0)
}
fn valueExpression_all(&self) ->  Vec<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn valueExpression(&self, i: usize) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token IN
/// Returns `None` if there is no child corresponding to token IN
fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IN, 0)
}
fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token RLIKE
/// Returns `None` if there is no child corresponding to token RLIKE
fn RLIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RLIKE, 0)
}
/// Retrieves first TerminalNode corresponding to token LIKE
/// Returns `None` if there is no child corresponding to token LIKE
fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIKE, 0)
}
/// Retrieves first TerminalNode corresponding to token ANY
/// Returns `None` if there is no child corresponding to token ANY
fn ANY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ANY, 0)
}
/// Retrieves first TerminalNode corresponding to token SOME
/// Returns `None` if there is no child corresponding to token SOME
fn SOME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SOME, 0)
}
/// Retrieves first TerminalNode corresponding to token ALL
/// Returns `None` if there is no child corresponding to token ALL
fn ALL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ALL, 0)
}
/// Retrieves first TerminalNode corresponding to token ESCAPE
/// Returns `None` if there is no child corresponding to token ESCAPE
fn ESCAPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ESCAPE, 0)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}
/// Retrieves first TerminalNode corresponding to token IS
/// Returns `None` if there is no child corresponding to token IS
fn IS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IS, 0)
}
/// Retrieves first TerminalNode corresponding to token NULL
/// Returns `None` if there is no child corresponding to token NULL
fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULL, 0)
}
/// Retrieves first TerminalNode corresponding to token TRUE
/// Returns `None` if there is no child corresponding to token TRUE
fn TRUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRUE, 0)
}
/// Retrieves first TerminalNode corresponding to token FALSE
/// Returns `None` if there is no child corresponding to token FALSE
fn FALSE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FALSE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNKNOWN
/// Returns `None` if there is no child corresponding to token UNKNOWN
fn UNKNOWN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNKNOWN, 0)
}
/// Retrieves first TerminalNode corresponding to token FROM
/// Returns `None` if there is no child corresponding to token FROM
fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FROM, 0)
}
/// Retrieves first TerminalNode corresponding to token DISTINCT
/// Returns `None` if there is no child corresponding to token DISTINCT
fn DISTINCT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DISTINCT, 0)
}

}

impl<'input> PredicateContextAttrs<'input> for PredicateContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn predicate(&mut self,)
	-> Result<Rc<PredicateContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PredicateContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 192, RULE_predicate);
        let mut _localctx: Rc<PredicateContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2446);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(315,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(2365);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2364);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2367);
					let tmp = recog.base.match_token(BETWEEN,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					/*InvokeRule valueExpression*/
					recog.base.set_state(2368);
					let tmp = recog.valueExpression_rec(0)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).lower = Some(tmp.clone());
					  

					recog.base.set_state(2369);
					recog.base.match_token(AND,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2370);
					let tmp = recog.valueExpression_rec(0)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).upper = Some(tmp.clone());
					  

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2373);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2372);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2375);
					let tmp = recog.base.match_token(IN,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					recog.base.set_state(2376);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2377);
					recog.expression()?;

					recog.base.set_state(2382);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					while _la==T__2 {
						{
						{
						recog.base.set_state(2378);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule expression*/
						recog.base.set_state(2379);
						recog.expression()?;

						}
						}
						recog.base.set_state(2384);
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
					}
					recog.base.set_state(2385);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					recog.base.set_state(2388);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2387);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2390);
					let tmp = recog.base.match_token(IN,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					recog.base.set_state(2391);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule query*/
					recog.base.set_state(2392);
					recog.query()?;

					recog.base.set_state(2393);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				4 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					recog.base.set_state(2396);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2395);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2398);
					let tmp = recog.base.match_token(RLIKE,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					/*InvokeRule valueExpression*/
					recog.base.set_state(2399);
					let tmp = recog.valueExpression_rec(0)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).pattern = Some(tmp.clone());
					  

					}
				}
			,
				5 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 5);
					recog.base.enter_outer_alt(None, 5);
					{
					recog.base.set_state(2401);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2400);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2403);
					let tmp = recog.base.match_token(LIKE,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					recog.base.set_state(2404);
					 cast_mut::<_,PredicateContext >(&mut _localctx).quantifier = recog.base.input.lt(1).cloned();
					 
					_la = recog.base.input.la(1);
					if { !(_la==ALL || _la==ANY || _la==SOME) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						 cast_mut::<_,PredicateContext >(&mut _localctx).quantifier = Some(tmp.clone());
						  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(2418);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(309,&mut recog.base)? {
						1 =>{
							{
							recog.base.set_state(2405);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							recog.base.set_state(2406);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}
					,
						2 =>{
							{
							recog.base.set_state(2407);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							/*InvokeRule expression*/
							recog.base.set_state(2408);
							recog.expression()?;

							recog.base.set_state(2413);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							while _la==T__2 {
								{
								{
								recog.base.set_state(2409);
								recog.base.match_token(T__2,&mut recog.err_handler)?;

								/*InvokeRule expression*/
								recog.base.set_state(2410);
								recog.expression()?;

								}
								}
								recog.base.set_state(2415);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
							}
							recog.base.set_state(2416);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					}
				}
			,
				6 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 6);
					recog.base.enter_outer_alt(None, 6);
					{
					recog.base.set_state(2421);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2420);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2423);
					let tmp = recog.base.match_token(LIKE,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					/*InvokeRule valueExpression*/
					recog.base.set_state(2424);
					let tmp = recog.valueExpression_rec(0)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).pattern = Some(tmp.clone());
					  

					recog.base.set_state(2427);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(311,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2425);
							recog.base.match_token(ESCAPE,&mut recog.err_handler)?;

							recog.base.set_state(2426);
							let tmp = recog.base.match_token(STRING,&mut recog.err_handler)?;
							 cast_mut::<_,PredicateContext >(&mut _localctx).escapeChar = Some(tmp.clone());
							  

							}
						}

						_ => {}
					}
					}
				}
			,
				7 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 7);
					recog.base.enter_outer_alt(None, 7);
					{
					recog.base.set_state(2429);
					recog.base.match_token(IS,&mut recog.err_handler)?;

					recog.base.set_state(2431);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2430);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2433);
					let tmp = recog.base.match_token(NULL,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					}
				}
			,
				8 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 8);
					recog.base.enter_outer_alt(None, 8);
					{
					recog.base.set_state(2434);
					recog.base.match_token(IS,&mut recog.err_handler)?;

					recog.base.set_state(2436);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2435);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2438);
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = recog.base.input.lt(1).cloned();
					 
					_la = recog.base.input.la(1);
					if { !(_la==FALSE || _la==TRUE || _la==UNKNOWN) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
						  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}
			,
				9 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 9);
					recog.base.enter_outer_alt(None, 9);
					{
					recog.base.set_state(2439);
					recog.base.match_token(IS,&mut recog.err_handler)?;

					recog.base.set_state(2441);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==NOT {
						{
						recog.base.set_state(2440);
						recog.base.match_token(NOT,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2443);
					let tmp = recog.base.match_token(DISTINCT,&mut recog.err_handler)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).kind = Some(tmp.clone());
					  

					recog.base.set_state(2444);
					recog.base.match_token(FROM,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2445);
					let tmp = recog.valueExpression_rec(0)?;
					 cast_mut::<_,PredicateContext >(&mut _localctx).right = Some(tmp.clone());
					  

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- valueExpression ----------------
#[derive(Debug)]
pub enum ValueExpressionContextAll<'input>{
	ValueExpressionDefaultContext(ValueExpressionDefaultContext<'input>),
	ComparisonContext(ComparisonContext<'input>),
	ArithmeticBinaryContext(ArithmeticBinaryContext<'input>),
	ArithmeticUnaryContext(ArithmeticUnaryContext<'input>),
Error(ValueExpressionContext<'input>)
}
crate::tid!{ValueExpressionContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for ValueExpressionContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for ValueExpressionContextAll<'input>{}

impl<'input> Deref for ValueExpressionContextAll<'input>{
	type Target = dyn ValueExpressionContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use ValueExpressionContextAll::*;
		match self{
			ValueExpressionDefaultContext(inner) => inner,
			ComparisonContext(inner) => inner,
			ArithmeticBinaryContext(inner) => inner,
			ArithmeticUnaryContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ValueExpressionContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type ValueExpressionContext<'input> = BaseParserRuleContext<'input,ValueExpressionContextExt<'input>>;

#[derive(Clone)]
pub struct ValueExpressionContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ValueExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ValueExpressionContext<'input>{
}

impl<'input> CustomRuleContext<'input> for ValueExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_valueExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_valueExpression }
}
crate::tid!{ValueExpressionContextExt<'a>}

impl<'input> ValueExpressionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ValueExpressionContextAll<'input>> {
		Rc::new(
		ValueExpressionContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ValueExpressionContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait ValueExpressionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ValueExpressionContextExt<'input>>{


}

impl<'input> ValueExpressionContextAttrs<'input> for ValueExpressionContext<'input>{}

pub type ValueExpressionDefaultContext<'input> = BaseParserRuleContext<'input,ValueExpressionDefaultContextExt<'input>>;

pub trait ValueExpressionDefaultContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn primaryExpression(&self) -> Option<Rc<PrimaryExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ValueExpressionDefaultContextAttrs<'input> for ValueExpressionDefaultContext<'input>{}

pub struct ValueExpressionDefaultContextExt<'input>{
	base:ValueExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ValueExpressionDefaultContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ValueExpressionDefaultContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ValueExpressionDefaultContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_valueExpressionDefault(self);
	}
}

impl<'input> CustomRuleContext<'input> for ValueExpressionDefaultContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_valueExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_valueExpression }
}

impl<'input> Borrow<ValueExpressionContextExt<'input>> for ValueExpressionDefaultContext<'input>{
	fn borrow(&self) -> &ValueExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ValueExpressionContextExt<'input>> for ValueExpressionDefaultContext<'input>{
	fn borrow_mut(&mut self) -> &mut ValueExpressionContextExt<'input> { &mut self.base }
}

impl<'input> ValueExpressionContextAttrs<'input> for ValueExpressionDefaultContext<'input> {}

impl<'input> ValueExpressionDefaultContextExt<'input>{
	fn new(ctx: &dyn ValueExpressionContextAttrs<'input>) -> Rc<ValueExpressionContextAll<'input>>  {
		Rc::new(
			ValueExpressionContextAll::ValueExpressionDefaultContext(
				BaseParserRuleContext::copy_from(ctx,ValueExpressionDefaultContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ComparisonContext<'input> = BaseParserRuleContext<'input,ComparisonContextExt<'input>>;

pub trait ComparisonContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn comparisonOperator(&self) -> Option<Rc<ComparisonOperatorContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn valueExpression_all(&self) ->  Vec<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn valueExpression(&self, i: usize) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> ComparisonContextAttrs<'input> for ComparisonContext<'input>{}

pub struct ComparisonContextExt<'input>{
	base:ValueExpressionContextExt<'input>,
	pub left: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub right: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ComparisonContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ComparisonContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ComparisonContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_comparison(self);
	}
}

impl<'input> CustomRuleContext<'input> for ComparisonContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_valueExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_valueExpression }
}

impl<'input> Borrow<ValueExpressionContextExt<'input>> for ComparisonContext<'input>{
	fn borrow(&self) -> &ValueExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ValueExpressionContextExt<'input>> for ComparisonContext<'input>{
	fn borrow_mut(&mut self) -> &mut ValueExpressionContextExt<'input> { &mut self.base }
}

impl<'input> ValueExpressionContextAttrs<'input> for ComparisonContext<'input> {}

impl<'input> ComparisonContextExt<'input>{
	fn new(ctx: &dyn ValueExpressionContextAttrs<'input>) -> Rc<ValueExpressionContextAll<'input>>  {
		Rc::new(
			ValueExpressionContextAll::ComparisonContext(
				BaseParserRuleContext::copy_from(ctx,ComparisonContextExt{
        			left:None, right:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ArithmeticBinaryContext<'input> = BaseParserRuleContext<'input,ArithmeticBinaryContextExt<'input>>;

pub trait ArithmeticBinaryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn valueExpression_all(&self) ->  Vec<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn valueExpression(&self, i: usize) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token ASTERISK
	/// Returns `None` if there is no child corresponding to token ASTERISK
	fn ASTERISK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ASTERISK, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SLASH
	/// Returns `None` if there is no child corresponding to token SLASH
	fn SLASH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SLASH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PERCENT
	/// Returns `None` if there is no child corresponding to token PERCENT
	fn PERCENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PERCENT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DIV
	/// Returns `None` if there is no child corresponding to token DIV
	fn DIV(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DIV, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PLUS
	/// Returns `None` if there is no child corresponding to token PLUS
	fn PLUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PLUS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CONCAT_PIPE
	/// Returns `None` if there is no child corresponding to token CONCAT_PIPE
	fn CONCAT_PIPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CONCAT_PIPE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token AMPERSAND
	/// Returns `None` if there is no child corresponding to token AMPERSAND
	fn AMPERSAND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AMPERSAND, 0)
	}
	/// Retrieves first TerminalNode corresponding to token HAT
	/// Returns `None` if there is no child corresponding to token HAT
	fn HAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(HAT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PIPE
	/// Returns `None` if there is no child corresponding to token PIPE
	fn PIPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PIPE, 0)
	}
}

impl<'input> ArithmeticBinaryContextAttrs<'input> for ArithmeticBinaryContext<'input>{}

pub struct ArithmeticBinaryContextExt<'input>{
	base:ValueExpressionContextExt<'input>,
	pub left: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub operator: Option<TokenType<'input>>,
	pub right: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ArithmeticBinaryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ArithmeticBinaryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ArithmeticBinaryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_arithmeticBinary(self);
	}
}

impl<'input> CustomRuleContext<'input> for ArithmeticBinaryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_valueExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_valueExpression }
}

impl<'input> Borrow<ValueExpressionContextExt<'input>> for ArithmeticBinaryContext<'input>{
	fn borrow(&self) -> &ValueExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ValueExpressionContextExt<'input>> for ArithmeticBinaryContext<'input>{
	fn borrow_mut(&mut self) -> &mut ValueExpressionContextExt<'input> { &mut self.base }
}

impl<'input> ValueExpressionContextAttrs<'input> for ArithmeticBinaryContext<'input> {}

impl<'input> ArithmeticBinaryContextExt<'input>{
	fn new(ctx: &dyn ValueExpressionContextAttrs<'input>) -> Rc<ValueExpressionContextAll<'input>>  {
		Rc::new(
			ValueExpressionContextAll::ArithmeticBinaryContext(
				BaseParserRuleContext::copy_from(ctx,ArithmeticBinaryContextExt{
					operator:None, 
        			left:None, right:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ArithmeticUnaryContext<'input> = BaseParserRuleContext<'input,ArithmeticUnaryContextExt<'input>>;

pub trait ArithmeticUnaryContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn valueExpression(&self) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PLUS
	/// Returns `None` if there is no child corresponding to token PLUS
	fn PLUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PLUS, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TILDE
	/// Returns `None` if there is no child corresponding to token TILDE
	fn TILDE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TILDE, 0)
	}
}

impl<'input> ArithmeticUnaryContextAttrs<'input> for ArithmeticUnaryContext<'input>{}

pub struct ArithmeticUnaryContextExt<'input>{
	base:ValueExpressionContextExt<'input>,
	pub operator: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ArithmeticUnaryContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ArithmeticUnaryContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ArithmeticUnaryContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_arithmeticUnary(self);
	}
}

impl<'input> CustomRuleContext<'input> for ArithmeticUnaryContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_valueExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_valueExpression }
}

impl<'input> Borrow<ValueExpressionContextExt<'input>> for ArithmeticUnaryContext<'input>{
	fn borrow(&self) -> &ValueExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ValueExpressionContextExt<'input>> for ArithmeticUnaryContext<'input>{
	fn borrow_mut(&mut self) -> &mut ValueExpressionContextExt<'input> { &mut self.base }
}

impl<'input> ValueExpressionContextAttrs<'input> for ArithmeticUnaryContext<'input> {}

impl<'input> ArithmeticUnaryContextExt<'input>{
	fn new(ctx: &dyn ValueExpressionContextAttrs<'input>) -> Rc<ValueExpressionContextAll<'input>>  {
		Rc::new(
			ValueExpressionContextAll::ArithmeticUnaryContext(
				BaseParserRuleContext::copy_from(ctx,ArithmeticUnaryContextExt{
					operator:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn  valueExpression(&mut self,)
	-> Result<Rc<ValueExpressionContextAll<'input>>,ANTLRError> {
		self.valueExpression_rec(0)
	}

	fn valueExpression_rec(&mut self, _p: isize)
	-> Result<Rc<ValueExpressionContextAll<'input>>,ANTLRError> {
		let recog = self;
		let _parentctx = recog.ctx.take();
		let _parentState = recog.base.get_state();
		let mut _localctx = ValueExpressionContextExt::new(_parentctx.clone(), recog.base.get_state());
		recog.base.enter_recursion_rule(_localctx.clone(), 194, RULE_valueExpression, _p);
	    let mut _localctx: Rc<ValueExpressionContextAll> = _localctx;
        let mut _prevctx = _localctx.clone();
		let _startState = 194;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {
			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2452);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(316,&mut recog.base)? {
				1 =>{
					{
					let mut tmp = ValueExpressionDefaultContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();


					/*InvokeRule primaryExpression*/
					recog.base.set_state(2449);
					recog.primaryExpression_rec(0)?;

					}
				}
			,
				2 =>{
					{
					let mut tmp = ArithmeticUnaryContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2450);
					if let ValueExpressionContextAll::ArithmeticUnaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
					ctx.operator = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
					_la = recog.base.input.la(1);
					if { !(((((_la - 268)) & !0x3f) == 0 && ((1usize << (_la - 268)) & ((1usize << (PLUS - 268)) | (1usize << (MINUS - 268)) | (1usize << (TILDE - 268)))) != 0)) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						if let ValueExpressionContextAll::ArithmeticUnaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
						ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule valueExpression*/
					recog.base.set_state(2451);
					recog.valueExpression_rec(7)?;

					}
				}

				_ => {}
			}

			let tmp = recog.input.lt(-1).cloned();
			recog.ctx.as_ref().unwrap().set_stop(tmp);
			recog.base.set_state(2475);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(318,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					recog.trigger_exit_rule_event();
					_prevctx = _localctx.clone();
					{
					recog.base.set_state(2473);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(317,&mut recog.base)? {
						1 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = ArithmeticBinaryContextExt::new(&**ValueExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_valueExpression);
							_localctx = tmp;
							recog.base.set_state(2454);
							if !({recog.precpred(None, 6)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 6)".to_owned()), None))?;
							}
							recog.base.set_state(2455);
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.operator = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
							_la = recog.base.input.la(1);
							if { !(_la==DIV || ((((_la - 270)) & !0x3f) == 0 && ((1usize << (_la - 270)) & ((1usize << (ASTERISK - 270)) | (1usize << (SLASH - 270)) | (1usize << (PERCENT - 270)))) != 0)) } {
								let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
								if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
								ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							/*InvokeRule valueExpression*/
							recog.base.set_state(2456);
							let tmp = recog.valueExpression_rec(7)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						2 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = ArithmeticBinaryContextExt::new(&**ValueExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_valueExpression);
							_localctx = tmp;
							recog.base.set_state(2457);
							if !({recog.precpred(None, 5)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 5)".to_owned()), None))?;
							}
							recog.base.set_state(2458);
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.operator = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
							_la = recog.base.input.la(1);
							if { !(((((_la - 268)) & !0x3f) == 0 && ((1usize << (_la - 268)) & ((1usize << (PLUS - 268)) | (1usize << (MINUS - 268)) | (1usize << (CONCAT_PIPE - 268)))) != 0)) } {
								let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
								if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
								ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							/*InvokeRule valueExpression*/
							recog.base.set_state(2459);
							let tmp = recog.valueExpression_rec(6)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						3 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = ArithmeticBinaryContextExt::new(&**ValueExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_valueExpression);
							_localctx = tmp;
							recog.base.set_state(2460);
							if !({recog.precpred(None, 4)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 4)".to_owned()), None))?;
							}
							recog.base.set_state(2461);
							let tmp = recog.base.match_token(AMPERSAND,&mut recog.err_handler)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							/*InvokeRule valueExpression*/
							recog.base.set_state(2462);
							let tmp = recog.valueExpression_rec(5)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						4 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = ArithmeticBinaryContextExt::new(&**ValueExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_valueExpression);
							_localctx = tmp;
							recog.base.set_state(2463);
							if !({recog.precpred(None, 3)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 3)".to_owned()), None))?;
							}
							recog.base.set_state(2464);
							let tmp = recog.base.match_token(HAT,&mut recog.err_handler)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							/*InvokeRule valueExpression*/
							recog.base.set_state(2465);
							let tmp = recog.valueExpression_rec(4)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						5 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = ArithmeticBinaryContextExt::new(&**ValueExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_valueExpression);
							_localctx = tmp;
							recog.base.set_state(2466);
							if !({recog.precpred(None, 2)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 2)".to_owned()), None))?;
							}
							recog.base.set_state(2467);
							let tmp = recog.base.match_token(PIPE,&mut recog.err_handler)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.operator = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							/*InvokeRule valueExpression*/
							recog.base.set_state(2468);
							let tmp = recog.valueExpression_rec(3)?;
							if let ValueExpressionContextAll::ArithmeticBinaryContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}
					,
						6 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = ComparisonContextExt::new(&**ValueExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let ValueExpressionContextAll::ComparisonContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut tmp){
								ctx.left = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_valueExpression);
							_localctx = tmp;
							recog.base.set_state(2469);
							if !({recog.precpred(None, 1)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 1)".to_owned()), None))?;
							}
							/*InvokeRule comparisonOperator*/
							recog.base.set_state(2470);
							recog.comparisonOperator()?;

							/*InvokeRule valueExpression*/
							recog.base.set_state(2471);
							let tmp = recog.valueExpression_rec(2)?;
							if let ValueExpressionContextAll::ComparisonContext(ctx) = cast_mut::<_,ValueExpressionContextAll >(&mut _localctx){
							ctx.right = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					} 
				}
				recog.base.set_state(2477);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(318,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_) => {},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re)=>{
			//_localctx.exception = re;
			recog.err_handler.report_error(&mut recog.base, re);
	        recog.err_handler.recover(&mut recog.base, re)?;}
		}
		recog.base.unroll_recursion_context(_parentctx);

		Ok(_localctx)
	}
}
//------------------- primaryExpression ----------------
#[derive(Debug)]
pub enum PrimaryExpressionContextAll<'input>{
	StructContext(StructContext<'input>),
	DereferenceContext(DereferenceContext<'input>),
	SimpleCaseContext(SimpleCaseContext<'input>),
	ColumnReferenceContext(ColumnReferenceContext<'input>),
	RowConstructorContext(RowConstructorContext<'input>),
	LastContext(LastContext<'input>),
	StarContext(StarContext<'input>),
	OverlayContext(OverlayContext<'input>),
	SubscriptContext(SubscriptContext<'input>),
	SubqueryExpressionContext(SubqueryExpressionContext<'input>),
	SubstringContext(SubstringContext<'input>),
	CurrentDatetimeContext(CurrentDatetimeContext<'input>),
	CastContext(CastContext<'input>),
	ConstantDefaultContext(ConstantDefaultContext<'input>),
	LambdaContext(LambdaContext<'input>),
	ParenthesizedExpressionContext(ParenthesizedExpressionContext<'input>),
	ExtractContext(ExtractContext<'input>),
	TrimContext(TrimContext<'input>),
	FunctionCallContext(FunctionCallContext<'input>),
	SearchedCaseContext(SearchedCaseContext<'input>),
	PositionContext(PositionContext<'input>),
	FirstContext(FirstContext<'input>),
Error(PrimaryExpressionContext<'input>)
}
crate::tid!{PrimaryExpressionContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for PrimaryExpressionContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for PrimaryExpressionContextAll<'input>{}

impl<'input> Deref for PrimaryExpressionContextAll<'input>{
	type Target = dyn PrimaryExpressionContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use PrimaryExpressionContextAll::*;
		match self{
			StructContext(inner) => inner,
			DereferenceContext(inner) => inner,
			SimpleCaseContext(inner) => inner,
			ColumnReferenceContext(inner) => inner,
			RowConstructorContext(inner) => inner,
			LastContext(inner) => inner,
			StarContext(inner) => inner,
			OverlayContext(inner) => inner,
			SubscriptContext(inner) => inner,
			SubqueryExpressionContext(inner) => inner,
			SubstringContext(inner) => inner,
			CurrentDatetimeContext(inner) => inner,
			CastContext(inner) => inner,
			ConstantDefaultContext(inner) => inner,
			LambdaContext(inner) => inner,
			ParenthesizedExpressionContext(inner) => inner,
			ExtractContext(inner) => inner,
			TrimContext(inner) => inner,
			FunctionCallContext(inner) => inner,
			SearchedCaseContext(inner) => inner,
			PositionContext(inner) => inner,
			FirstContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PrimaryExpressionContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type PrimaryExpressionContext<'input> = BaseParserRuleContext<'input,PrimaryExpressionContextExt<'input>>;

#[derive(Clone)]
pub struct PrimaryExpressionContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PrimaryExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PrimaryExpressionContext<'input>{
}

impl<'input> CustomRuleContext<'input> for PrimaryExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}
crate::tid!{PrimaryExpressionContextExt<'a>}

impl<'input> PrimaryExpressionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PrimaryExpressionContextAll<'input>> {
		Rc::new(
		PrimaryExpressionContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PrimaryExpressionContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait PrimaryExpressionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PrimaryExpressionContextExt<'input>>{


}

impl<'input> PrimaryExpressionContextAttrs<'input> for PrimaryExpressionContext<'input>{}

pub type StructContext<'input> = BaseParserRuleContext<'input,StructContextExt<'input>>;

pub trait StructContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token STRUCT
	/// Returns `None` if there is no child corresponding to token STRUCT
	fn STRUCT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRUCT, 0)
	}
	fn namedExpression_all(&self) ->  Vec<Rc<NamedExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn namedExpression(&self, i: usize) -> Option<Rc<NamedExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> StructContextAttrs<'input> for StructContext<'input>{}

pub struct StructContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub namedExpression: Option<Rc<NamedExpressionContextAll<'input>>>,
	pub argument:Vec<Rc<NamedExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{StructContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for StructContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StructContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_struct(self);
	}
}

impl<'input> CustomRuleContext<'input> for StructContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for StructContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for StructContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for StructContext<'input> {}

impl<'input> StructContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::StructContext(
				BaseParserRuleContext::copy_from(ctx,StructContextExt{
        			namedExpression:None, 
        			argument:Vec::new(), 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DereferenceContext<'input> = BaseParserRuleContext<'input,DereferenceContextExt<'input>>;

pub trait DereferenceContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn primaryExpression(&self) -> Option<Rc<PrimaryExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> DereferenceContextAttrs<'input> for DereferenceContext<'input>{}

pub struct DereferenceContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub opt_base: Option<Rc<PrimaryExpressionContextAll<'input>>>,
	pub fieldName: Option<Rc<IdentifierContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{DereferenceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DereferenceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DereferenceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_dereference(self);
	}
}

impl<'input> CustomRuleContext<'input> for DereferenceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for DereferenceContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for DereferenceContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for DereferenceContext<'input> {}

impl<'input> DereferenceContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::DereferenceContext(
				BaseParserRuleContext::copy_from(ctx,DereferenceContextExt{
        			opt_base:None, fieldName:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SimpleCaseContext<'input> = BaseParserRuleContext<'input,SimpleCaseContextExt<'input>>;

pub trait SimpleCaseContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CASE
	/// Returns `None` if there is no child corresponding to token CASE
	fn CASE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CASE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token END
	/// Returns `None` if there is no child corresponding to token END
	fn END(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(END, 0)
	}
	fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn whenClause_all(&self) ->  Vec<Rc<WhenClauseContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn whenClause(&self, i: usize) -> Option<Rc<WhenClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token ELSE
	/// Returns `None` if there is no child corresponding to token ELSE
	fn ELSE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ELSE, 0)
	}
}

impl<'input> SimpleCaseContextAttrs<'input> for SimpleCaseContext<'input>{}

pub struct SimpleCaseContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub value: Option<Rc<ExpressionContextAll<'input>>>,
	pub elseExpression: Option<Rc<ExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SimpleCaseContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SimpleCaseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SimpleCaseContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_simpleCase(self);
	}
}

impl<'input> CustomRuleContext<'input> for SimpleCaseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for SimpleCaseContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for SimpleCaseContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for SimpleCaseContext<'input> {}

impl<'input> SimpleCaseContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::SimpleCaseContext(
				BaseParserRuleContext::copy_from(ctx,SimpleCaseContextExt{
        			value:None, elseExpression:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ColumnReferenceContext<'input> = BaseParserRuleContext<'input,ColumnReferenceContextExt<'input>>;

pub trait ColumnReferenceContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ColumnReferenceContextAttrs<'input> for ColumnReferenceContext<'input>{}

pub struct ColumnReferenceContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ColumnReferenceContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ColumnReferenceContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ColumnReferenceContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_columnReference(self);
	}
}

impl<'input> CustomRuleContext<'input> for ColumnReferenceContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for ColumnReferenceContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for ColumnReferenceContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for ColumnReferenceContext<'input> {}

impl<'input> ColumnReferenceContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::ColumnReferenceContext(
				BaseParserRuleContext::copy_from(ctx,ColumnReferenceContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RowConstructorContext<'input> = BaseParserRuleContext<'input,RowConstructorContextExt<'input>>;

pub trait RowConstructorContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn namedExpression_all(&self) ->  Vec<Rc<NamedExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn namedExpression(&self, i: usize) -> Option<Rc<NamedExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> RowConstructorContextAttrs<'input> for RowConstructorContext<'input>{}

pub struct RowConstructorContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RowConstructorContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RowConstructorContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RowConstructorContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_rowConstructor(self);
	}
}

impl<'input> CustomRuleContext<'input> for RowConstructorContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for RowConstructorContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for RowConstructorContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for RowConstructorContext<'input> {}

impl<'input> RowConstructorContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::RowConstructorContext(
				BaseParserRuleContext::copy_from(ctx,RowConstructorContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type LastContext<'input> = BaseParserRuleContext<'input,LastContextExt<'input>>;

pub trait LastContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token LAST
	/// Returns `None` if there is no child corresponding to token LAST
	fn LAST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LAST, 0)
	}
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IGNORE
	/// Returns `None` if there is no child corresponding to token IGNORE
	fn IGNORE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IGNORE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NULLS
	/// Returns `None` if there is no child corresponding to token NULLS
	fn NULLS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NULLS, 0)
	}
}

impl<'input> LastContextAttrs<'input> for LastContext<'input>{}

pub struct LastContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{LastContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for LastContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LastContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_last(self);
	}
}

impl<'input> CustomRuleContext<'input> for LastContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for LastContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for LastContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for LastContext<'input> {}

impl<'input> LastContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::LastContext(
				BaseParserRuleContext::copy_from(ctx,LastContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type StarContext<'input> = BaseParserRuleContext<'input,StarContextExt<'input>>;

pub trait StarContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token ASTERISK
	/// Returns `None` if there is no child corresponding to token ASTERISK
	fn ASTERISK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ASTERISK, 0)
	}
	fn qualifiedName(&self) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> StarContextAttrs<'input> for StarContext<'input>{}

pub struct StarContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{StarContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for StarContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StarContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_star(self);
	}
}

impl<'input> CustomRuleContext<'input> for StarContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for StarContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for StarContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for StarContext<'input> {}

impl<'input> StarContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::StarContext(
				BaseParserRuleContext::copy_from(ctx,StarContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type OverlayContext<'input> = BaseParserRuleContext<'input,OverlayContextExt<'input>>;

pub trait OverlayContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token OVERLAY
	/// Returns `None` if there is no child corresponding to token OVERLAY
	fn OVERLAY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OVERLAY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token PLACING
	/// Returns `None` if there is no child corresponding to token PLACING
	fn PLACING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PLACING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	fn valueExpression_all(&self) ->  Vec<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn valueExpression(&self, i: usize) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token FOR
	/// Returns `None` if there is no child corresponding to token FOR
	fn FOR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FOR, 0)
	}
}

impl<'input> OverlayContextAttrs<'input> for OverlayContext<'input>{}

pub struct OverlayContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub input: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub replace: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub position: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub length: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{OverlayContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for OverlayContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for OverlayContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_overlay(self);
	}
}

impl<'input> CustomRuleContext<'input> for OverlayContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for OverlayContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for OverlayContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for OverlayContext<'input> {}

impl<'input> OverlayContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::OverlayContext(
				BaseParserRuleContext::copy_from(ctx,OverlayContextExt{
        			input:None, replace:None, position:None, length:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SubscriptContext<'input> = BaseParserRuleContext<'input,SubscriptContextExt<'input>>;

pub trait SubscriptContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn primaryExpression(&self) -> Option<Rc<PrimaryExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn valueExpression(&self) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SubscriptContextAttrs<'input> for SubscriptContext<'input>{}

pub struct SubscriptContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub value: Option<Rc<PrimaryExpressionContextAll<'input>>>,
	pub index: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SubscriptContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SubscriptContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SubscriptContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_subscript(self);
	}
}

impl<'input> CustomRuleContext<'input> for SubscriptContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for SubscriptContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for SubscriptContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for SubscriptContext<'input> {}

impl<'input> SubscriptContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::SubscriptContext(
				BaseParserRuleContext::copy_from(ctx,SubscriptContextExt{
        			value:None, index:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SubqueryExpressionContext<'input> = BaseParserRuleContext<'input,SubqueryExpressionContextExt<'input>>;

pub trait SubqueryExpressionContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn query(&self) -> Option<Rc<QueryContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SubqueryExpressionContextAttrs<'input> for SubqueryExpressionContext<'input>{}

pub struct SubqueryExpressionContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SubqueryExpressionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SubqueryExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SubqueryExpressionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_subqueryExpression(self);
	}
}

impl<'input> CustomRuleContext<'input> for SubqueryExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for SubqueryExpressionContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for SubqueryExpressionContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for SubqueryExpressionContext<'input> {}

impl<'input> SubqueryExpressionContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::SubqueryExpressionContext(
				BaseParserRuleContext::copy_from(ctx,SubqueryExpressionContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SubstringContext<'input> = BaseParserRuleContext<'input,SubstringContextExt<'input>>;

pub trait SubstringContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SUBSTR
	/// Returns `None` if there is no child corresponding to token SUBSTR
	fn SUBSTR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SUBSTR, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SUBSTRING
	/// Returns `None` if there is no child corresponding to token SUBSTRING
	fn SUBSTRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SUBSTRING, 0)
	}
	fn valueExpression_all(&self) ->  Vec<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn valueExpression(&self, i: usize) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FOR
	/// Returns `None` if there is no child corresponding to token FOR
	fn FOR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FOR, 0)
	}
}

impl<'input> SubstringContextAttrs<'input> for SubstringContext<'input>{}

pub struct SubstringContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub str: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub pos: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub len: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SubstringContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SubstringContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SubstringContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_substring(self);
	}
}

impl<'input> CustomRuleContext<'input> for SubstringContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for SubstringContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for SubstringContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for SubstringContext<'input> {}

impl<'input> SubstringContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::SubstringContext(
				BaseParserRuleContext::copy_from(ctx,SubstringContextExt{
        			str:None, pos:None, len:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CurrentDatetimeContext<'input> = BaseParserRuleContext<'input,CurrentDatetimeContextExt<'input>>;

pub trait CurrentDatetimeContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CURRENT_DATE
	/// Returns `None` if there is no child corresponding to token CURRENT_DATE
	fn CURRENT_DATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CURRENT_DATE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token CURRENT_TIMESTAMP
	/// Returns `None` if there is no child corresponding to token CURRENT_TIMESTAMP
	fn CURRENT_TIMESTAMP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CURRENT_TIMESTAMP, 0)
	}
}

impl<'input> CurrentDatetimeContextAttrs<'input> for CurrentDatetimeContext<'input>{}

pub struct CurrentDatetimeContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub name: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{CurrentDatetimeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CurrentDatetimeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CurrentDatetimeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_currentDatetime(self);
	}
}

impl<'input> CustomRuleContext<'input> for CurrentDatetimeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for CurrentDatetimeContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for CurrentDatetimeContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for CurrentDatetimeContext<'input> {}

impl<'input> CurrentDatetimeContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::CurrentDatetimeContext(
				BaseParserRuleContext::copy_from(ctx,CurrentDatetimeContextExt{
					name:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type CastContext<'input> = BaseParserRuleContext<'input,CastContextExt<'input>>;

pub trait CastContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CAST
	/// Returns `None` if there is no child corresponding to token CAST
	fn CAST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CAST, 0)
	}
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token AS
	/// Returns `None` if there is no child corresponding to token AS
	fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(AS, 0)
	}
	fn dataType(&self) -> Option<Rc<DataTypeContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> CastContextAttrs<'input> for CastContext<'input>{}

pub struct CastContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{CastContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for CastContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for CastContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_cast(self);
	}
}

impl<'input> CustomRuleContext<'input> for CastContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for CastContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for CastContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for CastContext<'input> {}

impl<'input> CastContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::CastContext(
				BaseParserRuleContext::copy_from(ctx,CastContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ConstantDefaultContext<'input> = BaseParserRuleContext<'input,ConstantDefaultContextExt<'input>>;

pub trait ConstantDefaultContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn constant(&self) -> Option<Rc<ConstantContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ConstantDefaultContextAttrs<'input> for ConstantDefaultContext<'input>{}

pub struct ConstantDefaultContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ConstantDefaultContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ConstantDefaultContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ConstantDefaultContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_constantDefault(self);
	}
}

impl<'input> CustomRuleContext<'input> for ConstantDefaultContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for ConstantDefaultContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for ConstantDefaultContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for ConstantDefaultContext<'input> {}

impl<'input> ConstantDefaultContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::ConstantDefaultContext(
				BaseParserRuleContext::copy_from(ctx,ConstantDefaultContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type LambdaContext<'input> = BaseParserRuleContext<'input,LambdaContextExt<'input>>;

pub trait LambdaContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> LambdaContextAttrs<'input> for LambdaContext<'input>{}

pub struct LambdaContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{LambdaContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for LambdaContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LambdaContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_lambda(self);
	}
}

impl<'input> CustomRuleContext<'input> for LambdaContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for LambdaContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for LambdaContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for LambdaContext<'input> {}

impl<'input> LambdaContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::LambdaContext(
				BaseParserRuleContext::copy_from(ctx,LambdaContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ParenthesizedExpressionContext<'input> = BaseParserRuleContext<'input,ParenthesizedExpressionContextExt<'input>>;

pub trait ParenthesizedExpressionContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ParenthesizedExpressionContextAttrs<'input> for ParenthesizedExpressionContext<'input>{}

pub struct ParenthesizedExpressionContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ParenthesizedExpressionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ParenthesizedExpressionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ParenthesizedExpressionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_parenthesizedExpression(self);
	}
}

impl<'input> CustomRuleContext<'input> for ParenthesizedExpressionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for ParenthesizedExpressionContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for ParenthesizedExpressionContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for ParenthesizedExpressionContext<'input> {}

impl<'input> ParenthesizedExpressionContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::ParenthesizedExpressionContext(
				BaseParserRuleContext::copy_from(ctx,ParenthesizedExpressionContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ExtractContext<'input> = BaseParserRuleContext<'input,ExtractContextExt<'input>>;

pub trait ExtractContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token EXTRACT
	/// Returns `None` if there is no child corresponding to token EXTRACT
	fn EXTRACT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXTRACT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn valueExpression(&self) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ExtractContextAttrs<'input> for ExtractContext<'input>{}

pub struct ExtractContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub field: Option<Rc<IdentifierContextAll<'input>>>,
	pub source: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ExtractContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ExtractContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ExtractContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_extract(self);
	}
}

impl<'input> CustomRuleContext<'input> for ExtractContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for ExtractContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for ExtractContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for ExtractContext<'input> {}

impl<'input> ExtractContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::ExtractContext(
				BaseParserRuleContext::copy_from(ctx,ExtractContextExt{
        			field:None, source:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type TrimContext<'input> = BaseParserRuleContext<'input,TrimContextExt<'input>>;

pub trait TrimContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token TRIM
	/// Returns `None` if there is no child corresponding to token TRIM
	fn TRIM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TRIM, 0)
	}
	/// Retrieves first TerminalNode corresponding to token FROM
	/// Returns `None` if there is no child corresponding to token FROM
	fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FROM, 0)
	}
	fn valueExpression_all(&self) ->  Vec<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn valueExpression(&self, i: usize) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token BOTH
	/// Returns `None` if there is no child corresponding to token BOTH
	fn BOTH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(BOTH, 0)
	}
	/// Retrieves first TerminalNode corresponding to token LEADING
	/// Returns `None` if there is no child corresponding to token LEADING
	fn LEADING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LEADING, 0)
	}
	/// Retrieves first TerminalNode corresponding to token TRAILING
	/// Returns `None` if there is no child corresponding to token TRAILING
	fn TRAILING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TRAILING, 0)
	}
}

impl<'input> TrimContextAttrs<'input> for TrimContext<'input>{}

pub struct TrimContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub trimOption: Option<TokenType<'input>>,
	pub trimStr: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub srcStr: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{TrimContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TrimContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TrimContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_trim(self);
	}
}

impl<'input> CustomRuleContext<'input> for TrimContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for TrimContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for TrimContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for TrimContext<'input> {}

impl<'input> TrimContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::TrimContext(
				BaseParserRuleContext::copy_from(ctx,TrimContextExt{
					trimOption:None, 
        			trimStr:None, srcStr:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type FunctionCallContext<'input> = BaseParserRuleContext<'input,FunctionCallContextExt<'input>>;

pub trait FunctionCallContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn functionName(&self) -> Option<Rc<FunctionNameContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token FILTER
	/// Returns `None` if there is no child corresponding to token FILTER
	fn FILTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FILTER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token WHERE
	/// Returns `None` if there is no child corresponding to token WHERE
	fn WHERE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(WHERE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token OVER
	/// Returns `None` if there is no child corresponding to token OVER
	fn OVER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(OVER, 0)
	}
	fn windowSpec(&self) -> Option<Rc<WindowSpecContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn booleanExpression(&self) -> Option<Rc<BooleanExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn setQuantifier(&self) -> Option<Rc<SetQuantifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> FunctionCallContextAttrs<'input> for FunctionCallContext<'input>{}

pub struct FunctionCallContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub expression: Option<Rc<ExpressionContextAll<'input>>>,
	pub argument:Vec<Rc<ExpressionContextAll<'input>>>,
	pub r#where: Option<Rc<BooleanExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{FunctionCallContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for FunctionCallContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FunctionCallContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_functionCall(self);
	}
}

impl<'input> CustomRuleContext<'input> for FunctionCallContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for FunctionCallContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for FunctionCallContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for FunctionCallContext<'input> {}

impl<'input> FunctionCallContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::FunctionCallContext(
				BaseParserRuleContext::copy_from(ctx,FunctionCallContextExt{
        			expression:None, r#where:None, 
        			argument:Vec::new(), 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SearchedCaseContext<'input> = BaseParserRuleContext<'input,SearchedCaseContextExt<'input>>;

pub trait SearchedCaseContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CASE
	/// Returns `None` if there is no child corresponding to token CASE
	fn CASE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CASE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token END
	/// Returns `None` if there is no child corresponding to token END
	fn END(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(END, 0)
	}
	fn whenClause_all(&self) ->  Vec<Rc<WhenClauseContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn whenClause(&self, i: usize) -> Option<Rc<WhenClauseContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token ELSE
	/// Returns `None` if there is no child corresponding to token ELSE
	fn ELSE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ELSE, 0)
	}
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> SearchedCaseContextAttrs<'input> for SearchedCaseContext<'input>{}

pub struct SearchedCaseContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub elseExpression: Option<Rc<ExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{SearchedCaseContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SearchedCaseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SearchedCaseContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_searchedCase(self);
	}
}

impl<'input> CustomRuleContext<'input> for SearchedCaseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for SearchedCaseContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for SearchedCaseContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for SearchedCaseContext<'input> {}

impl<'input> SearchedCaseContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::SearchedCaseContext(
				BaseParserRuleContext::copy_from(ctx,SearchedCaseContextExt{
        			elseExpression:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type PositionContext<'input> = BaseParserRuleContext<'input,PositionContextExt<'input>>;

pub trait PositionContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token POSITION
	/// Returns `None` if there is no child corresponding to token POSITION
	fn POSITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(POSITION, 0)
	}
	/// Retrieves first TerminalNode corresponding to token IN
	/// Returns `None` if there is no child corresponding to token IN
	fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IN, 0)
	}
	fn valueExpression_all(&self) ->  Vec<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn valueExpression(&self, i: usize) -> Option<Rc<ValueExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> PositionContextAttrs<'input> for PositionContext<'input>{}

pub struct PositionContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	pub substr: Option<Rc<ValueExpressionContextAll<'input>>>,
	pub str: Option<Rc<ValueExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{PositionContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for PositionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PositionContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_position(self);
	}
}

impl<'input> CustomRuleContext<'input> for PositionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for PositionContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for PositionContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for PositionContext<'input> {}

impl<'input> PositionContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::PositionContext(
				BaseParserRuleContext::copy_from(ctx,PositionContextExt{
        			substr:None, str:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type FirstContext<'input> = BaseParserRuleContext<'input,FirstContextExt<'input>>;

pub trait FirstContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token FIRST
	/// Returns `None` if there is no child corresponding to token FIRST
	fn FIRST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FIRST, 0)
	}
	fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token IGNORE
	/// Returns `None` if there is no child corresponding to token IGNORE
	fn IGNORE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IGNORE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NULLS
	/// Returns `None` if there is no child corresponding to token NULLS
	fn NULLS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NULLS, 0)
	}
}

impl<'input> FirstContextAttrs<'input> for FirstContext<'input>{}

pub struct FirstContextExt<'input>{
	base:PrimaryExpressionContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{FirstContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for FirstContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FirstContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_first(self);
	}
}

impl<'input> CustomRuleContext<'input> for FirstContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_primaryExpression }
	//fn type_rule_index() -> usize where Self: Sized { RULE_primaryExpression }
}

impl<'input> Borrow<PrimaryExpressionContextExt<'input>> for FirstContext<'input>{
	fn borrow(&self) -> &PrimaryExpressionContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<PrimaryExpressionContextExt<'input>> for FirstContext<'input>{
	fn borrow_mut(&mut self) -> &mut PrimaryExpressionContextExt<'input> { &mut self.base }
}

impl<'input> PrimaryExpressionContextAttrs<'input> for FirstContext<'input> {}

impl<'input> FirstContextExt<'input>{
	fn new(ctx: &dyn PrimaryExpressionContextAttrs<'input>) -> Rc<PrimaryExpressionContextAll<'input>>  {
		Rc::new(
			PrimaryExpressionContextAll::FirstContext(
				BaseParserRuleContext::copy_from(ctx,FirstContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn  primaryExpression(&mut self,)
	-> Result<Rc<PrimaryExpressionContextAll<'input>>,ANTLRError> {
		self.primaryExpression_rec(0)
	}

	fn primaryExpression_rec(&mut self, _p: isize)
	-> Result<Rc<PrimaryExpressionContextAll<'input>>,ANTLRError> {
		let recog = self;
		let _parentctx = recog.ctx.take();
		let _parentState = recog.base.get_state();
		let mut _localctx = PrimaryExpressionContextExt::new(_parentctx.clone(), recog.base.get_state());
		recog.base.enter_recursion_rule(_localctx.clone(), 196, RULE_primaryExpression, _p);
	    let mut _localctx: Rc<PrimaryExpressionContextAll> = _localctx;
        let mut _prevctx = _localctx.clone();
		let _startState = 196;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {
			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2662);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(338,&mut recog.base)? {
				1 =>{
					{
					let mut tmp = CurrentDatetimeContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();


					recog.base.set_state(2479);
					if let PrimaryExpressionContextAll::CurrentDatetimeContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.name = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
					_la = recog.base.input.la(1);
					if { !(_la==CURRENT_DATE || _la==CURRENT_TIMESTAMP) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						if let PrimaryExpressionContextAll::CurrentDatetimeContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
						ctx.name = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}
			,
				2 =>{
					{
					let mut tmp = SearchedCaseContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2480);
					recog.base.match_token(CASE,&mut recog.err_handler)?;

					recog.base.set_state(2482); 
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					loop {
						{
						{
						/*InvokeRule whenClause*/
						recog.base.set_state(2481);
						recog.whenClause()?;

						}
						}
						recog.base.set_state(2484); 
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if !(_la==WHEN) {break}
					}
					recog.base.set_state(2488);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==ELSE {
						{
						recog.base.set_state(2486);
						recog.base.match_token(ELSE,&mut recog.err_handler)?;

						/*InvokeRule expression*/
						recog.base.set_state(2487);
						let tmp = recog.expression()?;
						if let PrimaryExpressionContextAll::SearchedCaseContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
						ctx.elseExpression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					recog.base.set_state(2490);
					recog.base.match_token(END,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					{
					let mut tmp = SimpleCaseContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2492);
					recog.base.match_token(CASE,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2493);
					let tmp = recog.expression()?;
					if let PrimaryExpressionContextAll::SimpleCaseContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.value = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2495); 
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					loop {
						{
						{
						/*InvokeRule whenClause*/
						recog.base.set_state(2494);
						recog.whenClause()?;

						}
						}
						recog.base.set_state(2497); 
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if !(_la==WHEN) {break}
					}
					recog.base.set_state(2501);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==ELSE {
						{
						recog.base.set_state(2499);
						recog.base.match_token(ELSE,&mut recog.err_handler)?;

						/*InvokeRule expression*/
						recog.base.set_state(2500);
						let tmp = recog.expression()?;
						if let PrimaryExpressionContextAll::SimpleCaseContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
						ctx.elseExpression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					recog.base.set_state(2503);
					recog.base.match_token(END,&mut recog.err_handler)?;

					}
				}
			,
				4 =>{
					{
					let mut tmp = CastContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2505);
					recog.base.match_token(CAST,&mut recog.err_handler)?;

					recog.base.set_state(2506);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2507);
					recog.expression()?;

					recog.base.set_state(2508);
					recog.base.match_token(AS,&mut recog.err_handler)?;

					/*InvokeRule dataType*/
					recog.base.set_state(2509);
					recog.dataType()?;

					recog.base.set_state(2510);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				5 =>{
					{
					let mut tmp = StructContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2512);
					recog.base.match_token(STRUCT,&mut recog.err_handler)?;

					recog.base.set_state(2513);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					recog.base.set_state(2522);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(324,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule namedExpression*/
							recog.base.set_state(2514);
							let tmp = recog.namedExpression()?;
							if let PrimaryExpressionContextAll::StructContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.namedExpression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							let temp = if let PrimaryExpressionContextAll::StructContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.namedExpression.clone().unwrap() } else {unreachable!("cant cast");} ;
							if let PrimaryExpressionContextAll::StructContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.argument.push(temp); } else {unreachable!("cant cast");}  
							recog.base.set_state(2519);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							while _la==T__2 {
								{
								{
								recog.base.set_state(2515);
								recog.base.match_token(T__2,&mut recog.err_handler)?;

								/*InvokeRule namedExpression*/
								recog.base.set_state(2516);
								let tmp = recog.namedExpression()?;
								if let PrimaryExpressionContextAll::StructContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
								ctx.namedExpression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

								let temp = if let PrimaryExpressionContextAll::StructContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
								ctx.namedExpression.clone().unwrap() } else {unreachable!("cant cast");} ;
								if let PrimaryExpressionContextAll::StructContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
								ctx.argument.push(temp); } else {unreachable!("cant cast");}  
								}
								}
								recog.base.set_state(2521);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
							}
							}
						}

						_ => {}
					}
					recog.base.set_state(2524);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				6 =>{
					{
					let mut tmp = FirstContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2525);
					recog.base.match_token(FIRST,&mut recog.err_handler)?;

					recog.base.set_state(2526);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2527);
					recog.expression()?;

					recog.base.set_state(2530);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==IGNORE {
						{
						recog.base.set_state(2528);
						recog.base.match_token(IGNORE,&mut recog.err_handler)?;

						recog.base.set_state(2529);
						recog.base.match_token(NULLS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2532);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				7 =>{
					{
					let mut tmp = LastContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2534);
					recog.base.match_token(LAST,&mut recog.err_handler)?;

					recog.base.set_state(2535);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2536);
					recog.expression()?;

					recog.base.set_state(2539);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==IGNORE {
						{
						recog.base.set_state(2537);
						recog.base.match_token(IGNORE,&mut recog.err_handler)?;

						recog.base.set_state(2538);
						recog.base.match_token(NULLS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2541);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				8 =>{
					{
					let mut tmp = PositionContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2543);
					recog.base.match_token(POSITION,&mut recog.err_handler)?;

					recog.base.set_state(2544);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2545);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::PositionContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.substr = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2546);
					recog.base.match_token(IN,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2547);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::PositionContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.str = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2548);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				9 =>{
					{
					let mut tmp = ConstantDefaultContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					/*InvokeRule constant*/
					recog.base.set_state(2550);
					recog.constant()?;

					}
				}
			,
				10 =>{
					{
					let mut tmp = StarContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2551);
					recog.base.match_token(ASTERISK,&mut recog.err_handler)?;

					}
				}
			,
				11 =>{
					{
					let mut tmp = StarContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					/*InvokeRule qualifiedName*/
					recog.base.set_state(2552);
					recog.qualifiedName()?;

					recog.base.set_state(2553);
					recog.base.match_token(T__3,&mut recog.err_handler)?;

					recog.base.set_state(2554);
					recog.base.match_token(ASTERISK,&mut recog.err_handler)?;

					}
				}
			,
				12 =>{
					{
					let mut tmp = RowConstructorContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2556);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule namedExpression*/
					recog.base.set_state(2557);
					recog.namedExpression()?;

					recog.base.set_state(2560); 
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					loop {
						{
						{
						recog.base.set_state(2558);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule namedExpression*/
						recog.base.set_state(2559);
						recog.namedExpression()?;

						}
						}
						recog.base.set_state(2562); 
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if !(_la==T__2) {break}
					}
					recog.base.set_state(2564);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				13 =>{
					{
					let mut tmp = SubqueryExpressionContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2566);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule query*/
					recog.base.set_state(2567);
					recog.query()?;

					recog.base.set_state(2568);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				14 =>{
					{
					let mut tmp = FunctionCallContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					/*InvokeRule functionName*/
					recog.base.set_state(2570);
					recog.functionName()?;

					recog.base.set_state(2571);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					recog.base.set_state(2583);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(330,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2573);
							recog.err_handler.sync(&mut recog.base)?;
							match  recog.interpreter.adaptive_predict(328,&mut recog.base)? {
								x if x == 1=>{
									{
									/*InvokeRule setQuantifier*/
									recog.base.set_state(2572);
									recog.setQuantifier()?;

									}
								}

								_ => {}
							}
							/*InvokeRule expression*/
							recog.base.set_state(2575);
							let tmp = recog.expression()?;
							if let PrimaryExpressionContextAll::FunctionCallContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.expression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							let temp = if let PrimaryExpressionContextAll::FunctionCallContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.expression.clone().unwrap() } else {unreachable!("cant cast");} ;
							if let PrimaryExpressionContextAll::FunctionCallContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.argument.push(temp); } else {unreachable!("cant cast");}  
							recog.base.set_state(2580);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							while _la==T__2 {
								{
								{
								recog.base.set_state(2576);
								recog.base.match_token(T__2,&mut recog.err_handler)?;

								/*InvokeRule expression*/
								recog.base.set_state(2577);
								let tmp = recog.expression()?;
								if let PrimaryExpressionContextAll::FunctionCallContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
								ctx.expression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

								let temp = if let PrimaryExpressionContextAll::FunctionCallContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
								ctx.expression.clone().unwrap() } else {unreachable!("cant cast");} ;
								if let PrimaryExpressionContextAll::FunctionCallContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
								ctx.argument.push(temp); } else {unreachable!("cant cast");}  
								}
								}
								recog.base.set_state(2582);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
							}
							}
						}

						_ => {}
					}
					recog.base.set_state(2585);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					recog.base.set_state(2592);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(331,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2586);
							recog.base.match_token(FILTER,&mut recog.err_handler)?;

							recog.base.set_state(2587);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							recog.base.set_state(2588);
							recog.base.match_token(WHERE,&mut recog.err_handler)?;

							/*InvokeRule booleanExpression*/
							recog.base.set_state(2589);
							let tmp = recog.booleanExpression_rec(0)?;
							if let PrimaryExpressionContextAll::FunctionCallContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.r#where = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							recog.base.set_state(2590);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					recog.base.set_state(2596);
					recog.err_handler.sync(&mut recog.base)?;
					match recog.interpreter.adaptive_predict(332,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2594);
							recog.base.match_token(OVER,&mut recog.err_handler)?;

							/*InvokeRule windowSpec*/
							recog.base.set_state(2595);
							recog.windowSpec()?;

							}
						}

						_ => {}
					}
					}
				}
			,
				15 =>{
					{
					let mut tmp = LambdaContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					/*InvokeRule identifier*/
					recog.base.set_state(2598);
					recog.identifier()?;

					recog.base.set_state(2599);
					recog.base.match_token(T__6,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2600);
					recog.expression()?;

					}
				}
			,
				16 =>{
					{
					let mut tmp = LambdaContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2602);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule identifier*/
					recog.base.set_state(2603);
					recog.identifier()?;

					recog.base.set_state(2606); 
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					loop {
						{
						{
						recog.base.set_state(2604);
						recog.base.match_token(T__2,&mut recog.err_handler)?;

						/*InvokeRule identifier*/
						recog.base.set_state(2605);
						recog.identifier()?;

						}
						}
						recog.base.set_state(2608); 
						recog.err_handler.sync(&mut recog.base)?;
						_la = recog.base.input.la(1);
						if !(_la==T__2) {break}
					}
					recog.base.set_state(2610);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					recog.base.set_state(2611);
					recog.base.match_token(T__6,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2612);
					recog.expression()?;

					}
				}
			,
				17 =>{
					{
					let mut tmp = ColumnReferenceContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					/*InvokeRule identifier*/
					recog.base.set_state(2614);
					recog.identifier()?;

					}
				}
			,
				18 =>{
					{
					let mut tmp = ParenthesizedExpressionContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2615);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule expression*/
					recog.base.set_state(2616);
					recog.expression()?;

					recog.base.set_state(2617);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				19 =>{
					{
					let mut tmp = ExtractContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2619);
					recog.base.match_token(EXTRACT,&mut recog.err_handler)?;

					recog.base.set_state(2620);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule identifier*/
					recog.base.set_state(2621);
					let tmp = recog.identifier()?;
					if let PrimaryExpressionContextAll::ExtractContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.field = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2622);
					recog.base.match_token(FROM,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2623);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::ExtractContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.source = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2624);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				20 =>{
					{
					let mut tmp = SubstringContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2626);
					_la = recog.base.input.la(1);
					if { !(_la==SUBSTR || _la==SUBSTRING) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(2627);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2628);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::SubstringContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.str = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2629);
					_la = recog.base.input.la(1);
					if { !(_la==T__2 || _la==FROM) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					/*InvokeRule valueExpression*/
					recog.base.set_state(2630);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::SubstringContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.pos = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2633);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==T__2 || _la==FOR {
						{
						recog.base.set_state(2631);
						_la = recog.base.input.la(1);
						if { !(_la==T__2 || _la==FOR) } {
							recog.err_handler.recover_inline(&mut recog.base)?;

						}
						else {
							if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
							recog.err_handler.report_match(&mut recog.base);
							recog.base.consume(&mut recog.err_handler);
						}
						/*InvokeRule valueExpression*/
						recog.base.set_state(2632);
						let tmp = recog.valueExpression_rec(0)?;
						if let PrimaryExpressionContextAll::SubstringContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
						ctx.len = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					recog.base.set_state(2635);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				21 =>{
					{
					let mut tmp = TrimContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2637);
					recog.base.match_token(TRIM,&mut recog.err_handler)?;

					recog.base.set_state(2638);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					recog.base.set_state(2640);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(335,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2639);
							if let PrimaryExpressionContextAll::TrimContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.trimOption = recog.base.input.lt(1).cloned(); } else {unreachable!("cant cast");} 
							_la = recog.base.input.la(1);
							if { !(_la==BOTH || _la==LEADING || _la==TRAILING) } {
								let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
								if let PrimaryExpressionContextAll::TrimContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
								ctx.trimOption = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
							else {
								if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
								recog.err_handler.report_match(&mut recog.base);
								recog.base.consume(&mut recog.err_handler);
							}
							}
						}

						_ => {}
					}
					recog.base.set_state(2643);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(336,&mut recog.base)? {
						x if x == 1=>{
							{
							/*InvokeRule valueExpression*/
							recog.base.set_state(2642);
							let tmp = recog.valueExpression_rec(0)?;
							if let PrimaryExpressionContextAll::TrimContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.trimStr = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					recog.base.set_state(2645);
					recog.base.match_token(FROM,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2646);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::TrimContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.srcStr = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2647);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				22 =>{
					{
					let mut tmp = OverlayContextExt::new(&**_localctx);
					recog.ctx = Some(tmp.clone());
					_localctx = tmp;
					_prevctx = _localctx.clone();
					recog.base.set_state(2649);
					recog.base.match_token(OVERLAY,&mut recog.err_handler)?;

					recog.base.set_state(2650);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2651);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::OverlayContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.input = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2652);
					recog.base.match_token(PLACING,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2653);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::OverlayContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.replace = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2654);
					recog.base.match_token(FROM,&mut recog.err_handler)?;

					/*InvokeRule valueExpression*/
					recog.base.set_state(2655);
					let tmp = recog.valueExpression_rec(0)?;
					if let PrimaryExpressionContextAll::OverlayContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
					ctx.position = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2658);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==FOR {
						{
						recog.base.set_state(2656);
						recog.base.match_token(FOR,&mut recog.err_handler)?;

						/*InvokeRule valueExpression*/
						recog.base.set_state(2657);
						let tmp = recog.valueExpression_rec(0)?;
						if let PrimaryExpressionContextAll::OverlayContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
						ctx.length = Some(tmp.clone()); } else {unreachable!("cant cast");}  

						}
					}

					recog.base.set_state(2660);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}

			let tmp = recog.input.lt(-1).cloned();
			recog.ctx.as_ref().unwrap().set_stop(tmp);
			recog.base.set_state(2674);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(340,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					recog.trigger_exit_rule_event();
					_prevctx = _localctx.clone();
					{
					recog.base.set_state(2672);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(339,&mut recog.base)? {
						1 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = SubscriptContextExt::new(&**PrimaryExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let PrimaryExpressionContextAll::SubscriptContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut tmp){
								ctx.value = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_primaryExpression);
							_localctx = tmp;
							recog.base.set_state(2664);
							if !({recog.precpred(None, 8)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 8)".to_owned()), None))?;
							}
							recog.base.set_state(2665);
							recog.base.match_token(T__7,&mut recog.err_handler)?;

							/*InvokeRule valueExpression*/
							recog.base.set_state(2666);
							let tmp = recog.valueExpression_rec(0)?;
							if let PrimaryExpressionContextAll::SubscriptContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.index = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							recog.base.set_state(2667);
							recog.base.match_token(T__8,&mut recog.err_handler)?;

							}
						}
					,
						2 =>{
							{
							/*recRuleLabeledAltStartAction*/
							let mut tmp = DereferenceContextExt::new(&**PrimaryExpressionContextExt::new(_parentctx.clone(), _parentState));
							if let PrimaryExpressionContextAll::DereferenceContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut tmp){
								ctx.opt_base = Some(_prevctx.clone());
							} else {unreachable!("cant cast");}
							recog.push_new_recursion_context(tmp.clone(), _startState, RULE_primaryExpression);
							_localctx = tmp;
							recog.base.set_state(2669);
							if !({recog.precpred(None, 6)}) {
								Err(FailedPredicateError::new(&mut recog.base, Some("recog.precpred(None, 6)".to_owned()), None))?;
							}
							recog.base.set_state(2670);
							recog.base.match_token(T__3,&mut recog.err_handler)?;

							/*InvokeRule identifier*/
							recog.base.set_state(2671);
							let tmp = recog.identifier()?;
							if let PrimaryExpressionContextAll::DereferenceContext(ctx) = cast_mut::<_,PrimaryExpressionContextAll >(&mut _localctx){
							ctx.fieldName = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							}
						}

						_ => {}
					}
					} 
				}
				recog.base.set_state(2676);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(340,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_) => {},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re)=>{
			//_localctx.exception = re;
			recog.err_handler.report_error(&mut recog.base, re);
	        recog.err_handler.recover(&mut recog.base, re)?;}
		}
		recog.base.unroll_recursion_context(_parentctx);

		Ok(_localctx)
	}
}
//------------------- constant ----------------
#[derive(Debug)]
pub enum ConstantContextAll<'input>{
	NullLiteralContext(NullLiteralContext<'input>),
	StringLiteralContext(StringLiteralContext<'input>),
	TypeConstructorContext(TypeConstructorContext<'input>),
	IntervalLiteralContext(IntervalLiteralContext<'input>),
	NumericLiteralContext(NumericLiteralContext<'input>),
	BooleanLiteralContext(BooleanLiteralContext<'input>),
Error(ConstantContext<'input>)
}
crate::tid!{ConstantContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for ConstantContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for ConstantContextAll<'input>{}

impl<'input> Deref for ConstantContextAll<'input>{
	type Target = dyn ConstantContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use ConstantContextAll::*;
		match self{
			NullLiteralContext(inner) => inner,
			StringLiteralContext(inner) => inner,
			TypeConstructorContext(inner) => inner,
			IntervalLiteralContext(inner) => inner,
			NumericLiteralContext(inner) => inner,
			BooleanLiteralContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ConstantContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type ConstantContext<'input> = BaseParserRuleContext<'input,ConstantContextExt<'input>>;

#[derive(Clone)]
pub struct ConstantContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ConstantContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ConstantContext<'input>{
}

impl<'input> CustomRuleContext<'input> for ConstantContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constant }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constant }
}
crate::tid!{ConstantContextExt<'a>}

impl<'input> ConstantContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ConstantContextAll<'input>> {
		Rc::new(
		ConstantContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ConstantContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait ConstantContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ConstantContextExt<'input>>{


}

impl<'input> ConstantContextAttrs<'input> for ConstantContext<'input>{}

pub type NullLiteralContext<'input> = BaseParserRuleContext<'input,NullLiteralContextExt<'input>>;

pub trait NullLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token NULL
	/// Returns `None` if there is no child corresponding to token NULL
	fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NULL, 0)
	}
}

impl<'input> NullLiteralContextAttrs<'input> for NullLiteralContext<'input>{}

pub struct NullLiteralContextExt<'input>{
	base:ConstantContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{NullLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for NullLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NullLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_nullLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for NullLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constant }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constant }
}

impl<'input> Borrow<ConstantContextExt<'input>> for NullLiteralContext<'input>{
	fn borrow(&self) -> &ConstantContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ConstantContextExt<'input>> for NullLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut ConstantContextExt<'input> { &mut self.base }
}

impl<'input> ConstantContextAttrs<'input> for NullLiteralContext<'input> {}

impl<'input> NullLiteralContextExt<'input>{
	fn new(ctx: &dyn ConstantContextAttrs<'input>) -> Rc<ConstantContextAll<'input>>  {
		Rc::new(
			ConstantContextAll::NullLiteralContext(
				BaseParserRuleContext::copy_from(ctx,NullLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type StringLiteralContext<'input> = BaseParserRuleContext<'input,StringLiteralContextExt<'input>>;

pub trait StringLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves all `TerminalNode`s corresponding to token STRING in current rule
	fn STRING_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token STRING, starting from 0.
	/// Returns `None` if number of children corresponding to token STRING is less or equal than `i`.
	fn STRING(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, i)
	}
}

impl<'input> StringLiteralContextAttrs<'input> for StringLiteralContext<'input>{}

pub struct StringLiteralContextExt<'input>{
	base:ConstantContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{StringLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for StringLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StringLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_stringLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for StringLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constant }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constant }
}

impl<'input> Borrow<ConstantContextExt<'input>> for StringLiteralContext<'input>{
	fn borrow(&self) -> &ConstantContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ConstantContextExt<'input>> for StringLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut ConstantContextExt<'input> { &mut self.base }
}

impl<'input> ConstantContextAttrs<'input> for StringLiteralContext<'input> {}

impl<'input> StringLiteralContextExt<'input>{
	fn new(ctx: &dyn ConstantContextAttrs<'input>) -> Rc<ConstantContextAll<'input>>  {
		Rc::new(
			ConstantContextAll::StringLiteralContext(
				BaseParserRuleContext::copy_from(ctx,StringLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type TypeConstructorContext<'input> = BaseParserRuleContext<'input,TypeConstructorContextExt<'input>>;

pub trait TypeConstructorContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves first TerminalNode corresponding to token STRING
	/// Returns `None` if there is no child corresponding to token STRING
	fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRING, 0)
	}
}

impl<'input> TypeConstructorContextAttrs<'input> for TypeConstructorContext<'input>{}

pub struct TypeConstructorContextExt<'input>{
	base:ConstantContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{TypeConstructorContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TypeConstructorContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TypeConstructorContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_typeConstructor(self);
	}
}

impl<'input> CustomRuleContext<'input> for TypeConstructorContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constant }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constant }
}

impl<'input> Borrow<ConstantContextExt<'input>> for TypeConstructorContext<'input>{
	fn borrow(&self) -> &ConstantContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ConstantContextExt<'input>> for TypeConstructorContext<'input>{
	fn borrow_mut(&mut self) -> &mut ConstantContextExt<'input> { &mut self.base }
}

impl<'input> ConstantContextAttrs<'input> for TypeConstructorContext<'input> {}

impl<'input> TypeConstructorContextExt<'input>{
	fn new(ctx: &dyn ConstantContextAttrs<'input>) -> Rc<ConstantContextAll<'input>>  {
		Rc::new(
			ConstantContextAll::TypeConstructorContext(
				BaseParserRuleContext::copy_from(ctx,TypeConstructorContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type IntervalLiteralContext<'input> = BaseParserRuleContext<'input,IntervalLiteralContextExt<'input>>;

pub trait IntervalLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn interval(&self) -> Option<Rc<IntervalContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> IntervalLiteralContextAttrs<'input> for IntervalLiteralContext<'input>{}

pub struct IntervalLiteralContextExt<'input>{
	base:ConstantContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{IntervalLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for IntervalLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IntervalLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_intervalLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for IntervalLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constant }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constant }
}

impl<'input> Borrow<ConstantContextExt<'input>> for IntervalLiteralContext<'input>{
	fn borrow(&self) -> &ConstantContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ConstantContextExt<'input>> for IntervalLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut ConstantContextExt<'input> { &mut self.base }
}

impl<'input> ConstantContextAttrs<'input> for IntervalLiteralContext<'input> {}

impl<'input> IntervalLiteralContextExt<'input>{
	fn new(ctx: &dyn ConstantContextAttrs<'input>) -> Rc<ConstantContextAll<'input>>  {
		Rc::new(
			ConstantContextAll::IntervalLiteralContext(
				BaseParserRuleContext::copy_from(ctx,IntervalLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type NumericLiteralContext<'input> = BaseParserRuleContext<'input,NumericLiteralContextExt<'input>>;

pub trait NumericLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn number(&self) -> Option<Rc<NumberContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> NumericLiteralContextAttrs<'input> for NumericLiteralContext<'input>{}

pub struct NumericLiteralContextExt<'input>{
	base:ConstantContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{NumericLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for NumericLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NumericLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_numericLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for NumericLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constant }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constant }
}

impl<'input> Borrow<ConstantContextExt<'input>> for NumericLiteralContext<'input>{
	fn borrow(&self) -> &ConstantContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ConstantContextExt<'input>> for NumericLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut ConstantContextExt<'input> { &mut self.base }
}

impl<'input> ConstantContextAttrs<'input> for NumericLiteralContext<'input> {}

impl<'input> NumericLiteralContextExt<'input>{
	fn new(ctx: &dyn ConstantContextAttrs<'input>) -> Rc<ConstantContextAll<'input>>  {
		Rc::new(
			ConstantContextAll::NumericLiteralContext(
				BaseParserRuleContext::copy_from(ctx,NumericLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type BooleanLiteralContext<'input> = BaseParserRuleContext<'input,BooleanLiteralContextExt<'input>>;

pub trait BooleanLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn booleanValue(&self) -> Option<Rc<BooleanValueContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> BooleanLiteralContextAttrs<'input> for BooleanLiteralContext<'input>{}

pub struct BooleanLiteralContextExt<'input>{
	base:ConstantContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{BooleanLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for BooleanLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for BooleanLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_booleanLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for BooleanLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_constant }
	//fn type_rule_index() -> usize where Self: Sized { RULE_constant }
}

impl<'input> Borrow<ConstantContextExt<'input>> for BooleanLiteralContext<'input>{
	fn borrow(&self) -> &ConstantContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ConstantContextExt<'input>> for BooleanLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut ConstantContextExt<'input> { &mut self.base }
}

impl<'input> ConstantContextAttrs<'input> for BooleanLiteralContext<'input> {}

impl<'input> BooleanLiteralContextExt<'input>{
	fn new(ctx: &dyn ConstantContextAttrs<'input>) -> Rc<ConstantContextAll<'input>>  {
		Rc::new(
			ConstantContextAll::BooleanLiteralContext(
				BaseParserRuleContext::copy_from(ctx,BooleanLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn constant(&mut self,)
	-> Result<Rc<ConstantContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ConstantContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 198, RULE_constant);
        let mut _localctx: Rc<ConstantContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			recog.base.set_state(2689);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(342,&mut recog.base)? {
				1 =>{
					let tmp = NullLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(2677);
					recog.base.match_token(NULL,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					let tmp = IntervalLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule interval*/
					recog.base.set_state(2678);
					recog.interval()?;

					}
				}
			,
				3 =>{
					let tmp = TypeConstructorContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					/*InvokeRule identifier*/
					recog.base.set_state(2679);
					recog.identifier()?;

					recog.base.set_state(2680);
					recog.base.match_token(STRING,&mut recog.err_handler)?;

					}
				}
			,
				4 =>{
					let tmp = NumericLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					/*InvokeRule number*/
					recog.base.set_state(2682);
					recog.number()?;

					}
				}
			,
				5 =>{
					let tmp = BooleanLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 5);
					_localctx = tmp;
					{
					/*InvokeRule booleanValue*/
					recog.base.set_state(2683);
					recog.booleanValue()?;

					}
				}
			,
				6 =>{
					let tmp = StringLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 6);
					_localctx = tmp;
					{
					recog.base.set_state(2685); 
					recog.err_handler.sync(&mut recog.base)?;
					_alt = 1;
					loop {
						match _alt {
						    x if x == 1=>
							{
							{
							recog.base.set_state(2684);
							recog.base.match_token(STRING,&mut recog.err_handler)?;

							}
							}

						_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
						}
						recog.base.set_state(2687); 
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(341,&mut recog.base)?;
						if _alt==2 || _alt==INVALID_ALT { break }
					}
					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- comparisonOperator ----------------
pub type ComparisonOperatorContextAll<'input> = ComparisonOperatorContext<'input>;


pub type ComparisonOperatorContext<'input> = BaseParserRuleContext<'input,ComparisonOperatorContextExt<'input>>;

#[derive(Clone)]
pub struct ComparisonOperatorContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ComparisonOperatorContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ComparisonOperatorContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_comparisonOperator(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_comparisonOperator(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ComparisonOperatorContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_comparisonOperator }
	//fn type_rule_index() -> usize where Self: Sized { RULE_comparisonOperator }
}
crate::tid!{ComparisonOperatorContextExt<'a>}

impl<'input> ComparisonOperatorContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ComparisonOperatorContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ComparisonOperatorContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ComparisonOperatorContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ComparisonOperatorContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token EQ
/// Returns `None` if there is no child corresponding to token EQ
fn EQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EQ, 0)
}
/// Retrieves first TerminalNode corresponding to token NEQ
/// Returns `None` if there is no child corresponding to token NEQ
fn NEQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NEQ, 0)
}
/// Retrieves first TerminalNode corresponding to token NEQJ
/// Returns `None` if there is no child corresponding to token NEQJ
fn NEQJ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NEQJ, 0)
}
/// Retrieves first TerminalNode corresponding to token LT
/// Returns `None` if there is no child corresponding to token LT
fn LT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LT, 0)
}
/// Retrieves first TerminalNode corresponding to token LTE
/// Returns `None` if there is no child corresponding to token LTE
fn LTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LTE, 0)
}
/// Retrieves first TerminalNode corresponding to token GT
/// Returns `None` if there is no child corresponding to token GT
fn GT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GT, 0)
}
/// Retrieves first TerminalNode corresponding to token GTE
/// Returns `None` if there is no child corresponding to token GTE
fn GTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GTE, 0)
}
/// Retrieves first TerminalNode corresponding to token NSEQ
/// Returns `None` if there is no child corresponding to token NSEQ
fn NSEQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NSEQ, 0)
}

}

impl<'input> ComparisonOperatorContextAttrs<'input> for ComparisonOperatorContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn comparisonOperator(&mut self,)
	-> Result<Rc<ComparisonOperatorContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ComparisonOperatorContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 200, RULE_comparisonOperator);
        let mut _localctx: Rc<ComparisonOperatorContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2691);
			_la = recog.base.input.la(1);
			if { !(((((_la - 260)) & !0x3f) == 0 && ((1usize << (_la - 260)) & ((1usize << (EQ - 260)) | (1usize << (NSEQ - 260)) | (1usize << (NEQ - 260)) | (1usize << (NEQJ - 260)) | (1usize << (LT - 260)) | (1usize << (LTE - 260)) | (1usize << (GT - 260)) | (1usize << (GTE - 260)))) != 0)) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- arithmeticOperator ----------------
pub type ArithmeticOperatorContextAll<'input> = ArithmeticOperatorContext<'input>;


pub type ArithmeticOperatorContext<'input> = BaseParserRuleContext<'input,ArithmeticOperatorContextExt<'input>>;

#[derive(Clone)]
pub struct ArithmeticOperatorContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ArithmeticOperatorContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ArithmeticOperatorContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_arithmeticOperator(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_arithmeticOperator(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ArithmeticOperatorContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_arithmeticOperator }
	//fn type_rule_index() -> usize where Self: Sized { RULE_arithmeticOperator }
}
crate::tid!{ArithmeticOperatorContextExt<'a>}

impl<'input> ArithmeticOperatorContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ArithmeticOperatorContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ArithmeticOperatorContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ArithmeticOperatorContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ArithmeticOperatorContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token PLUS
/// Returns `None` if there is no child corresponding to token PLUS
fn PLUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PLUS, 0)
}
/// Retrieves first TerminalNode corresponding to token MINUS
/// Returns `None` if there is no child corresponding to token MINUS
fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MINUS, 0)
}
/// Retrieves first TerminalNode corresponding to token ASTERISK
/// Returns `None` if there is no child corresponding to token ASTERISK
fn ASTERISK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ASTERISK, 0)
}
/// Retrieves first TerminalNode corresponding to token SLASH
/// Returns `None` if there is no child corresponding to token SLASH
fn SLASH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SLASH, 0)
}
/// Retrieves first TerminalNode corresponding to token PERCENT
/// Returns `None` if there is no child corresponding to token PERCENT
fn PERCENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PERCENT, 0)
}
/// Retrieves first TerminalNode corresponding to token DIV
/// Returns `None` if there is no child corresponding to token DIV
fn DIV(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIV, 0)
}
/// Retrieves first TerminalNode corresponding to token TILDE
/// Returns `None` if there is no child corresponding to token TILDE
fn TILDE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TILDE, 0)
}
/// Retrieves first TerminalNode corresponding to token AMPERSAND
/// Returns `None` if there is no child corresponding to token AMPERSAND
fn AMPERSAND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AMPERSAND, 0)
}
/// Retrieves first TerminalNode corresponding to token PIPE
/// Returns `None` if there is no child corresponding to token PIPE
fn PIPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PIPE, 0)
}
/// Retrieves first TerminalNode corresponding to token CONCAT_PIPE
/// Returns `None` if there is no child corresponding to token CONCAT_PIPE
fn CONCAT_PIPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CONCAT_PIPE, 0)
}
/// Retrieves first TerminalNode corresponding to token HAT
/// Returns `None` if there is no child corresponding to token HAT
fn HAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(HAT, 0)
}

}

impl<'input> ArithmeticOperatorContextAttrs<'input> for ArithmeticOperatorContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn arithmeticOperator(&mut self,)
	-> Result<Rc<ArithmeticOperatorContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ArithmeticOperatorContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 202, RULE_arithmeticOperator);
        let mut _localctx: Rc<ArithmeticOperatorContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2693);
			_la = recog.base.input.la(1);
			if { !(_la==DIV || ((((_la - 268)) & !0x3f) == 0 && ((1usize << (_la - 268)) & ((1usize << (PLUS - 268)) | (1usize << (MINUS - 268)) | (1usize << (ASTERISK - 268)) | (1usize << (SLASH - 268)) | (1usize << (PERCENT - 268)) | (1usize << (TILDE - 268)) | (1usize << (AMPERSAND - 268)) | (1usize << (PIPE - 268)) | (1usize << (CONCAT_PIPE - 268)) | (1usize << (HAT - 268)))) != 0)) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- predicateOperator ----------------
pub type PredicateOperatorContextAll<'input> = PredicateOperatorContext<'input>;


pub type PredicateOperatorContext<'input> = BaseParserRuleContext<'input,PredicateOperatorContextExt<'input>>;

#[derive(Clone)]
pub struct PredicateOperatorContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for PredicateOperatorContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PredicateOperatorContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_predicateOperator(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_predicateOperator(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for PredicateOperatorContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_predicateOperator }
	//fn type_rule_index() -> usize where Self: Sized { RULE_predicateOperator }
}
crate::tid!{PredicateOperatorContextExt<'a>}

impl<'input> PredicateOperatorContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<PredicateOperatorContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,PredicateOperatorContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait PredicateOperatorContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<PredicateOperatorContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token OR
/// Returns `None` if there is no child corresponding to token OR
fn OR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OR, 0)
}
/// Retrieves first TerminalNode corresponding to token AND
/// Returns `None` if there is no child corresponding to token AND
fn AND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AND, 0)
}
/// Retrieves first TerminalNode corresponding to token IN
/// Returns `None` if there is no child corresponding to token IN
fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IN, 0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}

}

impl<'input> PredicateOperatorContextAttrs<'input> for PredicateOperatorContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn predicateOperator(&mut self,)
	-> Result<Rc<PredicateOperatorContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = PredicateOperatorContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 204, RULE_predicateOperator);
        let mut _localctx: Rc<PredicateOperatorContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2695);
			_la = recog.base.input.la(1);
			if { !(_la==AND || _la==IN || _la==NOT || _la==OR) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- booleanValue ----------------
pub type BooleanValueContextAll<'input> = BooleanValueContext<'input>;


pub type BooleanValueContext<'input> = BaseParserRuleContext<'input,BooleanValueContextExt<'input>>;

#[derive(Clone)]
pub struct BooleanValueContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for BooleanValueContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for BooleanValueContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_booleanValue(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_booleanValue(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for BooleanValueContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_booleanValue }
	//fn type_rule_index() -> usize where Self: Sized { RULE_booleanValue }
}
crate::tid!{BooleanValueContextExt<'a>}

impl<'input> BooleanValueContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<BooleanValueContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,BooleanValueContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait BooleanValueContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<BooleanValueContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token TRUE
/// Returns `None` if there is no child corresponding to token TRUE
fn TRUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRUE, 0)
}
/// Retrieves first TerminalNode corresponding to token FALSE
/// Returns `None` if there is no child corresponding to token FALSE
fn FALSE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FALSE, 0)
}

}

impl<'input> BooleanValueContextAttrs<'input> for BooleanValueContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn booleanValue(&mut self,)
	-> Result<Rc<BooleanValueContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = BooleanValueContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 206, RULE_booleanValue);
        let mut _localctx: Rc<BooleanValueContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2697);
			_la = recog.base.input.la(1);
			if { !(_la==FALSE || _la==TRUE) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- interval ----------------
pub type IntervalContextAll<'input> = IntervalContext<'input>;


pub type IntervalContext<'input> = BaseParserRuleContext<'input,IntervalContextExt<'input>>;

#[derive(Clone)]
pub struct IntervalContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for IntervalContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IntervalContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_interval(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_interval(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for IntervalContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_interval }
	//fn type_rule_index() -> usize where Self: Sized { RULE_interval }
}
crate::tid!{IntervalContextExt<'a>}

impl<'input> IntervalContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<IntervalContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,IntervalContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait IntervalContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<IntervalContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token INTERVAL
/// Returns `None` if there is no child corresponding to token INTERVAL
fn INTERVAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTERVAL, 0)
}
fn errorCapturingMultiUnitsInterval(&self) -> Option<Rc<ErrorCapturingMultiUnitsIntervalContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn errorCapturingUnitToUnitInterval(&self) -> Option<Rc<ErrorCapturingUnitToUnitIntervalContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> IntervalContextAttrs<'input> for IntervalContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn interval(&mut self,)
	-> Result<Rc<IntervalContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = IntervalContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 208, RULE_interval);
        let mut _localctx: Rc<IntervalContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2699);
			recog.base.match_token(INTERVAL,&mut recog.err_handler)?;

			recog.base.set_state(2702);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(343,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule errorCapturingMultiUnitsInterval*/
					recog.base.set_state(2700);
					recog.errorCapturingMultiUnitsInterval()?;

					}
				}

				x if x == 2=>{
					{
					/*InvokeRule errorCapturingUnitToUnitInterval*/
					recog.base.set_state(2701);
					recog.errorCapturingUnitToUnitInterval()?;

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- errorCapturingMultiUnitsInterval ----------------
pub type ErrorCapturingMultiUnitsIntervalContextAll<'input> = ErrorCapturingMultiUnitsIntervalContext<'input>;


pub type ErrorCapturingMultiUnitsIntervalContext<'input> = BaseParserRuleContext<'input,ErrorCapturingMultiUnitsIntervalContextExt<'input>>;

#[derive(Clone)]
pub struct ErrorCapturingMultiUnitsIntervalContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ErrorCapturingMultiUnitsIntervalContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ErrorCapturingMultiUnitsIntervalContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_errorCapturingMultiUnitsInterval(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_errorCapturingMultiUnitsInterval(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ErrorCapturingMultiUnitsIntervalContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_errorCapturingMultiUnitsInterval }
	//fn type_rule_index() -> usize where Self: Sized { RULE_errorCapturingMultiUnitsInterval }
}
crate::tid!{ErrorCapturingMultiUnitsIntervalContextExt<'a>}

impl<'input> ErrorCapturingMultiUnitsIntervalContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ErrorCapturingMultiUnitsIntervalContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ErrorCapturingMultiUnitsIntervalContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ErrorCapturingMultiUnitsIntervalContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ErrorCapturingMultiUnitsIntervalContextExt<'input>>{

fn multiUnitsInterval(&self) -> Option<Rc<MultiUnitsIntervalContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn unitToUnitInterval(&self) -> Option<Rc<UnitToUnitIntervalContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ErrorCapturingMultiUnitsIntervalContextAttrs<'input> for ErrorCapturingMultiUnitsIntervalContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn errorCapturingMultiUnitsInterval(&mut self,)
	-> Result<Rc<ErrorCapturingMultiUnitsIntervalContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ErrorCapturingMultiUnitsIntervalContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 210, RULE_errorCapturingMultiUnitsInterval);
        let mut _localctx: Rc<ErrorCapturingMultiUnitsIntervalContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule multiUnitsInterval*/
			recog.base.set_state(2704);
			recog.multiUnitsInterval()?;

			recog.base.set_state(2706);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(344,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule unitToUnitInterval*/
					recog.base.set_state(2705);
					recog.unitToUnitInterval()?;

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- multiUnitsInterval ----------------
pub type MultiUnitsIntervalContextAll<'input> = MultiUnitsIntervalContext<'input>;


pub type MultiUnitsIntervalContext<'input> = BaseParserRuleContext<'input,MultiUnitsIntervalContextExt<'input>>;

#[derive(Clone)]
pub struct MultiUnitsIntervalContextExt<'input>{
	pub identifier: Option<Rc<IdentifierContextAll<'input>>>,
	pub unit:Vec<Rc<IdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for MultiUnitsIntervalContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for MultiUnitsIntervalContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_multiUnitsInterval(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_multiUnitsInterval(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for MultiUnitsIntervalContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_multiUnitsInterval }
	//fn type_rule_index() -> usize where Self: Sized { RULE_multiUnitsInterval }
}
crate::tid!{MultiUnitsIntervalContextExt<'a>}

impl<'input> MultiUnitsIntervalContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<MultiUnitsIntervalContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,MultiUnitsIntervalContextExt{
				identifier: None, 
				unit: Vec::new(), 
				ph:PhantomData
			}),
		)
	}
}

pub trait MultiUnitsIntervalContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<MultiUnitsIntervalContextExt<'input>>{

fn intervalValue_all(&self) ->  Vec<Rc<IntervalValueContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn intervalValue(&self, i: usize) -> Option<Rc<IntervalValueContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> MultiUnitsIntervalContextAttrs<'input> for MultiUnitsIntervalContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn multiUnitsInterval(&mut self,)
	-> Result<Rc<MultiUnitsIntervalContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = MultiUnitsIntervalContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 212, RULE_multiUnitsInterval);
        let mut _localctx: Rc<MultiUnitsIntervalContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2711); 
			recog.err_handler.sync(&mut recog.base)?;
			_alt = 1;
			loop {
				match _alt {
				    x if x == 1=>
					{
					{
					/*InvokeRule intervalValue*/
					recog.base.set_state(2708);
					recog.intervalValue()?;

					/*InvokeRule identifier*/
					recog.base.set_state(2709);
					let tmp = recog.identifier()?;
					 cast_mut::<_,MultiUnitsIntervalContext >(&mut _localctx).identifier = Some(tmp.clone());
					  

					let temp =  cast_mut::<_,MultiUnitsIntervalContext >(&mut _localctx).identifier.clone().unwrap()
					 ;
					 cast_mut::<_,MultiUnitsIntervalContext >(&mut _localctx).unit.push(temp);
					  
					}
					}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
				}
				recog.base.set_state(2713); 
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(345,&mut recog.base)?;
				if _alt==2 || _alt==INVALID_ALT { break }
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- errorCapturingUnitToUnitInterval ----------------
pub type ErrorCapturingUnitToUnitIntervalContextAll<'input> = ErrorCapturingUnitToUnitIntervalContext<'input>;


pub type ErrorCapturingUnitToUnitIntervalContext<'input> = BaseParserRuleContext<'input,ErrorCapturingUnitToUnitIntervalContextExt<'input>>;

#[derive(Clone)]
pub struct ErrorCapturingUnitToUnitIntervalContextExt<'input>{
	pub body: Option<Rc<UnitToUnitIntervalContextAll<'input>>>,
	pub error1: Option<Rc<MultiUnitsIntervalContextAll<'input>>>,
	pub error2: Option<Rc<UnitToUnitIntervalContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ErrorCapturingUnitToUnitIntervalContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ErrorCapturingUnitToUnitIntervalContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_errorCapturingUnitToUnitInterval(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_errorCapturingUnitToUnitInterval(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ErrorCapturingUnitToUnitIntervalContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_errorCapturingUnitToUnitInterval }
	//fn type_rule_index() -> usize where Self: Sized { RULE_errorCapturingUnitToUnitInterval }
}
crate::tid!{ErrorCapturingUnitToUnitIntervalContextExt<'a>}

impl<'input> ErrorCapturingUnitToUnitIntervalContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ErrorCapturingUnitToUnitIntervalContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ErrorCapturingUnitToUnitIntervalContextExt{
				body: None, error1: None, error2: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait ErrorCapturingUnitToUnitIntervalContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ErrorCapturingUnitToUnitIntervalContextExt<'input>>{

fn unitToUnitInterval_all(&self) ->  Vec<Rc<UnitToUnitIntervalContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn unitToUnitInterval(&self, i: usize) -> Option<Rc<UnitToUnitIntervalContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
fn multiUnitsInterval(&self) -> Option<Rc<MultiUnitsIntervalContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ErrorCapturingUnitToUnitIntervalContextAttrs<'input> for ErrorCapturingUnitToUnitIntervalContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn errorCapturingUnitToUnitInterval(&mut self,)
	-> Result<Rc<ErrorCapturingUnitToUnitIntervalContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ErrorCapturingUnitToUnitIntervalContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 214, RULE_errorCapturingUnitToUnitInterval);
        let mut _localctx: Rc<ErrorCapturingUnitToUnitIntervalContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule unitToUnitInterval*/
			recog.base.set_state(2715);
			let tmp = recog.unitToUnitInterval()?;
			 cast_mut::<_,ErrorCapturingUnitToUnitIntervalContext >(&mut _localctx).body = Some(tmp.clone());
			  

			recog.base.set_state(2718);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(346,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule multiUnitsInterval*/
					recog.base.set_state(2716);
					let tmp = recog.multiUnitsInterval()?;
					 cast_mut::<_,ErrorCapturingUnitToUnitIntervalContext >(&mut _localctx).error1 = Some(tmp.clone());
					  

					}
				}

				x if x == 2=>{
					{
					/*InvokeRule unitToUnitInterval*/
					recog.base.set_state(2717);
					let tmp = recog.unitToUnitInterval()?;
					 cast_mut::<_,ErrorCapturingUnitToUnitIntervalContext >(&mut _localctx).error2 = Some(tmp.clone());
					  

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- unitToUnitInterval ----------------
pub type UnitToUnitIntervalContextAll<'input> = UnitToUnitIntervalContext<'input>;


pub type UnitToUnitIntervalContext<'input> = BaseParserRuleContext<'input,UnitToUnitIntervalContextExt<'input>>;

#[derive(Clone)]
pub struct UnitToUnitIntervalContextExt<'input>{
	pub value: Option<Rc<IntervalValueContextAll<'input>>>,
	pub from: Option<Rc<IdentifierContextAll<'input>>>,
	pub to: Option<Rc<IdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for UnitToUnitIntervalContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for UnitToUnitIntervalContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_unitToUnitInterval(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_unitToUnitInterval(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for UnitToUnitIntervalContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_unitToUnitInterval }
	//fn type_rule_index() -> usize where Self: Sized { RULE_unitToUnitInterval }
}
crate::tid!{UnitToUnitIntervalContextExt<'a>}

impl<'input> UnitToUnitIntervalContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<UnitToUnitIntervalContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,UnitToUnitIntervalContextExt{
				value: None, from: None, to: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait UnitToUnitIntervalContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<UnitToUnitIntervalContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token TO
/// Returns `None` if there is no child corresponding to token TO
fn TO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TO, 0)
}
fn intervalValue(&self) -> Option<Rc<IntervalValueContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> UnitToUnitIntervalContextAttrs<'input> for UnitToUnitIntervalContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn unitToUnitInterval(&mut self,)
	-> Result<Rc<UnitToUnitIntervalContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = UnitToUnitIntervalContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 216, RULE_unitToUnitInterval);
        let mut _localctx: Rc<UnitToUnitIntervalContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule intervalValue*/
			recog.base.set_state(2720);
			let tmp = recog.intervalValue()?;
			 cast_mut::<_,UnitToUnitIntervalContext >(&mut _localctx).value = Some(tmp.clone());
			  

			/*InvokeRule identifier*/
			recog.base.set_state(2721);
			let tmp = recog.identifier()?;
			 cast_mut::<_,UnitToUnitIntervalContext >(&mut _localctx).from = Some(tmp.clone());
			  

			recog.base.set_state(2722);
			recog.base.match_token(TO,&mut recog.err_handler)?;

			/*InvokeRule identifier*/
			recog.base.set_state(2723);
			let tmp = recog.identifier()?;
			 cast_mut::<_,UnitToUnitIntervalContext >(&mut _localctx).to = Some(tmp.clone());
			  

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- intervalValue ----------------
pub type IntervalValueContextAll<'input> = IntervalValueContext<'input>;


pub type IntervalValueContext<'input> = BaseParserRuleContext<'input,IntervalValueContextExt<'input>>;

#[derive(Clone)]
pub struct IntervalValueContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for IntervalValueContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IntervalValueContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_intervalValue(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_intervalValue(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for IntervalValueContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_intervalValue }
	//fn type_rule_index() -> usize where Self: Sized { RULE_intervalValue }
}
crate::tid!{IntervalValueContextExt<'a>}

impl<'input> IntervalValueContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<IntervalValueContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,IntervalValueContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait IntervalValueContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<IntervalValueContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token INTEGER_VALUE
/// Returns `None` if there is no child corresponding to token INTEGER_VALUE
fn INTEGER_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTEGER_VALUE, 0)
}
/// Retrieves first TerminalNode corresponding to token DECIMAL_VALUE
/// Returns `None` if there is no child corresponding to token DECIMAL_VALUE
fn DECIMAL_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DECIMAL_VALUE, 0)
}
/// Retrieves first TerminalNode corresponding to token PLUS
/// Returns `None` if there is no child corresponding to token PLUS
fn PLUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PLUS, 0)
}
/// Retrieves first TerminalNode corresponding to token MINUS
/// Returns `None` if there is no child corresponding to token MINUS
fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MINUS, 0)
}
/// Retrieves first TerminalNode corresponding to token STRING
/// Returns `None` if there is no child corresponding to token STRING
fn STRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRING, 0)
}

}

impl<'input> IntervalValueContextAttrs<'input> for IntervalValueContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn intervalValue(&mut self,)
	-> Result<Rc<IntervalValueContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = IntervalValueContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 218, RULE_intervalValue);
        let mut _localctx: Rc<IntervalValueContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2730);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 PLUS | MINUS | INTEGER_VALUE | DECIMAL_VALUE 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(2726);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==PLUS || _la==MINUS {
						{
						recog.base.set_state(2725);
						_la = recog.base.input.la(1);
						if { !(_la==PLUS || _la==MINUS) } {
							recog.err_handler.recover_inline(&mut recog.base)?;

						}
						else {
							if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
							recog.err_handler.report_match(&mut recog.base);
							recog.base.consume(&mut recog.err_handler);
						}
						}
					}

					recog.base.set_state(2728);
					_la = recog.base.input.la(1);
					if { !(_la==INTEGER_VALUE || _la==DECIMAL_VALUE) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}

			 STRING 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2729);
					recog.base.match_token(STRING,&mut recog.err_handler)?;

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- colPosition ----------------
pub type ColPositionContextAll<'input> = ColPositionContext<'input>;


pub type ColPositionContext<'input> = BaseParserRuleContext<'input,ColPositionContextExt<'input>>;

#[derive(Clone)]
pub struct ColPositionContextExt<'input>{
	pub position: Option<TokenType<'input>>,
	pub afterCol: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ColPositionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ColPositionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_colPosition(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_colPosition(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ColPositionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_colPosition }
	//fn type_rule_index() -> usize where Self: Sized { RULE_colPosition }
}
crate::tid!{ColPositionContextExt<'a>}

impl<'input> ColPositionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ColPositionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ColPositionContextExt{
				position: None, 
				afterCol: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait ColPositionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ColPositionContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token FIRST
/// Returns `None` if there is no child corresponding to token FIRST
fn FIRST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FIRST, 0)
}
/// Retrieves first TerminalNode corresponding to token AFTER
/// Returns `None` if there is no child corresponding to token AFTER
fn AFTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AFTER, 0)
}
fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ColPositionContextAttrs<'input> for ColPositionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn colPosition(&mut self,)
	-> Result<Rc<ColPositionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ColPositionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 220, RULE_colPosition);
        let mut _localctx: Rc<ColPositionContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2735);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 FIRST 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(2732);
					let tmp = recog.base.match_token(FIRST,&mut recog.err_handler)?;
					 cast_mut::<_,ColPositionContext >(&mut _localctx).position = Some(tmp.clone());
					  

					}
				}

			 AFTER 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2733);
					let tmp = recog.base.match_token(AFTER,&mut recog.err_handler)?;
					 cast_mut::<_,ColPositionContext >(&mut _localctx).position = Some(tmp.clone());
					  

					/*InvokeRule errorCapturingIdentifier*/
					recog.base.set_state(2734);
					let tmp = recog.errorCapturingIdentifier()?;
					 cast_mut::<_,ColPositionContext >(&mut _localctx).afterCol = Some(tmp.clone());
					  

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- dataType ----------------
#[derive(Debug)]
pub enum DataTypeContextAll<'input>{
	ComplexDataTypeContext(ComplexDataTypeContext<'input>),
	PrimitiveDataTypeContext(PrimitiveDataTypeContext<'input>),
Error(DataTypeContext<'input>)
}
crate::tid!{DataTypeContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for DataTypeContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for DataTypeContextAll<'input>{}

impl<'input> Deref for DataTypeContextAll<'input>{
	type Target = dyn DataTypeContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use DataTypeContextAll::*;
		match self{
			ComplexDataTypeContext(inner) => inner,
			PrimitiveDataTypeContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DataTypeContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type DataTypeContext<'input> = BaseParserRuleContext<'input,DataTypeContextExt<'input>>;

#[derive(Clone)]
pub struct DataTypeContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for DataTypeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DataTypeContext<'input>{
}

impl<'input> CustomRuleContext<'input> for DataTypeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dataType }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dataType }
}
crate::tid!{DataTypeContextExt<'a>}

impl<'input> DataTypeContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<DataTypeContextAll<'input>> {
		Rc::new(
		DataTypeContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,DataTypeContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait DataTypeContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<DataTypeContextExt<'input>>{


}

impl<'input> DataTypeContextAttrs<'input> for DataTypeContext<'input>{}

pub type ComplexDataTypeContext<'input> = BaseParserRuleContext<'input,ComplexDataTypeContextExt<'input>>;

pub trait ComplexDataTypeContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token LT
	/// Returns `None` if there is no child corresponding to token LT
	fn LT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(LT, 0)
	}
	fn dataType_all(&self) ->  Vec<Rc<DataTypeContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn dataType(&self, i: usize) -> Option<Rc<DataTypeContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token GT
	/// Returns `None` if there is no child corresponding to token GT
	fn GT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(GT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ARRAY
	/// Returns `None` if there is no child corresponding to token ARRAY
	fn ARRAY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ARRAY, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MAP
	/// Returns `None` if there is no child corresponding to token MAP
	fn MAP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MAP, 0)
	}
	/// Retrieves first TerminalNode corresponding to token STRUCT
	/// Returns `None` if there is no child corresponding to token STRUCT
	fn STRUCT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(STRUCT, 0)
	}
	/// Retrieves first TerminalNode corresponding to token NEQ
	/// Returns `None` if there is no child corresponding to token NEQ
	fn NEQ(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(NEQ, 0)
	}
	fn complexColTypeList(&self) -> Option<Rc<ComplexColTypeListContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> ComplexDataTypeContextAttrs<'input> for ComplexDataTypeContext<'input>{}

pub struct ComplexDataTypeContextExt<'input>{
	base:DataTypeContextExt<'input>,
	pub complex: Option<TokenType<'input>>,
	ph:PhantomData<&'input str>
}

crate::tid!{ComplexDataTypeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ComplexDataTypeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ComplexDataTypeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_complexDataType(self);
	}
}

impl<'input> CustomRuleContext<'input> for ComplexDataTypeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dataType }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dataType }
}

impl<'input> Borrow<DataTypeContextExt<'input>> for ComplexDataTypeContext<'input>{
	fn borrow(&self) -> &DataTypeContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<DataTypeContextExt<'input>> for ComplexDataTypeContext<'input>{
	fn borrow_mut(&mut self) -> &mut DataTypeContextExt<'input> { &mut self.base }
}

impl<'input> DataTypeContextAttrs<'input> for ComplexDataTypeContext<'input> {}

impl<'input> ComplexDataTypeContextExt<'input>{
	fn new(ctx: &dyn DataTypeContextAttrs<'input>) -> Rc<DataTypeContextAll<'input>>  {
		Rc::new(
			DataTypeContextAll::ComplexDataTypeContext(
				BaseParserRuleContext::copy_from(ctx,ComplexDataTypeContextExt{
					complex:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type PrimitiveDataTypeContext<'input> = BaseParserRuleContext<'input,PrimitiveDataTypeContextExt<'input>>;

pub trait PrimitiveDataTypeContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	/// Retrieves all `TerminalNode`s corresponding to token INTEGER_VALUE in current rule
	fn INTEGER_VALUE_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token INTEGER_VALUE, starting from 0.
	/// Returns `None` if number of children corresponding to token INTEGER_VALUE is less or equal than `i`.
	fn INTEGER_VALUE(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTEGER_VALUE, i)
	}
}

impl<'input> PrimitiveDataTypeContextAttrs<'input> for PrimitiveDataTypeContext<'input>{}

pub struct PrimitiveDataTypeContextExt<'input>{
	base:DataTypeContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{PrimitiveDataTypeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for PrimitiveDataTypeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for PrimitiveDataTypeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_primitiveDataType(self);
	}
}

impl<'input> CustomRuleContext<'input> for PrimitiveDataTypeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_dataType }
	//fn type_rule_index() -> usize where Self: Sized { RULE_dataType }
}

impl<'input> Borrow<DataTypeContextExt<'input>> for PrimitiveDataTypeContext<'input>{
	fn borrow(&self) -> &DataTypeContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<DataTypeContextExt<'input>> for PrimitiveDataTypeContext<'input>{
	fn borrow_mut(&mut self) -> &mut DataTypeContextExt<'input> { &mut self.base }
}

impl<'input> DataTypeContextAttrs<'input> for PrimitiveDataTypeContext<'input> {}

impl<'input> PrimitiveDataTypeContextExt<'input>{
	fn new(ctx: &dyn DataTypeContextAttrs<'input>) -> Rc<DataTypeContextAll<'input>>  {
		Rc::new(
			DataTypeContextAll::PrimitiveDataTypeContext(
				BaseParserRuleContext::copy_from(ctx,PrimitiveDataTypeContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn dataType(&mut self,)
	-> Result<Rc<DataTypeContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = DataTypeContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 222, RULE_dataType);
        let mut _localctx: Rc<DataTypeContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2771);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(354,&mut recog.base)? {
				1 =>{
					let tmp = ComplexDataTypeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(2737);
					let tmp = recog.base.match_token(ARRAY,&mut recog.err_handler)?;
					if let DataTypeContextAll::ComplexDataTypeContext(ctx) = cast_mut::<_,DataTypeContextAll >(&mut _localctx){
					ctx.complex = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2738);
					recog.base.match_token(LT,&mut recog.err_handler)?;

					/*InvokeRule dataType*/
					recog.base.set_state(2739);
					recog.dataType()?;

					recog.base.set_state(2740);
					recog.base.match_token(GT,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					let tmp = ComplexDataTypeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					recog.base.set_state(2742);
					let tmp = recog.base.match_token(MAP,&mut recog.err_handler)?;
					if let DataTypeContextAll::ComplexDataTypeContext(ctx) = cast_mut::<_,DataTypeContextAll >(&mut _localctx){
					ctx.complex = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2743);
					recog.base.match_token(LT,&mut recog.err_handler)?;

					/*InvokeRule dataType*/
					recog.base.set_state(2744);
					recog.dataType()?;

					recog.base.set_state(2745);
					recog.base.match_token(T__2,&mut recog.err_handler)?;

					/*InvokeRule dataType*/
					recog.base.set_state(2746);
					recog.dataType()?;

					recog.base.set_state(2747);
					recog.base.match_token(GT,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					let tmp = ComplexDataTypeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(2749);
					let tmp = recog.base.match_token(STRUCT,&mut recog.err_handler)?;
					if let DataTypeContextAll::ComplexDataTypeContext(ctx) = cast_mut::<_,DataTypeContextAll >(&mut _localctx){
					ctx.complex = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2756);
					recog.err_handler.sync(&mut recog.base)?;
					match recog.base.input.la(1) {
					 LT 
						=> {
							{
							recog.base.set_state(2750);
							recog.base.match_token(LT,&mut recog.err_handler)?;

							recog.base.set_state(2752);
							recog.err_handler.sync(&mut recog.base)?;
							match  recog.interpreter.adaptive_predict(350,&mut recog.base)? {
								x if x == 1=>{
									{
									/*InvokeRule complexColTypeList*/
									recog.base.set_state(2751);
									recog.complexColTypeList()?;

									}
								}

								_ => {}
							}
							recog.base.set_state(2754);
							recog.base.match_token(GT,&mut recog.err_handler)?;

							}
						}

					 NEQ 
						=> {
							{
							recog.base.set_state(2755);
							recog.base.match_token(NEQ,&mut recog.err_handler)?;

							}
						}

						_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
					}
					}
				}
			,
				4 =>{
					let tmp = PrimitiveDataTypeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					/*InvokeRule identifier*/
					recog.base.set_state(2758);
					recog.identifier()?;

					recog.base.set_state(2769);
					recog.err_handler.sync(&mut recog.base)?;
					match  recog.interpreter.adaptive_predict(353,&mut recog.base)? {
						x if x == 1=>{
							{
							recog.base.set_state(2759);
							recog.base.match_token(T__0,&mut recog.err_handler)?;

							recog.base.set_state(2760);
							recog.base.match_token(INTEGER_VALUE,&mut recog.err_handler)?;

							recog.base.set_state(2765);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							while _la==T__2 {
								{
								{
								recog.base.set_state(2761);
								recog.base.match_token(T__2,&mut recog.err_handler)?;

								recog.base.set_state(2762);
								recog.base.match_token(INTEGER_VALUE,&mut recog.err_handler)?;

								}
								}
								recog.base.set_state(2767);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
							}
							recog.base.set_state(2768);
							recog.base.match_token(T__1,&mut recog.err_handler)?;

							}
						}

						_ => {}
					}
					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- qualifiedColTypeWithPositionList ----------------
pub type QualifiedColTypeWithPositionListContextAll<'input> = QualifiedColTypeWithPositionListContext<'input>;


pub type QualifiedColTypeWithPositionListContext<'input> = BaseParserRuleContext<'input,QualifiedColTypeWithPositionListContextExt<'input>>;

#[derive(Clone)]
pub struct QualifiedColTypeWithPositionListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QualifiedColTypeWithPositionListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QualifiedColTypeWithPositionListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_qualifiedColTypeWithPositionList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_qualifiedColTypeWithPositionList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for QualifiedColTypeWithPositionListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_qualifiedColTypeWithPositionList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_qualifiedColTypeWithPositionList }
}
crate::tid!{QualifiedColTypeWithPositionListContextExt<'a>}

impl<'input> QualifiedColTypeWithPositionListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QualifiedColTypeWithPositionListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QualifiedColTypeWithPositionListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait QualifiedColTypeWithPositionListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QualifiedColTypeWithPositionListContextExt<'input>>{

fn qualifiedColTypeWithPosition_all(&self) ->  Vec<Rc<QualifiedColTypeWithPositionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn qualifiedColTypeWithPosition(&self, i: usize) -> Option<Rc<QualifiedColTypeWithPositionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> QualifiedColTypeWithPositionListContextAttrs<'input> for QualifiedColTypeWithPositionListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn qualifiedColTypeWithPositionList(&mut self,)
	-> Result<Rc<QualifiedColTypeWithPositionListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QualifiedColTypeWithPositionListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 224, RULE_qualifiedColTypeWithPositionList);
        let mut _localctx: Rc<QualifiedColTypeWithPositionListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule qualifiedColTypeWithPosition*/
			recog.base.set_state(2773);
			recog.qualifiedColTypeWithPosition()?;

			recog.base.set_state(2778);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(2774);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule qualifiedColTypeWithPosition*/
				recog.base.set_state(2775);
				recog.qualifiedColTypeWithPosition()?;

				}
				}
				recog.base.set_state(2780);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- qualifiedColTypeWithPosition ----------------
pub type QualifiedColTypeWithPositionContextAll<'input> = QualifiedColTypeWithPositionContext<'input>;


pub type QualifiedColTypeWithPositionContext<'input> = BaseParserRuleContext<'input,QualifiedColTypeWithPositionContextExt<'input>>;

#[derive(Clone)]
pub struct QualifiedColTypeWithPositionContextExt<'input>{
	pub name: Option<Rc<MultipartIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QualifiedColTypeWithPositionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QualifiedColTypeWithPositionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_qualifiedColTypeWithPosition(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_qualifiedColTypeWithPosition(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for QualifiedColTypeWithPositionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_qualifiedColTypeWithPosition }
	//fn type_rule_index() -> usize where Self: Sized { RULE_qualifiedColTypeWithPosition }
}
crate::tid!{QualifiedColTypeWithPositionContextExt<'a>}

impl<'input> QualifiedColTypeWithPositionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QualifiedColTypeWithPositionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QualifiedColTypeWithPositionContextExt{
				name: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait QualifiedColTypeWithPositionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QualifiedColTypeWithPositionContextExt<'input>>{

fn dataType(&self) -> Option<Rc<DataTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn multipartIdentifier(&self) -> Option<Rc<MultipartIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token NULL
/// Returns `None` if there is no child corresponding to token NULL
fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULL, 0)
}
fn commentSpec(&self) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn colPosition(&self) -> Option<Rc<ColPositionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> QualifiedColTypeWithPositionContextAttrs<'input> for QualifiedColTypeWithPositionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn qualifiedColTypeWithPosition(&mut self,)
	-> Result<Rc<QualifiedColTypeWithPositionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QualifiedColTypeWithPositionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 226, RULE_qualifiedColTypeWithPosition);
        let mut _localctx: Rc<QualifiedColTypeWithPositionContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule multipartIdentifier*/
			recog.base.set_state(2781);
			let tmp = recog.multipartIdentifier()?;
			 cast_mut::<_,QualifiedColTypeWithPositionContext >(&mut _localctx).name = Some(tmp.clone());
			  

			/*InvokeRule dataType*/
			recog.base.set_state(2782);
			recog.dataType()?;

			recog.base.set_state(2785);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==NOT {
				{
				recog.base.set_state(2783);
				recog.base.match_token(NOT,&mut recog.err_handler)?;

				recog.base.set_state(2784);
				recog.base.match_token(NULL,&mut recog.err_handler)?;

				}
			}

			recog.base.set_state(2788);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(357,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule commentSpec*/
					recog.base.set_state(2787);
					recog.commentSpec()?;

					}
				}

				_ => {}
			}
			recog.base.set_state(2791);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==AFTER || _la==FIRST {
				{
				/*InvokeRule colPosition*/
				recog.base.set_state(2790);
				recog.colPosition()?;

				}
			}

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- colTypeList ----------------
pub type ColTypeListContextAll<'input> = ColTypeListContext<'input>;


pub type ColTypeListContext<'input> = BaseParserRuleContext<'input,ColTypeListContextExt<'input>>;

#[derive(Clone)]
pub struct ColTypeListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ColTypeListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ColTypeListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_colTypeList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_colTypeList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ColTypeListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_colTypeList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_colTypeList }
}
crate::tid!{ColTypeListContextExt<'a>}

impl<'input> ColTypeListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ColTypeListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ColTypeListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ColTypeListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ColTypeListContextExt<'input>>{

fn colType_all(&self) ->  Vec<Rc<ColTypeContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn colType(&self, i: usize) -> Option<Rc<ColTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> ColTypeListContextAttrs<'input> for ColTypeListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn colTypeList(&mut self,)
	-> Result<Rc<ColTypeListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ColTypeListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 228, RULE_colTypeList);
        let mut _localctx: Rc<ColTypeListContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule colType*/
			recog.base.set_state(2793);
			recog.colType()?;

			recog.base.set_state(2798);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(359,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(2794);
					recog.base.match_token(T__2,&mut recog.err_handler)?;

					/*InvokeRule colType*/
					recog.base.set_state(2795);
					recog.colType()?;

					}
					} 
				}
				recog.base.set_state(2800);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(359,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- colType ----------------
pub type ColTypeContextAll<'input> = ColTypeContext<'input>;


pub type ColTypeContext<'input> = BaseParserRuleContext<'input,ColTypeContextExt<'input>>;

#[derive(Clone)]
pub struct ColTypeContextExt<'input>{
	pub colName: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ColTypeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ColTypeContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_colType(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_colType(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ColTypeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_colType }
	//fn type_rule_index() -> usize where Self: Sized { RULE_colType }
}
crate::tid!{ColTypeContextExt<'a>}

impl<'input> ColTypeContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ColTypeContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ColTypeContextExt{
				colName: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait ColTypeContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ColTypeContextExt<'input>>{

fn dataType(&self) -> Option<Rc<DataTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token NULL
/// Returns `None` if there is no child corresponding to token NULL
fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULL, 0)
}
fn commentSpec(&self) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ColTypeContextAttrs<'input> for ColTypeContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn colType(&mut self,)
	-> Result<Rc<ColTypeContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ColTypeContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 230, RULE_colType);
        let mut _localctx: Rc<ColTypeContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(2801);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,ColTypeContext >(&mut _localctx).colName = Some(tmp.clone());
			  

			/*InvokeRule dataType*/
			recog.base.set_state(2802);
			recog.dataType()?;

			recog.base.set_state(2805);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(360,&mut recog.base)? {
				x if x == 1=>{
					{
					recog.base.set_state(2803);
					recog.base.match_token(NOT,&mut recog.err_handler)?;

					recog.base.set_state(2804);
					recog.base.match_token(NULL,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			recog.base.set_state(2808);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(361,&mut recog.base)? {
				x if x == 1=>{
					{
					/*InvokeRule commentSpec*/
					recog.base.set_state(2807);
					recog.commentSpec()?;

					}
				}

				_ => {}
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- complexColTypeList ----------------
pub type ComplexColTypeListContextAll<'input> = ComplexColTypeListContext<'input>;


pub type ComplexColTypeListContext<'input> = BaseParserRuleContext<'input,ComplexColTypeListContextExt<'input>>;

#[derive(Clone)]
pub struct ComplexColTypeListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ComplexColTypeListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ComplexColTypeListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_complexColTypeList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_complexColTypeList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ComplexColTypeListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_complexColTypeList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_complexColTypeList }
}
crate::tid!{ComplexColTypeListContextExt<'a>}

impl<'input> ComplexColTypeListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ComplexColTypeListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ComplexColTypeListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ComplexColTypeListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ComplexColTypeListContextExt<'input>>{

fn complexColType_all(&self) ->  Vec<Rc<ComplexColTypeContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn complexColType(&self, i: usize) -> Option<Rc<ComplexColTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> ComplexColTypeListContextAttrs<'input> for ComplexColTypeListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn complexColTypeList(&mut self,)
	-> Result<Rc<ComplexColTypeListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ComplexColTypeListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 232, RULE_complexColTypeList);
        let mut _localctx: Rc<ComplexColTypeListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule complexColType*/
			recog.base.set_state(2810);
			recog.complexColType()?;

			recog.base.set_state(2815);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(2811);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule complexColType*/
				recog.base.set_state(2812);
				recog.complexColType()?;

				}
				}
				recog.base.set_state(2817);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- complexColType ----------------
pub type ComplexColTypeContextAll<'input> = ComplexColTypeContext<'input>;


pub type ComplexColTypeContext<'input> = BaseParserRuleContext<'input,ComplexColTypeContextExt<'input>>;

#[derive(Clone)]
pub struct ComplexColTypeContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ComplexColTypeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ComplexColTypeContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_complexColType(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_complexColType(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ComplexColTypeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_complexColType }
	//fn type_rule_index() -> usize where Self: Sized { RULE_complexColType }
}
crate::tid!{ComplexColTypeContextExt<'a>}

impl<'input> ComplexColTypeContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ComplexColTypeContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ComplexColTypeContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ComplexColTypeContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ComplexColTypeContextExt<'input>>{

fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn dataType(&self) -> Option<Rc<DataTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token NULL
/// Returns `None` if there is no child corresponding to token NULL
fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULL, 0)
}
fn commentSpec(&self) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ComplexColTypeContextAttrs<'input> for ComplexColTypeContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn complexColType(&mut self,)
	-> Result<Rc<ComplexColTypeContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ComplexColTypeContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 234, RULE_complexColType);
        let mut _localctx: Rc<ComplexColTypeContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule identifier*/
			recog.base.set_state(2818);
			recog.identifier()?;

			recog.base.set_state(2819);
			recog.base.match_token(T__9,&mut recog.err_handler)?;

			/*InvokeRule dataType*/
			recog.base.set_state(2820);
			recog.dataType()?;

			recog.base.set_state(2823);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==NOT {
				{
				recog.base.set_state(2821);
				recog.base.match_token(NOT,&mut recog.err_handler)?;

				recog.base.set_state(2822);
				recog.base.match_token(NULL,&mut recog.err_handler)?;

				}
			}

			recog.base.set_state(2826);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			if _la==COMMENT {
				{
				/*InvokeRule commentSpec*/
				recog.base.set_state(2825);
				recog.commentSpec()?;

				}
			}

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- whenClause ----------------
pub type WhenClauseContextAll<'input> = WhenClauseContext<'input>;


pub type WhenClauseContext<'input> = BaseParserRuleContext<'input,WhenClauseContextExt<'input>>;

#[derive(Clone)]
pub struct WhenClauseContextExt<'input>{
	pub condition: Option<Rc<ExpressionContextAll<'input>>>,
	pub result: Option<Rc<ExpressionContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for WhenClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WhenClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_whenClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_whenClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for WhenClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_whenClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_whenClause }
}
crate::tid!{WhenClauseContextExt<'a>}

impl<'input> WhenClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<WhenClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,WhenClauseContextExt{
				condition: None, result: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait WhenClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<WhenClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token WHEN
/// Returns `None` if there is no child corresponding to token WHEN
fn WHEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WHEN, 0)
}
/// Retrieves first TerminalNode corresponding to token THEN
/// Returns `None` if there is no child corresponding to token THEN
fn THEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(THEN, 0)
}
fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> WhenClauseContextAttrs<'input> for WhenClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn whenClause(&mut self,)
	-> Result<Rc<WhenClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = WhenClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 236, RULE_whenClause);
        let mut _localctx: Rc<WhenClauseContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2828);
			recog.base.match_token(WHEN,&mut recog.err_handler)?;

			/*InvokeRule expression*/
			recog.base.set_state(2829);
			let tmp = recog.expression()?;
			 cast_mut::<_,WhenClauseContext >(&mut _localctx).condition = Some(tmp.clone());
			  

			recog.base.set_state(2830);
			recog.base.match_token(THEN,&mut recog.err_handler)?;

			/*InvokeRule expression*/
			recog.base.set_state(2831);
			let tmp = recog.expression()?;
			 cast_mut::<_,WhenClauseContext >(&mut _localctx).result = Some(tmp.clone());
			  

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- windowClause ----------------
pub type WindowClauseContextAll<'input> = WindowClauseContext<'input>;


pub type WindowClauseContext<'input> = BaseParserRuleContext<'input,WindowClauseContextExt<'input>>;

#[derive(Clone)]
pub struct WindowClauseContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for WindowClauseContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WindowClauseContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_windowClause(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_windowClause(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for WindowClauseContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_windowClause }
	//fn type_rule_index() -> usize where Self: Sized { RULE_windowClause }
}
crate::tid!{WindowClauseContextExt<'a>}

impl<'input> WindowClauseContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<WindowClauseContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,WindowClauseContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait WindowClauseContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<WindowClauseContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token WINDOW
/// Returns `None` if there is no child corresponding to token WINDOW
fn WINDOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WINDOW, 0)
}
fn namedWindow_all(&self) ->  Vec<Rc<NamedWindowContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn namedWindow(&self, i: usize) -> Option<Rc<NamedWindowContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> WindowClauseContextAttrs<'input> for WindowClauseContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn windowClause(&mut self,)
	-> Result<Rc<WindowClauseContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = WindowClauseContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 238, RULE_windowClause);
        let mut _localctx: Rc<WindowClauseContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2833);
			recog.base.match_token(WINDOW,&mut recog.err_handler)?;

			/*InvokeRule namedWindow*/
			recog.base.set_state(2834);
			recog.namedWindow()?;

			recog.base.set_state(2839);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(365,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(2835);
					recog.base.match_token(T__2,&mut recog.err_handler)?;

					/*InvokeRule namedWindow*/
					recog.base.set_state(2836);
					recog.namedWindow()?;

					}
					} 
				}
				recog.base.set_state(2841);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(365,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- namedWindow ----------------
pub type NamedWindowContextAll<'input> = NamedWindowContext<'input>;


pub type NamedWindowContext<'input> = BaseParserRuleContext<'input,NamedWindowContextExt<'input>>;

#[derive(Clone)]
pub struct NamedWindowContextExt<'input>{
	pub name: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NamedWindowContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NamedWindowContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_namedWindow(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_namedWindow(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NamedWindowContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_namedWindow }
	//fn type_rule_index() -> usize where Self: Sized { RULE_namedWindow }
}
crate::tid!{NamedWindowContextExt<'a>}

impl<'input> NamedWindowContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NamedWindowContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NamedWindowContextExt{
				name: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait NamedWindowContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NamedWindowContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
fn windowSpec(&self) -> Option<Rc<WindowSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> NamedWindowContextAttrs<'input> for NamedWindowContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn namedWindow(&mut self,)
	-> Result<Rc<NamedWindowContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NamedWindowContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 240, RULE_namedWindow);
        let mut _localctx: Rc<NamedWindowContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule errorCapturingIdentifier*/
			recog.base.set_state(2842);
			let tmp = recog.errorCapturingIdentifier()?;
			 cast_mut::<_,NamedWindowContext >(&mut _localctx).name = Some(tmp.clone());
			  

			recog.base.set_state(2843);
			recog.base.match_token(AS,&mut recog.err_handler)?;

			/*InvokeRule windowSpec*/
			recog.base.set_state(2844);
			recog.windowSpec()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- windowSpec ----------------
#[derive(Debug)]
pub enum WindowSpecContextAll<'input>{
	WindowRefContext(WindowRefContext<'input>),
	WindowDefContext(WindowDefContext<'input>),
Error(WindowSpecContext<'input>)
}
crate::tid!{WindowSpecContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for WindowSpecContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for WindowSpecContextAll<'input>{}

impl<'input> Deref for WindowSpecContextAll<'input>{
	type Target = dyn WindowSpecContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use WindowSpecContextAll::*;
		match self{
			WindowRefContext(inner) => inner,
			WindowDefContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WindowSpecContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type WindowSpecContext<'input> = BaseParserRuleContext<'input,WindowSpecContextExt<'input>>;

#[derive(Clone)]
pub struct WindowSpecContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for WindowSpecContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WindowSpecContext<'input>{
}

impl<'input> CustomRuleContext<'input> for WindowSpecContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_windowSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_windowSpec }
}
crate::tid!{WindowSpecContextExt<'a>}

impl<'input> WindowSpecContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<WindowSpecContextAll<'input>> {
		Rc::new(
		WindowSpecContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,WindowSpecContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait WindowSpecContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<WindowSpecContextExt<'input>>{


}

impl<'input> WindowSpecContextAttrs<'input> for WindowSpecContext<'input>{}

pub type WindowRefContext<'input> = BaseParserRuleContext<'input,WindowRefContextExt<'input>>;

pub trait WindowRefContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn errorCapturingIdentifier(&self) -> Option<Rc<ErrorCapturingIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> WindowRefContextAttrs<'input> for WindowRefContext<'input>{}

pub struct WindowRefContextExt<'input>{
	base:WindowSpecContextExt<'input>,
	pub name: Option<Rc<ErrorCapturingIdentifierContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{WindowRefContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for WindowRefContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WindowRefContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_windowRef(self);
	}
}

impl<'input> CustomRuleContext<'input> for WindowRefContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_windowSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_windowSpec }
}

impl<'input> Borrow<WindowSpecContextExt<'input>> for WindowRefContext<'input>{
	fn borrow(&self) -> &WindowSpecContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<WindowSpecContextExt<'input>> for WindowRefContext<'input>{
	fn borrow_mut(&mut self) -> &mut WindowSpecContextExt<'input> { &mut self.base }
}

impl<'input> WindowSpecContextAttrs<'input> for WindowRefContext<'input> {}

impl<'input> WindowRefContextExt<'input>{
	fn new(ctx: &dyn WindowSpecContextAttrs<'input>) -> Rc<WindowSpecContextAll<'input>>  {
		Rc::new(
			WindowSpecContextAll::WindowRefContext(
				BaseParserRuleContext::copy_from(ctx,WindowRefContextExt{
        			name:None, 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type WindowDefContext<'input> = BaseParserRuleContext<'input,WindowDefContextExt<'input>>;

pub trait WindowDefContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token CLUSTER
	/// Returns `None` if there is no child corresponding to token CLUSTER
	fn CLUSTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(CLUSTER, 0)
	}
	/// Retrieves all `TerminalNode`s corresponding to token BY in current rule
	fn BY_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token BY, starting from 0.
	/// Returns `None` if number of children corresponding to token BY is less or equal than `i`.
	fn BY(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(BY, i)
	}
	fn expression_all(&self) ->  Vec<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn expression(&self, i: usize) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	fn windowFrame(&self) -> Option<Rc<WindowFrameContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn sortItem_all(&self) ->  Vec<Rc<SortItemContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn sortItem(&self, i: usize) -> Option<Rc<SortItemContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
	/// Retrieves first TerminalNode corresponding to token PARTITION
	/// Returns `None` if there is no child corresponding to token PARTITION
	fn PARTITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(PARTITION, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DISTRIBUTE
	/// Returns `None` if there is no child corresponding to token DISTRIBUTE
	fn DISTRIBUTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DISTRIBUTE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token ORDER
	/// Returns `None` if there is no child corresponding to token ORDER
	fn ORDER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(ORDER, 0)
	}
	/// Retrieves first TerminalNode corresponding to token SORT
	/// Returns `None` if there is no child corresponding to token SORT
	fn SORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SORT, 0)
	}
}

impl<'input> WindowDefContextAttrs<'input> for WindowDefContext<'input>{}

pub struct WindowDefContextExt<'input>{
	base:WindowSpecContextExt<'input>,
	pub expression: Option<Rc<ExpressionContextAll<'input>>>,
	pub partition:Vec<Rc<ExpressionContextAll<'input>>>,
	ph:PhantomData<&'input str>
}

crate::tid!{WindowDefContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for WindowDefContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WindowDefContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_windowDef(self);
	}
}

impl<'input> CustomRuleContext<'input> for WindowDefContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_windowSpec }
	//fn type_rule_index() -> usize where Self: Sized { RULE_windowSpec }
}

impl<'input> Borrow<WindowSpecContextExt<'input>> for WindowDefContext<'input>{
	fn borrow(&self) -> &WindowSpecContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<WindowSpecContextExt<'input>> for WindowDefContext<'input>{
	fn borrow_mut(&mut self) -> &mut WindowSpecContextExt<'input> { &mut self.base }
}

impl<'input> WindowSpecContextAttrs<'input> for WindowDefContext<'input> {}

impl<'input> WindowDefContextExt<'input>{
	fn new(ctx: &dyn WindowSpecContextAttrs<'input>) -> Rc<WindowSpecContextAll<'input>>  {
		Rc::new(
			WindowSpecContextAll::WindowDefContext(
				BaseParserRuleContext::copy_from(ctx,WindowDefContextExt{
        			expression:None, 
        			partition:Vec::new(), 
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn windowSpec(&mut self,)
	-> Result<Rc<WindowSpecContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = WindowSpecContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 242, RULE_windowSpec);
        let mut _localctx: Rc<WindowSpecContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2892);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(373,&mut recog.base)? {
				1 =>{
					let tmp = WindowRefContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					/*InvokeRule errorCapturingIdentifier*/
					recog.base.set_state(2846);
					let tmp = recog.errorCapturingIdentifier()?;
					if let WindowSpecContextAll::WindowRefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
					ctx.name = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					}
				}
			,
				2 =>{
					let tmp = WindowRefContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					recog.base.set_state(2847);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					/*InvokeRule errorCapturingIdentifier*/
					recog.base.set_state(2848);
					let tmp = recog.errorCapturingIdentifier()?;
					if let WindowSpecContextAll::WindowRefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
					ctx.name = Some(tmp.clone()); } else {unreachable!("cant cast");}  

					recog.base.set_state(2849);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					let tmp = WindowDefContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(2851);
					recog.base.match_token(T__0,&mut recog.err_handler)?;

					recog.base.set_state(2886);
					recog.err_handler.sync(&mut recog.base)?;
					match recog.base.input.la(1) {
					 CLUSTER 
						=> {
							{
							recog.base.set_state(2852);
							recog.base.match_token(CLUSTER,&mut recog.err_handler)?;

							recog.base.set_state(2853);
							recog.base.match_token(BY,&mut recog.err_handler)?;

							/*InvokeRule expression*/
							recog.base.set_state(2854);
							let tmp = recog.expression()?;
							if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
							ctx.expression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

							let temp = if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
							ctx.expression.clone().unwrap() } else {unreachable!("cant cast");} ;
							if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
							ctx.partition.push(temp); } else {unreachable!("cant cast");}  
							recog.base.set_state(2859);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							while _la==T__2 {
								{
								{
								recog.base.set_state(2855);
								recog.base.match_token(T__2,&mut recog.err_handler)?;

								/*InvokeRule expression*/
								recog.base.set_state(2856);
								let tmp = recog.expression()?;
								if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
								ctx.expression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

								let temp = if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
								ctx.expression.clone().unwrap() } else {unreachable!("cant cast");} ;
								if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
								ctx.partition.push(temp); } else {unreachable!("cant cast");}  
								}
								}
								recog.base.set_state(2861);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
							}
							}
						}

					 T__1 | DISTRIBUTE | ORDER | PARTITION | RANGE | ROWS | SORT 
						=> {
							{
							recog.base.set_state(2872);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==DISTRIBUTE || _la==PARTITION {
								{
								recog.base.set_state(2862);
								_la = recog.base.input.la(1);
								if { !(_la==DISTRIBUTE || _la==PARTITION) } {
									recog.err_handler.recover_inline(&mut recog.base)?;

								}
								else {
									if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
									recog.err_handler.report_match(&mut recog.base);
									recog.base.consume(&mut recog.err_handler);
								}
								recog.base.set_state(2863);
								recog.base.match_token(BY,&mut recog.err_handler)?;

								/*InvokeRule expression*/
								recog.base.set_state(2864);
								let tmp = recog.expression()?;
								if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
								ctx.expression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

								let temp = if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
								ctx.expression.clone().unwrap() } else {unreachable!("cant cast");} ;
								if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
								ctx.partition.push(temp); } else {unreachable!("cant cast");}  
								recog.base.set_state(2869);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
								while _la==T__2 {
									{
									{
									recog.base.set_state(2865);
									recog.base.match_token(T__2,&mut recog.err_handler)?;

									/*InvokeRule expression*/
									recog.base.set_state(2866);
									let tmp = recog.expression()?;
									if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
									ctx.expression = Some(tmp.clone()); } else {unreachable!("cant cast");}  

									let temp = if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
									ctx.expression.clone().unwrap() } else {unreachable!("cant cast");} ;
									if let WindowSpecContextAll::WindowDefContext(ctx) = cast_mut::<_,WindowSpecContextAll >(&mut _localctx){
									ctx.partition.push(temp); } else {unreachable!("cant cast");}  
									}
									}
									recog.base.set_state(2871);
									recog.err_handler.sync(&mut recog.base)?;
									_la = recog.base.input.la(1);
								}
								}
							}

							recog.base.set_state(2884);
							recog.err_handler.sync(&mut recog.base)?;
							_la = recog.base.input.la(1);
							if _la==ORDER || _la==SORT {
								{
								recog.base.set_state(2874);
								_la = recog.base.input.la(1);
								if { !(_la==ORDER || _la==SORT) } {
									recog.err_handler.recover_inline(&mut recog.base)?;

								}
								else {
									if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
									recog.err_handler.report_match(&mut recog.base);
									recog.base.consume(&mut recog.err_handler);
								}
								recog.base.set_state(2875);
								recog.base.match_token(BY,&mut recog.err_handler)?;

								/*InvokeRule sortItem*/
								recog.base.set_state(2876);
								recog.sortItem()?;

								recog.base.set_state(2881);
								recog.err_handler.sync(&mut recog.base)?;
								_la = recog.base.input.la(1);
								while _la==T__2 {
									{
									{
									recog.base.set_state(2877);
									recog.base.match_token(T__2,&mut recog.err_handler)?;

									/*InvokeRule sortItem*/
									recog.base.set_state(2878);
									recog.sortItem()?;

									}
									}
									recog.base.set_state(2883);
									recog.err_handler.sync(&mut recog.base)?;
									_la = recog.base.input.la(1);
								}
								}
							}

							}
						}

						_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
					}
					recog.base.set_state(2889);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==RANGE || _la==ROWS {
						{
						/*InvokeRule windowFrame*/
						recog.base.set_state(2888);
						recog.windowFrame()?;

						}
					}

					recog.base.set_state(2891);
					recog.base.match_token(T__1,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- windowFrame ----------------
pub type WindowFrameContextAll<'input> = WindowFrameContext<'input>;


pub type WindowFrameContext<'input> = BaseParserRuleContext<'input,WindowFrameContextExt<'input>>;

#[derive(Clone)]
pub struct WindowFrameContextExt<'input>{
	pub frameType: Option<TokenType<'input>>,
	pub frameStart: Option<Rc<FrameBoundContextAll<'input>>>,
	pub end: Option<Rc<FrameBoundContextAll<'input>>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for WindowFrameContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for WindowFrameContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_windowFrame(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_windowFrame(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for WindowFrameContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_windowFrame }
	//fn type_rule_index() -> usize where Self: Sized { RULE_windowFrame }
}
crate::tid!{WindowFrameContextExt<'a>}

impl<'input> WindowFrameContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<WindowFrameContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,WindowFrameContextExt{
				frameType: None, 
				frameStart: None, end: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait WindowFrameContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<WindowFrameContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token RANGE
/// Returns `None` if there is no child corresponding to token RANGE
fn RANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RANGE, 0)
}
fn frameBound_all(&self) ->  Vec<Rc<FrameBoundContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn frameBound(&self, i: usize) -> Option<Rc<FrameBoundContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}
/// Retrieves first TerminalNode corresponding to token ROWS
/// Returns `None` if there is no child corresponding to token ROWS
fn ROWS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROWS, 0)
}
/// Retrieves first TerminalNode corresponding to token BETWEEN
/// Returns `None` if there is no child corresponding to token BETWEEN
fn BETWEEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BETWEEN, 0)
}
/// Retrieves first TerminalNode corresponding to token AND
/// Returns `None` if there is no child corresponding to token AND
fn AND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AND, 0)
}

}

impl<'input> WindowFrameContextAttrs<'input> for WindowFrameContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn windowFrame(&mut self,)
	-> Result<Rc<WindowFrameContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = WindowFrameContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 244, RULE_windowFrame);
        let mut _localctx: Rc<WindowFrameContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2910);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(374,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(2894);
					let tmp = recog.base.match_token(RANGE,&mut recog.err_handler)?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameType = Some(tmp.clone());
					  

					/*InvokeRule frameBound*/
					recog.base.set_state(2895);
					let tmp = recog.frameBound()?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameStart = Some(tmp.clone());
					  

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2896);
					let tmp = recog.base.match_token(ROWS,&mut recog.err_handler)?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameType = Some(tmp.clone());
					  

					/*InvokeRule frameBound*/
					recog.base.set_state(2897);
					let tmp = recog.frameBound()?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameStart = Some(tmp.clone());
					  

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					recog.base.set_state(2898);
					let tmp = recog.base.match_token(RANGE,&mut recog.err_handler)?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameType = Some(tmp.clone());
					  

					recog.base.set_state(2899);
					recog.base.match_token(BETWEEN,&mut recog.err_handler)?;

					/*InvokeRule frameBound*/
					recog.base.set_state(2900);
					let tmp = recog.frameBound()?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameStart = Some(tmp.clone());
					  

					recog.base.set_state(2901);
					recog.base.match_token(AND,&mut recog.err_handler)?;

					/*InvokeRule frameBound*/
					recog.base.set_state(2902);
					let tmp = recog.frameBound()?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).end = Some(tmp.clone());
					  

					}
				}
			,
				4 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					recog.base.set_state(2904);
					let tmp = recog.base.match_token(ROWS,&mut recog.err_handler)?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameType = Some(tmp.clone());
					  

					recog.base.set_state(2905);
					recog.base.match_token(BETWEEN,&mut recog.err_handler)?;

					/*InvokeRule frameBound*/
					recog.base.set_state(2906);
					let tmp = recog.frameBound()?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).frameStart = Some(tmp.clone());
					  

					recog.base.set_state(2907);
					recog.base.match_token(AND,&mut recog.err_handler)?;

					/*InvokeRule frameBound*/
					recog.base.set_state(2908);
					let tmp = recog.frameBound()?;
					 cast_mut::<_,WindowFrameContext >(&mut _localctx).end = Some(tmp.clone());
					  

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- frameBound ----------------
pub type FrameBoundContextAll<'input> = FrameBoundContext<'input>;


pub type FrameBoundContext<'input> = BaseParserRuleContext<'input,FrameBoundContextExt<'input>>;

#[derive(Clone)]
pub struct FrameBoundContextExt<'input>{
	pub boundType: Option<TokenType<'input>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for FrameBoundContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FrameBoundContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_frameBound(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_frameBound(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for FrameBoundContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_frameBound }
	//fn type_rule_index() -> usize where Self: Sized { RULE_frameBound }
}
crate::tid!{FrameBoundContextExt<'a>}

impl<'input> FrameBoundContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<FrameBoundContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,FrameBoundContextExt{
				boundType: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait FrameBoundContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<FrameBoundContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token UNBOUNDED
/// Returns `None` if there is no child corresponding to token UNBOUNDED
fn UNBOUNDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNBOUNDED, 0)
}
/// Retrieves first TerminalNode corresponding to token PRECEDING
/// Returns `None` if there is no child corresponding to token PRECEDING
fn PRECEDING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PRECEDING, 0)
}
/// Retrieves first TerminalNode corresponding to token FOLLOWING
/// Returns `None` if there is no child corresponding to token FOLLOWING
fn FOLLOWING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FOLLOWING, 0)
}
/// Retrieves first TerminalNode corresponding to token ROW
/// Returns `None` if there is no child corresponding to token ROW
fn ROW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROW, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT
/// Returns `None` if there is no child corresponding to token CURRENT
fn CURRENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT, 0)
}
fn expression(&self) -> Option<Rc<ExpressionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> FrameBoundContextAttrs<'input> for FrameBoundContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn frameBound(&mut self,)
	-> Result<Rc<FrameBoundContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = FrameBoundContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 246, RULE_frameBound);
        let mut _localctx: Rc<FrameBoundContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2919);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(375,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(2912);
					recog.base.match_token(UNBOUNDED,&mut recog.err_handler)?;

					recog.base.set_state(2913);
					 cast_mut::<_,FrameBoundContext >(&mut _localctx).boundType = recog.base.input.lt(1).cloned();
					 
					_la = recog.base.input.la(1);
					if { !(_la==FOLLOWING || _la==PRECEDING) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						 cast_mut::<_,FrameBoundContext >(&mut _localctx).boundType = Some(tmp.clone());
						  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2914);
					let tmp = recog.base.match_token(CURRENT,&mut recog.err_handler)?;
					 cast_mut::<_,FrameBoundContext >(&mut _localctx).boundType = Some(tmp.clone());
					  

					recog.base.set_state(2915);
					recog.base.match_token(ROW,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					/*InvokeRule expression*/
					recog.base.set_state(2916);
					recog.expression()?;

					recog.base.set_state(2917);
					 cast_mut::<_,FrameBoundContext >(&mut _localctx).boundType = recog.base.input.lt(1).cloned();
					 
					_la = recog.base.input.la(1);
					if { !(_la==FOLLOWING || _la==PRECEDING) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						 cast_mut::<_,FrameBoundContext >(&mut _localctx).boundType = Some(tmp.clone());
						  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- qualifiedNameList ----------------
pub type QualifiedNameListContextAll<'input> = QualifiedNameListContext<'input>;


pub type QualifiedNameListContext<'input> = BaseParserRuleContext<'input,QualifiedNameListContextExt<'input>>;

#[derive(Clone)]
pub struct QualifiedNameListContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QualifiedNameListContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QualifiedNameListContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_qualifiedNameList(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_qualifiedNameList(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for QualifiedNameListContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_qualifiedNameList }
	//fn type_rule_index() -> usize where Self: Sized { RULE_qualifiedNameList }
}
crate::tid!{QualifiedNameListContextExt<'a>}

impl<'input> QualifiedNameListContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QualifiedNameListContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QualifiedNameListContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait QualifiedNameListContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QualifiedNameListContextExt<'input>>{

fn qualifiedName_all(&self) ->  Vec<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn qualifiedName(&self, i: usize) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> QualifiedNameListContextAttrs<'input> for QualifiedNameListContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn qualifiedNameList(&mut self,)
	-> Result<Rc<QualifiedNameListContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QualifiedNameListContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 248, RULE_qualifiedNameList);
        let mut _localctx: Rc<QualifiedNameListContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule qualifiedName*/
			recog.base.set_state(2921);
			recog.qualifiedName()?;

			recog.base.set_state(2926);
			recog.err_handler.sync(&mut recog.base)?;
			_la = recog.base.input.la(1);
			while _la==T__2 {
				{
				{
				recog.base.set_state(2922);
				recog.base.match_token(T__2,&mut recog.err_handler)?;

				/*InvokeRule qualifiedName*/
				recog.base.set_state(2923);
				recog.qualifiedName()?;

				}
				}
				recog.base.set_state(2928);
				recog.err_handler.sync(&mut recog.base)?;
				_la = recog.base.input.la(1);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- functionName ----------------
pub type FunctionNameContextAll<'input> = FunctionNameContext<'input>;


pub type FunctionNameContext<'input> = BaseParserRuleContext<'input,FunctionNameContextExt<'input>>;

#[derive(Clone)]
pub struct FunctionNameContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for FunctionNameContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FunctionNameContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_functionName(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_functionName(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for FunctionNameContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_functionName }
	//fn type_rule_index() -> usize where Self: Sized { RULE_functionName }
}
crate::tid!{FunctionNameContextExt<'a>}

impl<'input> FunctionNameContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<FunctionNameContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,FunctionNameContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait FunctionNameContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<FunctionNameContextExt<'input>>{

fn qualifiedName(&self) -> Option<Rc<QualifiedNameContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token FILTER
/// Returns `None` if there is no child corresponding to token FILTER
fn FILTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FILTER, 0)
}
/// Retrieves first TerminalNode corresponding to token LEFT
/// Returns `None` if there is no child corresponding to token LEFT
fn LEFT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LEFT, 0)
}
/// Retrieves first TerminalNode corresponding to token RIGHT
/// Returns `None` if there is no child corresponding to token RIGHT
fn RIGHT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RIGHT, 0)
}

}

impl<'input> FunctionNameContextAttrs<'input> for FunctionNameContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn functionName(&mut self,)
	-> Result<Rc<FunctionNameContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = FunctionNameContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 250, RULE_functionName);
        let mut _localctx: Rc<FunctionNameContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2933);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(377,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule qualifiedName*/
					recog.base.set_state(2929);
					recog.qualifiedName()?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2930);
					recog.base.match_token(FILTER,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					recog.base.set_state(2931);
					recog.base.match_token(LEFT,&mut recog.err_handler)?;

					}
				}
			,
				4 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					recog.base.set_state(2932);
					recog.base.match_token(RIGHT,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- qualifiedName ----------------
pub type QualifiedNameContextAll<'input> = QualifiedNameContext<'input>;


pub type QualifiedNameContext<'input> = BaseParserRuleContext<'input,QualifiedNameContextExt<'input>>;

#[derive(Clone)]
pub struct QualifiedNameContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QualifiedNameContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QualifiedNameContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_qualifiedName(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_qualifiedName(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for QualifiedNameContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_qualifiedName }
	//fn type_rule_index() -> usize where Self: Sized { RULE_qualifiedName }
}
crate::tid!{QualifiedNameContextExt<'a>}

impl<'input> QualifiedNameContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QualifiedNameContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QualifiedNameContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait QualifiedNameContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QualifiedNameContextExt<'input>>{

fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.children_of_type()
}
fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(i)
}

}

impl<'input> QualifiedNameContextAttrs<'input> for QualifiedNameContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn qualifiedName(&mut self,)
	-> Result<Rc<QualifiedNameContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QualifiedNameContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 252, RULE_qualifiedName);
        let mut _localctx: Rc<QualifiedNameContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule identifier*/
			recog.base.set_state(2935);
			recog.identifier()?;

			recog.base.set_state(2940);
			recog.err_handler.sync(&mut recog.base)?;
			_alt = recog.interpreter.adaptive_predict(378,&mut recog.base)?;
			while { _alt!=2 && _alt!=INVALID_ALT } {
				if _alt==1 {
					{
					{
					recog.base.set_state(2936);
					recog.base.match_token(T__3,&mut recog.err_handler)?;

					/*InvokeRule identifier*/
					recog.base.set_state(2937);
					recog.identifier()?;

					}
					} 
				}
				recog.base.set_state(2942);
				recog.err_handler.sync(&mut recog.base)?;
				_alt = recog.interpreter.adaptive_predict(378,&mut recog.base)?;
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- errorCapturingIdentifier ----------------
pub type ErrorCapturingIdentifierContextAll<'input> = ErrorCapturingIdentifierContext<'input>;


pub type ErrorCapturingIdentifierContext<'input> = BaseParserRuleContext<'input,ErrorCapturingIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct ErrorCapturingIdentifierContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ErrorCapturingIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ErrorCapturingIdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_errorCapturingIdentifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_errorCapturingIdentifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for ErrorCapturingIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_errorCapturingIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_errorCapturingIdentifier }
}
crate::tid!{ErrorCapturingIdentifierContextExt<'a>}

impl<'input> ErrorCapturingIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ErrorCapturingIdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ErrorCapturingIdentifierContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait ErrorCapturingIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ErrorCapturingIdentifierContextExt<'input>>{

fn identifier(&self) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn errorCapturingIdentifierExtra(&self) -> Option<Rc<ErrorCapturingIdentifierExtraContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> ErrorCapturingIdentifierContextAttrs<'input> for ErrorCapturingIdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn errorCapturingIdentifier(&mut self,)
	-> Result<Rc<ErrorCapturingIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ErrorCapturingIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 254, RULE_errorCapturingIdentifier);
        let mut _localctx: Rc<ErrorCapturingIdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			/*InvokeRule identifier*/
			recog.base.set_state(2943);
			recog.identifier()?;

			/*InvokeRule errorCapturingIdentifierExtra*/
			recog.base.set_state(2944);
			recog.errorCapturingIdentifierExtra()?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- errorCapturingIdentifierExtra ----------------
#[derive(Debug)]
pub enum ErrorCapturingIdentifierExtraContextAll<'input>{
	ErrorIdentContext(ErrorIdentContext<'input>),
	RealIdentContext(RealIdentContext<'input>),
Error(ErrorCapturingIdentifierExtraContext<'input>)
}
crate::tid!{ErrorCapturingIdentifierExtraContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for ErrorCapturingIdentifierExtraContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for ErrorCapturingIdentifierExtraContextAll<'input>{}

impl<'input> Deref for ErrorCapturingIdentifierExtraContextAll<'input>{
	type Target = dyn ErrorCapturingIdentifierExtraContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use ErrorCapturingIdentifierExtraContextAll::*;
		match self{
			ErrorIdentContext(inner) => inner,
			RealIdentContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ErrorCapturingIdentifierExtraContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type ErrorCapturingIdentifierExtraContext<'input> = BaseParserRuleContext<'input,ErrorCapturingIdentifierExtraContextExt<'input>>;

#[derive(Clone)]
pub struct ErrorCapturingIdentifierExtraContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for ErrorCapturingIdentifierExtraContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ErrorCapturingIdentifierExtraContext<'input>{
}

impl<'input> CustomRuleContext<'input> for ErrorCapturingIdentifierExtraContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_errorCapturingIdentifierExtra }
	//fn type_rule_index() -> usize where Self: Sized { RULE_errorCapturingIdentifierExtra }
}
crate::tid!{ErrorCapturingIdentifierExtraContextExt<'a>}

impl<'input> ErrorCapturingIdentifierExtraContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<ErrorCapturingIdentifierExtraContextAll<'input>> {
		Rc::new(
		ErrorCapturingIdentifierExtraContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,ErrorCapturingIdentifierExtraContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait ErrorCapturingIdentifierExtraContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<ErrorCapturingIdentifierExtraContextExt<'input>>{


}

impl<'input> ErrorCapturingIdentifierExtraContextAttrs<'input> for ErrorCapturingIdentifierExtraContext<'input>{}

pub type ErrorIdentContext<'input> = BaseParserRuleContext<'input,ErrorIdentContextExt<'input>>;

pub trait ErrorIdentContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves all `TerminalNode`s corresponding to token MINUS in current rule
	fn MINUS_all(&self) -> Vec<Rc<TerminalNode<'input,SparkSqlParserContextType>>>  where Self:Sized{
		self.children_of_type()
	}
	/// Retrieves 'i's TerminalNode corresponding to token MINUS, starting from 0.
	/// Returns `None` if number of children corresponding to token MINUS is less or equal than `i`.
	fn MINUS(&self, i: usize) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, i)
	}
	fn identifier_all(&self) ->  Vec<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.children_of_type()
	}
	fn identifier(&self, i: usize) -> Option<Rc<IdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(i)
	}
}

impl<'input> ErrorIdentContextAttrs<'input> for ErrorIdentContext<'input>{}

pub struct ErrorIdentContextExt<'input>{
	base:ErrorCapturingIdentifierExtraContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ErrorIdentContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ErrorIdentContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ErrorIdentContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_errorIdent(self);
	}
}

impl<'input> CustomRuleContext<'input> for ErrorIdentContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_errorCapturingIdentifierExtra }
	//fn type_rule_index() -> usize where Self: Sized { RULE_errorCapturingIdentifierExtra }
}

impl<'input> Borrow<ErrorCapturingIdentifierExtraContextExt<'input>> for ErrorIdentContext<'input>{
	fn borrow(&self) -> &ErrorCapturingIdentifierExtraContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ErrorCapturingIdentifierExtraContextExt<'input>> for ErrorIdentContext<'input>{
	fn borrow_mut(&mut self) -> &mut ErrorCapturingIdentifierExtraContextExt<'input> { &mut self.base }
}

impl<'input> ErrorCapturingIdentifierExtraContextAttrs<'input> for ErrorIdentContext<'input> {}

impl<'input> ErrorIdentContextExt<'input>{
	fn new(ctx: &dyn ErrorCapturingIdentifierExtraContextAttrs<'input>) -> Rc<ErrorCapturingIdentifierExtraContextAll<'input>>  {
		Rc::new(
			ErrorCapturingIdentifierExtraContextAll::ErrorIdentContext(
				BaseParserRuleContext::copy_from(ctx,ErrorIdentContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type RealIdentContext<'input> = BaseParserRuleContext<'input,RealIdentContextExt<'input>>;

pub trait RealIdentContextAttrs<'input>: SparkSqlParserContext<'input>{
}

impl<'input> RealIdentContextAttrs<'input> for RealIdentContext<'input>{}

pub struct RealIdentContextExt<'input>{
	base:ErrorCapturingIdentifierExtraContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{RealIdentContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for RealIdentContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for RealIdentContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_realIdent(self);
	}
}

impl<'input> CustomRuleContext<'input> for RealIdentContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_errorCapturingIdentifierExtra }
	//fn type_rule_index() -> usize where Self: Sized { RULE_errorCapturingIdentifierExtra }
}

impl<'input> Borrow<ErrorCapturingIdentifierExtraContextExt<'input>> for RealIdentContext<'input>{
	fn borrow(&self) -> &ErrorCapturingIdentifierExtraContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<ErrorCapturingIdentifierExtraContextExt<'input>> for RealIdentContext<'input>{
	fn borrow_mut(&mut self) -> &mut ErrorCapturingIdentifierExtraContextExt<'input> { &mut self.base }
}

impl<'input> ErrorCapturingIdentifierExtraContextAttrs<'input> for RealIdentContext<'input> {}

impl<'input> RealIdentContextExt<'input>{
	fn new(ctx: &dyn ErrorCapturingIdentifierExtraContextAttrs<'input>) -> Rc<ErrorCapturingIdentifierExtraContextAll<'input>>  {
		Rc::new(
			ErrorCapturingIdentifierExtraContextAll::RealIdentContext(
				BaseParserRuleContext::copy_from(ctx,RealIdentContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn errorCapturingIdentifierExtra(&mut self,)
	-> Result<Rc<ErrorCapturingIdentifierExtraContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = ErrorCapturingIdentifierExtraContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 256, RULE_errorCapturingIdentifierExtra);
        let mut _localctx: Rc<ErrorCapturingIdentifierExtraContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			let mut _alt: isize;
			recog.base.set_state(2953);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(380,&mut recog.base)? {
				1 =>{
					let tmp = ErrorIdentContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(2948); 
					recog.err_handler.sync(&mut recog.base)?;
					_alt = 1;
					loop {
						match _alt {
						    x if x == 1=>
							{
							{
							recog.base.set_state(2946);
							recog.base.match_token(MINUS,&mut recog.err_handler)?;

							/*InvokeRule identifier*/
							recog.base.set_state(2947);
							recog.identifier()?;

							}
							}

						_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
						}
						recog.base.set_state(2950); 
						recog.err_handler.sync(&mut recog.base)?;
						_alt = recog.interpreter.adaptive_predict(379,&mut recog.base)?;
						if _alt==2 || _alt==INVALID_ALT { break }
					}
					}
				}
			,
				2 =>{
					let tmp = RealIdentContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- identifier ----------------
pub type IdentifierContextAll<'input> = IdentifierContext<'input>;


pub type IdentifierContext<'input> = BaseParserRuleContext<'input,IdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct IdentifierContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for IdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_identifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_identifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for IdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_identifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_identifier }
}
crate::tid!{IdentifierContextExt<'a>}

impl<'input> IdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<IdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,IdentifierContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait IdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<IdentifierContextExt<'input>>{

fn strictIdentifier(&self) -> Option<Rc<StrictIdentifierContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn strictNonReserved(&self) -> Option<Rc<StrictNonReservedContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}

}

impl<'input> IdentifierContextAttrs<'input> for IdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn identifier(&mut self,)
	-> Result<Rc<IdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = IdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 258, RULE_identifier);
        let mut _localctx: Rc<IdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2958);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(381,&mut recog.base)? {
				1 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					/*InvokeRule strictIdentifier*/
					recog.base.set_state(2955);
					recog.strictIdentifier()?;

					}
				}
			,
				2 =>{
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					recog.base.set_state(2956);
					if !({!recog.SQL_standard_keyword_behavior}) {
						Err(FailedPredicateError::new(&mut recog.base, Some("!recog.,SQL_standard_keyword_behavior".to_owned()), None))?;
					}
					/*InvokeRule strictNonReserved*/
					recog.base.set_state(2957);
					recog.strictNonReserved()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- strictIdentifier ----------------
#[derive(Debug)]
pub enum StrictIdentifierContextAll<'input>{
	QuotedIdentifierAlternativeContext(QuotedIdentifierAlternativeContext<'input>),
	UnquotedIdentifierContext(UnquotedIdentifierContext<'input>),
Error(StrictIdentifierContext<'input>)
}
crate::tid!{StrictIdentifierContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for StrictIdentifierContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for StrictIdentifierContextAll<'input>{}

impl<'input> Deref for StrictIdentifierContextAll<'input>{
	type Target = dyn StrictIdentifierContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use StrictIdentifierContextAll::*;
		match self{
			QuotedIdentifierAlternativeContext(inner) => inner,
			UnquotedIdentifierContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StrictIdentifierContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type StrictIdentifierContext<'input> = BaseParserRuleContext<'input,StrictIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct StrictIdentifierContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for StrictIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StrictIdentifierContext<'input>{
}

impl<'input> CustomRuleContext<'input> for StrictIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_strictIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_strictIdentifier }
}
crate::tid!{StrictIdentifierContextExt<'a>}

impl<'input> StrictIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<StrictIdentifierContextAll<'input>> {
		Rc::new(
		StrictIdentifierContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,StrictIdentifierContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait StrictIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<StrictIdentifierContextExt<'input>>{


}

impl<'input> StrictIdentifierContextAttrs<'input> for StrictIdentifierContext<'input>{}

pub type QuotedIdentifierAlternativeContext<'input> = BaseParserRuleContext<'input,QuotedIdentifierAlternativeContextExt<'input>>;

pub trait QuotedIdentifierAlternativeContextAttrs<'input>: SparkSqlParserContext<'input>{
	fn quotedIdentifier(&self) -> Option<Rc<QuotedIdentifierContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> QuotedIdentifierAlternativeContextAttrs<'input> for QuotedIdentifierAlternativeContext<'input>{}

pub struct QuotedIdentifierAlternativeContextExt<'input>{
	base:StrictIdentifierContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{QuotedIdentifierAlternativeContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for QuotedIdentifierAlternativeContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QuotedIdentifierAlternativeContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_quotedIdentifierAlternative(self);
	}
}

impl<'input> CustomRuleContext<'input> for QuotedIdentifierAlternativeContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_strictIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_strictIdentifier }
}

impl<'input> Borrow<StrictIdentifierContextExt<'input>> for QuotedIdentifierAlternativeContext<'input>{
	fn borrow(&self) -> &StrictIdentifierContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StrictIdentifierContextExt<'input>> for QuotedIdentifierAlternativeContext<'input>{
	fn borrow_mut(&mut self) -> &mut StrictIdentifierContextExt<'input> { &mut self.base }
}

impl<'input> StrictIdentifierContextAttrs<'input> for QuotedIdentifierAlternativeContext<'input> {}

impl<'input> QuotedIdentifierAlternativeContextExt<'input>{
	fn new(ctx: &dyn StrictIdentifierContextAttrs<'input>) -> Rc<StrictIdentifierContextAll<'input>>  {
		Rc::new(
			StrictIdentifierContextAll::QuotedIdentifierAlternativeContext(
				BaseParserRuleContext::copy_from(ctx,QuotedIdentifierAlternativeContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type UnquotedIdentifierContext<'input> = BaseParserRuleContext<'input,UnquotedIdentifierContextExt<'input>>;

pub trait UnquotedIdentifierContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token IDENTIFIER
	/// Returns `None` if there is no child corresponding to token IDENTIFIER
	fn IDENTIFIER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(IDENTIFIER, 0)
	}
	fn ansiNonReserved(&self) -> Option<Rc<AnsiNonReservedContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
	fn nonReserved(&self) -> Option<Rc<NonReservedContextAll<'input>>> where Self:Sized{
		self.child_of_type(0)
	}
}

impl<'input> UnquotedIdentifierContextAttrs<'input> for UnquotedIdentifierContext<'input>{}

pub struct UnquotedIdentifierContextExt<'input>{
	base:StrictIdentifierContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{UnquotedIdentifierContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for UnquotedIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for UnquotedIdentifierContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_unquotedIdentifier(self);
	}
}

impl<'input> CustomRuleContext<'input> for UnquotedIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_strictIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_strictIdentifier }
}

impl<'input> Borrow<StrictIdentifierContextExt<'input>> for UnquotedIdentifierContext<'input>{
	fn borrow(&self) -> &StrictIdentifierContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<StrictIdentifierContextExt<'input>> for UnquotedIdentifierContext<'input>{
	fn borrow_mut(&mut self) -> &mut StrictIdentifierContextExt<'input> { &mut self.base }
}

impl<'input> StrictIdentifierContextAttrs<'input> for UnquotedIdentifierContext<'input> {}

impl<'input> UnquotedIdentifierContextExt<'input>{
	fn new(ctx: &dyn StrictIdentifierContextAttrs<'input>) -> Rc<StrictIdentifierContextAll<'input>>  {
		Rc::new(
			StrictIdentifierContextAll::UnquotedIdentifierContext(
				BaseParserRuleContext::copy_from(ctx,UnquotedIdentifierContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn strictIdentifier(&mut self,)
	-> Result<Rc<StrictIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = StrictIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 260, RULE_strictIdentifier);
        let mut _localctx: Rc<StrictIdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(2966);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(382,&mut recog.base)? {
				1 =>{
					let tmp = UnquotedIdentifierContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(2960);
					recog.base.match_token(IDENTIFIER,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					let tmp = QuotedIdentifierAlternativeContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					/*InvokeRule quotedIdentifier*/
					recog.base.set_state(2961);
					recog.quotedIdentifier()?;

					}
				}
			,
				3 =>{
					let tmp = UnquotedIdentifierContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(2962);
					if !({recog.SQL_standard_keyword_behavior}) {
						Err(FailedPredicateError::new(&mut recog.base, Some("recog.,SQL_standard_keyword_behavior".to_owned()), None))?;
					}
					/*InvokeRule ansiNonReserved*/
					recog.base.set_state(2963);
					recog.ansiNonReserved()?;

					}
				}
			,
				4 =>{
					let tmp = UnquotedIdentifierContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					recog.base.set_state(2964);
					if !({!recog.SQL_standard_keyword_behavior}) {
						Err(FailedPredicateError::new(&mut recog.base, Some("!recog.,SQL_standard_keyword_behavior".to_owned()), None))?;
					}
					/*InvokeRule nonReserved*/
					recog.base.set_state(2965);
					recog.nonReserved()?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- quotedIdentifier ----------------
pub type QuotedIdentifierContextAll<'input> = QuotedIdentifierContext<'input>;


pub type QuotedIdentifierContext<'input> = BaseParserRuleContext<'input,QuotedIdentifierContextExt<'input>>;

#[derive(Clone)]
pub struct QuotedIdentifierContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for QuotedIdentifierContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for QuotedIdentifierContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_quotedIdentifier(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_quotedIdentifier(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for QuotedIdentifierContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_quotedIdentifier }
	//fn type_rule_index() -> usize where Self: Sized { RULE_quotedIdentifier }
}
crate::tid!{QuotedIdentifierContextExt<'a>}

impl<'input> QuotedIdentifierContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<QuotedIdentifierContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,QuotedIdentifierContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait QuotedIdentifierContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<QuotedIdentifierContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token BACKQUOTED_IDENTIFIER
/// Returns `None` if there is no child corresponding to token BACKQUOTED_IDENTIFIER
fn BACKQUOTED_IDENTIFIER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BACKQUOTED_IDENTIFIER, 0)
}

}

impl<'input> QuotedIdentifierContextAttrs<'input> for QuotedIdentifierContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn quotedIdentifier(&mut self,)
	-> Result<Rc<QuotedIdentifierContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = QuotedIdentifierContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 262, RULE_quotedIdentifier);
        let mut _localctx: Rc<QuotedIdentifierContextAll> = _localctx;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(2968);
			recog.base.match_token(BACKQUOTED_IDENTIFIER,&mut recog.err_handler)?;

			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- number ----------------
#[derive(Debug)]
pub enum NumberContextAll<'input>{
	DecimalLiteralContext(DecimalLiteralContext<'input>),
	BigIntLiteralContext(BigIntLiteralContext<'input>),
	TinyIntLiteralContext(TinyIntLiteralContext<'input>),
	LegacyDecimalLiteralContext(LegacyDecimalLiteralContext<'input>),
	BigDecimalLiteralContext(BigDecimalLiteralContext<'input>),
	ExponentLiteralContext(ExponentLiteralContext<'input>),
	DoubleLiteralContext(DoubleLiteralContext<'input>),
	IntegerLiteralContext(IntegerLiteralContext<'input>),
	FloatLiteralContext(FloatLiteralContext<'input>),
	SmallIntLiteralContext(SmallIntLiteralContext<'input>),
Error(NumberContext<'input>)
}
crate::tid!{NumberContextAll<'a>}

impl<'input> crate::parser_rule_context::DerefSeal for NumberContextAll<'input>{}

impl<'input> SparkSqlParserContext<'input> for NumberContextAll<'input>{}

impl<'input> Deref for NumberContextAll<'input>{
	type Target = dyn NumberContextAttrs<'input> + 'input;
	fn deref(&self) -> &Self::Target{
		use NumberContextAll::*;
		match self{
			DecimalLiteralContext(inner) => inner,
			BigIntLiteralContext(inner) => inner,
			TinyIntLiteralContext(inner) => inner,
			LegacyDecimalLiteralContext(inner) => inner,
			BigDecimalLiteralContext(inner) => inner,
			ExponentLiteralContext(inner) => inner,
			DoubleLiteralContext(inner) => inner,
			IntegerLiteralContext(inner) => inner,
			FloatLiteralContext(inner) => inner,
			SmallIntLiteralContext(inner) => inner,
Error(inner) => inner
		}
	}
}
impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NumberContextAll<'input>{
    fn enter(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().enter(listener) }
    fn exit(&self, listener: &mut (dyn SparkSqlListener<'input> + 'a)) { self.deref().exit(listener) }
}



pub type NumberContext<'input> = BaseParserRuleContext<'input,NumberContextExt<'input>>;

#[derive(Clone)]
pub struct NumberContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NumberContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NumberContext<'input>{
}

impl<'input> CustomRuleContext<'input> for NumberContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}
crate::tid!{NumberContextExt<'a>}

impl<'input> NumberContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NumberContextAll<'input>> {
		Rc::new(
		NumberContextAll::Error(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NumberContextExt{
				ph:PhantomData
			}),
		)
		)
	}
}

pub trait NumberContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NumberContextExt<'input>>{


}

impl<'input> NumberContextAttrs<'input> for NumberContext<'input>{}

pub type DecimalLiteralContext<'input> = BaseParserRuleContext<'input,DecimalLiteralContextExt<'input>>;

pub trait DecimalLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token DECIMAL_VALUE
	/// Returns `None` if there is no child corresponding to token DECIMAL_VALUE
	fn DECIMAL_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DECIMAL_VALUE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> DecimalLiteralContextAttrs<'input> for DecimalLiteralContext<'input>{}

pub struct DecimalLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DecimalLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DecimalLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DecimalLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_decimalLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for DecimalLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for DecimalLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for DecimalLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for DecimalLiteralContext<'input> {}

impl<'input> DecimalLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::DecimalLiteralContext(
				BaseParserRuleContext::copy_from(ctx,DecimalLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type BigIntLiteralContext<'input> = BaseParserRuleContext<'input,BigIntLiteralContextExt<'input>>;

pub trait BigIntLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token BIGINT_LITERAL
	/// Returns `None` if there is no child corresponding to token BIGINT_LITERAL
	fn BIGINT_LITERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(BIGINT_LITERAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> BigIntLiteralContextAttrs<'input> for BigIntLiteralContext<'input>{}

pub struct BigIntLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{BigIntLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for BigIntLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for BigIntLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_bigIntLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for BigIntLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for BigIntLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for BigIntLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for BigIntLiteralContext<'input> {}

impl<'input> BigIntLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::BigIntLiteralContext(
				BaseParserRuleContext::copy_from(ctx,BigIntLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type TinyIntLiteralContext<'input> = BaseParserRuleContext<'input,TinyIntLiteralContextExt<'input>>;

pub trait TinyIntLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token TINYINT_LITERAL
	/// Returns `None` if there is no child corresponding to token TINYINT_LITERAL
	fn TINYINT_LITERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(TINYINT_LITERAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> TinyIntLiteralContextAttrs<'input> for TinyIntLiteralContext<'input>{}

pub struct TinyIntLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{TinyIntLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for TinyIntLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for TinyIntLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_tinyIntLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for TinyIntLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for TinyIntLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for TinyIntLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for TinyIntLiteralContext<'input> {}

impl<'input> TinyIntLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::TinyIntLiteralContext(
				BaseParserRuleContext::copy_from(ctx,TinyIntLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type LegacyDecimalLiteralContext<'input> = BaseParserRuleContext<'input,LegacyDecimalLiteralContextExt<'input>>;

pub trait LegacyDecimalLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token EXPONENT_VALUE
	/// Returns `None` if there is no child corresponding to token EXPONENT_VALUE
	fn EXPONENT_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXPONENT_VALUE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token DECIMAL_VALUE
	/// Returns `None` if there is no child corresponding to token DECIMAL_VALUE
	fn DECIMAL_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DECIMAL_VALUE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> LegacyDecimalLiteralContextAttrs<'input> for LegacyDecimalLiteralContext<'input>{}

pub struct LegacyDecimalLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{LegacyDecimalLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for LegacyDecimalLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for LegacyDecimalLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_legacyDecimalLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for LegacyDecimalLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for LegacyDecimalLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for LegacyDecimalLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for LegacyDecimalLiteralContext<'input> {}

impl<'input> LegacyDecimalLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::LegacyDecimalLiteralContext(
				BaseParserRuleContext::copy_from(ctx,LegacyDecimalLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type BigDecimalLiteralContext<'input> = BaseParserRuleContext<'input,BigDecimalLiteralContextExt<'input>>;

pub trait BigDecimalLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token BIGDECIMAL_LITERAL
	/// Returns `None` if there is no child corresponding to token BIGDECIMAL_LITERAL
	fn BIGDECIMAL_LITERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(BIGDECIMAL_LITERAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> BigDecimalLiteralContextAttrs<'input> for BigDecimalLiteralContext<'input>{}

pub struct BigDecimalLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{BigDecimalLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for BigDecimalLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for BigDecimalLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_bigDecimalLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for BigDecimalLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for BigDecimalLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for BigDecimalLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for BigDecimalLiteralContext<'input> {}

impl<'input> BigDecimalLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::BigDecimalLiteralContext(
				BaseParserRuleContext::copy_from(ctx,BigDecimalLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type ExponentLiteralContext<'input> = BaseParserRuleContext<'input,ExponentLiteralContextExt<'input>>;

pub trait ExponentLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token EXPONENT_VALUE
	/// Returns `None` if there is no child corresponding to token EXPONENT_VALUE
	fn EXPONENT_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(EXPONENT_VALUE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> ExponentLiteralContextAttrs<'input> for ExponentLiteralContext<'input>{}

pub struct ExponentLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{ExponentLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for ExponentLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for ExponentLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_exponentLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for ExponentLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for ExponentLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for ExponentLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for ExponentLiteralContext<'input> {}

impl<'input> ExponentLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::ExponentLiteralContext(
				BaseParserRuleContext::copy_from(ctx,ExponentLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type DoubleLiteralContext<'input> = BaseParserRuleContext<'input,DoubleLiteralContextExt<'input>>;

pub trait DoubleLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token DOUBLE_LITERAL
	/// Returns `None` if there is no child corresponding to token DOUBLE_LITERAL
	fn DOUBLE_LITERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(DOUBLE_LITERAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> DoubleLiteralContextAttrs<'input> for DoubleLiteralContext<'input>{}

pub struct DoubleLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{DoubleLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for DoubleLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for DoubleLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_doubleLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for DoubleLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for DoubleLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for DoubleLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for DoubleLiteralContext<'input> {}

impl<'input> DoubleLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::DoubleLiteralContext(
				BaseParserRuleContext::copy_from(ctx,DoubleLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type IntegerLiteralContext<'input> = BaseParserRuleContext<'input,IntegerLiteralContextExt<'input>>;

pub trait IntegerLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token INTEGER_VALUE
	/// Returns `None` if there is no child corresponding to token INTEGER_VALUE
	fn INTEGER_VALUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(INTEGER_VALUE, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> IntegerLiteralContextAttrs<'input> for IntegerLiteralContext<'input>{}

pub struct IntegerLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{IntegerLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for IntegerLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for IntegerLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_integerLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for IntegerLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for IntegerLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for IntegerLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for IntegerLiteralContext<'input> {}

impl<'input> IntegerLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::IntegerLiteralContext(
				BaseParserRuleContext::copy_from(ctx,IntegerLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type FloatLiteralContext<'input> = BaseParserRuleContext<'input,FloatLiteralContextExt<'input>>;

pub trait FloatLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token FLOAT_LITERAL
	/// Returns `None` if there is no child corresponding to token FLOAT_LITERAL
	fn FLOAT_LITERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(FLOAT_LITERAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> FloatLiteralContextAttrs<'input> for FloatLiteralContext<'input>{}

pub struct FloatLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{FloatLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for FloatLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for FloatLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_floatLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for FloatLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for FloatLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for FloatLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for FloatLiteralContext<'input> {}

impl<'input> FloatLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::FloatLiteralContext(
				BaseParserRuleContext::copy_from(ctx,FloatLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

pub type SmallIntLiteralContext<'input> = BaseParserRuleContext<'input,SmallIntLiteralContextExt<'input>>;

pub trait SmallIntLiteralContextAttrs<'input>: SparkSqlParserContext<'input>{
	/// Retrieves first TerminalNode corresponding to token SMALLINT_LITERAL
	/// Returns `None` if there is no child corresponding to token SMALLINT_LITERAL
	fn SMALLINT_LITERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(SMALLINT_LITERAL, 0)
	}
	/// Retrieves first TerminalNode corresponding to token MINUS
	/// Returns `None` if there is no child corresponding to token MINUS
	fn MINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
		self.get_token(MINUS, 0)
	}
}

impl<'input> SmallIntLiteralContextAttrs<'input> for SmallIntLiteralContext<'input>{}

pub struct SmallIntLiteralContextExt<'input>{
	base:NumberContextExt<'input>,
	ph:PhantomData<&'input str>
}

crate::tid!{SmallIntLiteralContextExt<'a>}

impl<'input> SparkSqlParserContext<'input> for SmallIntLiteralContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for SmallIntLiteralContext<'input>{
	fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
		listener.enter_every_rule(self);
		listener.enter_smallIntLiteral(self);
	}
}

impl<'input> CustomRuleContext<'input> for SmallIntLiteralContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_number }
	//fn type_rule_index() -> usize where Self: Sized { RULE_number }
}

impl<'input> Borrow<NumberContextExt<'input>> for SmallIntLiteralContext<'input>{
	fn borrow(&self) -> &NumberContextExt<'input> { &self.base }
}
impl<'input> BorrowMut<NumberContextExt<'input>> for SmallIntLiteralContext<'input>{
	fn borrow_mut(&mut self) -> &mut NumberContextExt<'input> { &mut self.base }
}

impl<'input> NumberContextAttrs<'input> for SmallIntLiteralContext<'input> {}

impl<'input> SmallIntLiteralContextExt<'input>{
	fn new(ctx: &dyn NumberContextAttrs<'input>) -> Rc<NumberContextAll<'input>>  {
		Rc::new(
			NumberContextAll::SmallIntLiteralContext(
				BaseParserRuleContext::copy_from(ctx,SmallIntLiteralContextExt{
        			base: ctx.borrow().clone(),
        			ph:PhantomData
				})
			)
		)
	}
}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn number(&mut self,)
	-> Result<Rc<NumberContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NumberContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 264, RULE_number);
        let mut _localctx: Rc<NumberContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(3013);
			recog.err_handler.sync(&mut recog.base)?;
			match  recog.interpreter.adaptive_predict(393,&mut recog.base)? {
				1 =>{
					let tmp = ExponentLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 1);
					_localctx = tmp;
					{
					recog.base.set_state(2970);
					if !({!recog.legacy_exponent_literal_as_decimal_enabled}) {
						Err(FailedPredicateError::new(&mut recog.base, Some("!recog.,legacy_exponent_literal_as_decimal_enabled".to_owned()), None))?;
					}
					recog.base.set_state(2972);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2971);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2974);
					recog.base.match_token(EXPONENT_VALUE,&mut recog.err_handler)?;

					}
				}
			,
				2 =>{
					let tmp = DecimalLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 2);
					_localctx = tmp;
					{
					recog.base.set_state(2975);
					if !({!recog.legacy_exponent_literal_as_decimal_enabled}) {
						Err(FailedPredicateError::new(&mut recog.base, Some("!recog.,legacy_exponent_literal_as_decimal_enabled".to_owned()), None))?;
					}
					recog.base.set_state(2977);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2976);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2979);
					recog.base.match_token(DECIMAL_VALUE,&mut recog.err_handler)?;

					}
				}
			,
				3 =>{
					let tmp = LegacyDecimalLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 3);
					_localctx = tmp;
					{
					recog.base.set_state(2980);
					if !({recog.legacy_exponent_literal_as_decimal_enabled}) {
						Err(FailedPredicateError::new(&mut recog.base, Some("recog.,legacy_exponent_literal_as_decimal_enabled".to_owned()), None))?;
					}
					recog.base.set_state(2982);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2981);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2984);
					_la = recog.base.input.la(1);
					if { !(_la==EXPONENT_VALUE || _la==DECIMAL_VALUE) } {
						recog.err_handler.recover_inline(&mut recog.base)?;

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					}
				}
			,
				4 =>{
					let tmp = IntegerLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 4);
					_localctx = tmp;
					{
					recog.base.set_state(2986);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2985);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2988);
					recog.base.match_token(INTEGER_VALUE,&mut recog.err_handler)?;

					}
				}
			,
				5 =>{
					let tmp = BigIntLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 5);
					_localctx = tmp;
					{
					recog.base.set_state(2990);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2989);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2992);
					recog.base.match_token(BIGINT_LITERAL,&mut recog.err_handler)?;

					}
				}
			,
				6 =>{
					let tmp = SmallIntLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 6);
					_localctx = tmp;
					{
					recog.base.set_state(2994);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2993);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(2996);
					recog.base.match_token(SMALLINT_LITERAL,&mut recog.err_handler)?;

					}
				}
			,
				7 =>{
					let tmp = TinyIntLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 7);
					_localctx = tmp;
					{
					recog.base.set_state(2998);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(2997);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(3000);
					recog.base.match_token(TINYINT_LITERAL,&mut recog.err_handler)?;

					}
				}
			,
				8 =>{
					let tmp = DoubleLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 8);
					_localctx = tmp;
					{
					recog.base.set_state(3002);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(3001);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(3004);
					recog.base.match_token(DOUBLE_LITERAL,&mut recog.err_handler)?;

					}
				}
			,
				9 =>{
					let tmp = FloatLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 9);
					_localctx = tmp;
					{
					recog.base.set_state(3006);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(3005);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(3008);
					recog.base.match_token(FLOAT_LITERAL,&mut recog.err_handler)?;

					}
				}
			,
				10 =>{
					let tmp = BigDecimalLiteralContextExt::new(&**_localctx);
					recog.base.enter_outer_alt(Some(tmp.clone()), 10);
					_localctx = tmp;
					{
					recog.base.set_state(3010);
					recog.err_handler.sync(&mut recog.base)?;
					_la = recog.base.input.la(1);
					if _la==MINUS {
						{
						recog.base.set_state(3009);
						recog.base.match_token(MINUS,&mut recog.err_handler)?;

						}
					}

					recog.base.set_state(3012);
					recog.base.match_token(BIGDECIMAL_LITERAL,&mut recog.err_handler)?;

					}
				}

				_ => {}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- alterColumnAction ----------------
pub type AlterColumnActionContextAll<'input> = AlterColumnActionContext<'input>;


pub type AlterColumnActionContext<'input> = BaseParserRuleContext<'input,AlterColumnActionContextExt<'input>>;

#[derive(Clone)]
pub struct AlterColumnActionContextExt<'input>{
	pub setOrDrop: Option<TokenType<'input>>,
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for AlterColumnActionContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AlterColumnActionContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_alterColumnAction(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_alterColumnAction(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for AlterColumnActionContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_alterColumnAction }
	//fn type_rule_index() -> usize where Self: Sized { RULE_alterColumnAction }
}
crate::tid!{AlterColumnActionContextExt<'a>}

impl<'input> AlterColumnActionContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<AlterColumnActionContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,AlterColumnActionContextExt{
				setOrDrop: None, 
				ph:PhantomData
			}),
		)
	}
}

pub trait AlterColumnActionContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<AlterColumnActionContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token TYPE
/// Returns `None` if there is no child corresponding to token TYPE
fn TYPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TYPE, 0)
}
fn dataType(&self) -> Option<Rc<DataTypeContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn commentSpec(&self) -> Option<Rc<CommentSpecContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
fn colPosition(&self) -> Option<Rc<ColPositionContextAll<'input>>> where Self:Sized{
	self.child_of_type(0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token NULL
/// Returns `None` if there is no child corresponding to token NULL
fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULL, 0)
}
/// Retrieves first TerminalNode corresponding to token SET
/// Returns `None` if there is no child corresponding to token SET
fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SET, 0)
}
/// Retrieves first TerminalNode corresponding to token DROP
/// Returns `None` if there is no child corresponding to token DROP
fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DROP, 0)
}

}

impl<'input> AlterColumnActionContextAttrs<'input> for AlterColumnActionContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn alterColumnAction(&mut self,)
	-> Result<Rc<AlterColumnActionContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = AlterColumnActionContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 266, RULE_alterColumnAction);
        let mut _localctx: Rc<AlterColumnActionContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			recog.base.set_state(3022);
			recog.err_handler.sync(&mut recog.base)?;
			match recog.base.input.la(1) {
			 TYPE 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 1);
					recog.base.enter_outer_alt(None, 1);
					{
					recog.base.set_state(3015);
					recog.base.match_token(TYPE,&mut recog.err_handler)?;

					/*InvokeRule dataType*/
					recog.base.set_state(3016);
					recog.dataType()?;

					}
				}

			 COMMENT 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 2);
					recog.base.enter_outer_alt(None, 2);
					{
					/*InvokeRule commentSpec*/
					recog.base.set_state(3017);
					recog.commentSpec()?;

					}
				}

			 AFTER | FIRST 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 3);
					recog.base.enter_outer_alt(None, 3);
					{
					/*InvokeRule colPosition*/
					recog.base.set_state(3018);
					recog.colPosition()?;

					}
				}

			 DROP | SET 
				=> {
					//recog.base.enter_outer_alt(_localctx.clone(), 4);
					recog.base.enter_outer_alt(None, 4);
					{
					recog.base.set_state(3019);
					 cast_mut::<_,AlterColumnActionContext >(&mut _localctx).setOrDrop = recog.base.input.lt(1).cloned();
					 
					_la = recog.base.input.la(1);
					if { !(_la==DROP || _la==SET) } {
						let tmp = recog.err_handler.recover_inline(&mut recog.base)?;
						 cast_mut::<_,AlterColumnActionContext >(&mut _localctx).setOrDrop = Some(tmp.clone());
						  

					}
					else {
						if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
						recog.err_handler.report_match(&mut recog.base);
						recog.base.consume(&mut recog.err_handler);
					}
					recog.base.set_state(3020);
					recog.base.match_token(NOT,&mut recog.err_handler)?;

					recog.base.set_state(3021);
					recog.base.match_token(NULL,&mut recog.err_handler)?;

					}
				}

				_ => Err(ANTLRError::NoAltError(NoViableAltError::new(&mut recog.base)))?
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- ansiNonReserved ----------------
pub type AnsiNonReservedContextAll<'input> = AnsiNonReservedContext<'input>;


pub type AnsiNonReservedContext<'input> = BaseParserRuleContext<'input,AnsiNonReservedContextExt<'input>>;

#[derive(Clone)]
pub struct AnsiNonReservedContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for AnsiNonReservedContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for AnsiNonReservedContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_ansiNonReserved(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_ansiNonReserved(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for AnsiNonReservedContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_ansiNonReserved }
	//fn type_rule_index() -> usize where Self: Sized { RULE_ansiNonReserved }
}
crate::tid!{AnsiNonReservedContextExt<'a>}

impl<'input> AnsiNonReservedContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<AnsiNonReservedContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,AnsiNonReservedContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait AnsiNonReservedContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<AnsiNonReservedContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token ADD
/// Returns `None` if there is no child corresponding to token ADD
fn ADD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ADD, 0)
}
/// Retrieves first TerminalNode corresponding to token AFTER
/// Returns `None` if there is no child corresponding to token AFTER
fn AFTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AFTER, 0)
}
/// Retrieves first TerminalNode corresponding to token ALTER
/// Returns `None` if there is no child corresponding to token ALTER
fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ALTER, 0)
}
/// Retrieves first TerminalNode corresponding to token ANALYZE
/// Returns `None` if there is no child corresponding to token ANALYZE
fn ANALYZE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ANALYZE, 0)
}
/// Retrieves first TerminalNode corresponding to token ANTI
/// Returns `None` if there is no child corresponding to token ANTI
fn ANTI(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ANTI, 0)
}
/// Retrieves first TerminalNode corresponding to token ARCHIVE
/// Returns `None` if there is no child corresponding to token ARCHIVE
fn ARCHIVE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ARCHIVE, 0)
}
/// Retrieves first TerminalNode corresponding to token ARRAY
/// Returns `None` if there is no child corresponding to token ARRAY
fn ARRAY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ARRAY, 0)
}
/// Retrieves first TerminalNode corresponding to token ASC
/// Returns `None` if there is no child corresponding to token ASC
fn ASC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ASC, 0)
}
/// Retrieves first TerminalNode corresponding to token AT
/// Returns `None` if there is no child corresponding to token AT
fn AT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AT, 0)
}
/// Retrieves first TerminalNode corresponding to token BETWEEN
/// Returns `None` if there is no child corresponding to token BETWEEN
fn BETWEEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BETWEEN, 0)
}
/// Retrieves first TerminalNode corresponding to token BUCKET
/// Returns `None` if there is no child corresponding to token BUCKET
fn BUCKET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BUCKET, 0)
}
/// Retrieves first TerminalNode corresponding to token BUCKETS
/// Returns `None` if there is no child corresponding to token BUCKETS
fn BUCKETS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BUCKETS, 0)
}
/// Retrieves first TerminalNode corresponding to token BY
/// Returns `None` if there is no child corresponding to token BY
fn BY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, 0)
}
/// Retrieves first TerminalNode corresponding to token CACHE
/// Returns `None` if there is no child corresponding to token CACHE
fn CACHE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CACHE, 0)
}
/// Retrieves first TerminalNode corresponding to token CASCADE
/// Returns `None` if there is no child corresponding to token CASCADE
fn CASCADE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CASCADE, 0)
}
/// Retrieves first TerminalNode corresponding to token CHANGE
/// Returns `None` if there is no child corresponding to token CHANGE
fn CHANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CHANGE, 0)
}
/// Retrieves first TerminalNode corresponding to token CLEAR
/// Returns `None` if there is no child corresponding to token CLEAR
fn CLEAR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLEAR, 0)
}
/// Retrieves first TerminalNode corresponding to token CLUSTER
/// Returns `None` if there is no child corresponding to token CLUSTER
fn CLUSTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLUSTER, 0)
}
/// Retrieves first TerminalNode corresponding to token CLUSTERED
/// Returns `None` if there is no child corresponding to token CLUSTERED
fn CLUSTERED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLUSTERED, 0)
}
/// Retrieves first TerminalNode corresponding to token CODEGEN
/// Returns `None` if there is no child corresponding to token CODEGEN
fn CODEGEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CODEGEN, 0)
}
/// Retrieves first TerminalNode corresponding to token COLLECTION
/// Returns `None` if there is no child corresponding to token COLLECTION
fn COLLECTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COLLECTION, 0)
}
/// Retrieves first TerminalNode corresponding to token COLUMNS
/// Returns `None` if there is no child corresponding to token COLUMNS
fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COLUMNS, 0)
}
/// Retrieves first TerminalNode corresponding to token COMMENT
/// Returns `None` if there is no child corresponding to token COMMENT
fn COMMENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMMENT, 0)
}
/// Retrieves first TerminalNode corresponding to token COMMIT
/// Returns `None` if there is no child corresponding to token COMMIT
fn COMMIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMMIT, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPACT
/// Returns `None` if there is no child corresponding to token COMPACT
fn COMPACT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPACT, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPACTIONS
/// Returns `None` if there is no child corresponding to token COMPACTIONS
fn COMPACTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPACTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPUTE
/// Returns `None` if there is no child corresponding to token COMPUTE
fn COMPUTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPUTE, 0)
}
/// Retrieves first TerminalNode corresponding to token CONCATENATE
/// Returns `None` if there is no child corresponding to token CONCATENATE
fn CONCATENATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CONCATENATE, 0)
}
/// Retrieves first TerminalNode corresponding to token COST
/// Returns `None` if there is no child corresponding to token COST
fn COST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COST, 0)
}
/// Retrieves first TerminalNode corresponding to token CUBE
/// Returns `None` if there is no child corresponding to token CUBE
fn CUBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CUBE, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT
/// Returns `None` if there is no child corresponding to token CURRENT
fn CURRENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT, 0)
}
/// Retrieves first TerminalNode corresponding to token DATA
/// Returns `None` if there is no child corresponding to token DATA
fn DATA(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATA, 0)
}
/// Retrieves first TerminalNode corresponding to token DATABASE
/// Returns `None` if there is no child corresponding to token DATABASE
fn DATABASE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATABASE, 0)
}
/// Retrieves first TerminalNode corresponding to token DATABASES
/// Returns `None` if there is no child corresponding to token DATABASES
fn DATABASES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATABASES, 0)
}
/// Retrieves first TerminalNode corresponding to token DBPROPERTIES
/// Returns `None` if there is no child corresponding to token DBPROPERTIES
fn DBPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DBPROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token DEFINED
/// Returns `None` if there is no child corresponding to token DEFINED
fn DEFINED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DEFINED, 0)
}
/// Retrieves first TerminalNode corresponding to token DELETE
/// Returns `None` if there is no child corresponding to token DELETE
fn DELETE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DELETE, 0)
}
/// Retrieves first TerminalNode corresponding to token DELIMITED
/// Returns `None` if there is no child corresponding to token DELIMITED
fn DELIMITED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DELIMITED, 0)
}
/// Retrieves first TerminalNode corresponding to token DESC
/// Returns `None` if there is no child corresponding to token DESC
fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DESC, 0)
}
/// Retrieves first TerminalNode corresponding to token DESCRIBE
/// Returns `None` if there is no child corresponding to token DESCRIBE
fn DESCRIBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DESCRIBE, 0)
}
/// Retrieves first TerminalNode corresponding to token DFS
/// Returns `None` if there is no child corresponding to token DFS
fn DFS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DFS, 0)
}
/// Retrieves first TerminalNode corresponding to token DIRECTORIES
/// Returns `None` if there is no child corresponding to token DIRECTORIES
fn DIRECTORIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIRECTORIES, 0)
}
/// Retrieves first TerminalNode corresponding to token DIRECTORY
/// Returns `None` if there is no child corresponding to token DIRECTORY
fn DIRECTORY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIRECTORY, 0)
}
/// Retrieves first TerminalNode corresponding to token DISTRIBUTE
/// Returns `None` if there is no child corresponding to token DISTRIBUTE
fn DISTRIBUTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DISTRIBUTE, 0)
}
/// Retrieves first TerminalNode corresponding to token DIV
/// Returns `None` if there is no child corresponding to token DIV
fn DIV(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIV, 0)
}
/// Retrieves first TerminalNode corresponding to token DROP
/// Returns `None` if there is no child corresponding to token DROP
fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DROP, 0)
}
/// Retrieves first TerminalNode corresponding to token ESCAPED
/// Returns `None` if there is no child corresponding to token ESCAPED
fn ESCAPED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ESCAPED, 0)
}
/// Retrieves first TerminalNode corresponding to token EXCHANGE
/// Returns `None` if there is no child corresponding to token EXCHANGE
fn EXCHANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXCHANGE, 0)
}
/// Retrieves first TerminalNode corresponding to token EXISTS
/// Returns `None` if there is no child corresponding to token EXISTS
fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXISTS, 0)
}
/// Retrieves first TerminalNode corresponding to token EXPLAIN
/// Returns `None` if there is no child corresponding to token EXPLAIN
fn EXPLAIN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXPLAIN, 0)
}
/// Retrieves first TerminalNode corresponding to token EXPORT
/// Returns `None` if there is no child corresponding to token EXPORT
fn EXPORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXPORT, 0)
}
/// Retrieves first TerminalNode corresponding to token EXTENDED
/// Returns `None` if there is no child corresponding to token EXTENDED
fn EXTENDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXTENDED, 0)
}
/// Retrieves first TerminalNode corresponding to token EXTERNAL
/// Returns `None` if there is no child corresponding to token EXTERNAL
fn EXTERNAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXTERNAL, 0)
}
/// Retrieves first TerminalNode corresponding to token EXTRACT
/// Returns `None` if there is no child corresponding to token EXTRACT
fn EXTRACT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXTRACT, 0)
}
/// Retrieves first TerminalNode corresponding to token FIELDS
/// Returns `None` if there is no child corresponding to token FIELDS
fn FIELDS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FIELDS, 0)
}
/// Retrieves first TerminalNode corresponding to token FILEFORMAT
/// Returns `None` if there is no child corresponding to token FILEFORMAT
fn FILEFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FILEFORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token FIRST
/// Returns `None` if there is no child corresponding to token FIRST
fn FIRST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FIRST, 0)
}
/// Retrieves first TerminalNode corresponding to token FOLLOWING
/// Returns `None` if there is no child corresponding to token FOLLOWING
fn FOLLOWING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FOLLOWING, 0)
}
/// Retrieves first TerminalNode corresponding to token FORMAT
/// Returns `None` if there is no child corresponding to token FORMAT
fn FORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token FORMATTED
/// Returns `None` if there is no child corresponding to token FORMATTED
fn FORMATTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FORMATTED, 0)
}
/// Retrieves first TerminalNode corresponding to token FUNCTION
/// Returns `None` if there is no child corresponding to token FUNCTION
fn FUNCTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FUNCTION, 0)
}
/// Retrieves first TerminalNode corresponding to token FUNCTIONS
/// Returns `None` if there is no child corresponding to token FUNCTIONS
fn FUNCTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FUNCTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token GLOBAL
/// Returns `None` if there is no child corresponding to token GLOBAL
fn GLOBAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GLOBAL, 0)
}
/// Retrieves first TerminalNode corresponding to token GROUPING
/// Returns `None` if there is no child corresponding to token GROUPING
fn GROUPING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GROUPING, 0)
}
/// Retrieves first TerminalNode corresponding to token IF
/// Returns `None` if there is no child corresponding to token IF
fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IF, 0)
}
/// Retrieves first TerminalNode corresponding to token IGNORE
/// Returns `None` if there is no child corresponding to token IGNORE
fn IGNORE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IGNORE, 0)
}
/// Retrieves first TerminalNode corresponding to token IMPORT
/// Returns `None` if there is no child corresponding to token IMPORT
fn IMPORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IMPORT, 0)
}
/// Retrieves first TerminalNode corresponding to token INDEX
/// Returns `None` if there is no child corresponding to token INDEX
fn INDEX(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INDEX, 0)
}
/// Retrieves first TerminalNode corresponding to token INDEXES
/// Returns `None` if there is no child corresponding to token INDEXES
fn INDEXES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INDEXES, 0)
}
/// Retrieves first TerminalNode corresponding to token INPATH
/// Returns `None` if there is no child corresponding to token INPATH
fn INPATH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INPATH, 0)
}
/// Retrieves first TerminalNode corresponding to token INPUTFORMAT
/// Returns `None` if there is no child corresponding to token INPUTFORMAT
fn INPUTFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INPUTFORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token INSERT
/// Returns `None` if there is no child corresponding to token INSERT
fn INSERT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INSERT, 0)
}
/// Retrieves first TerminalNode corresponding to token INTERVAL
/// Returns `None` if there is no child corresponding to token INTERVAL
fn INTERVAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTERVAL, 0)
}
/// Retrieves first TerminalNode corresponding to token ITEMS
/// Returns `None` if there is no child corresponding to token ITEMS
fn ITEMS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ITEMS, 0)
}
/// Retrieves first TerminalNode corresponding to token KEYS
/// Returns `None` if there is no child corresponding to token KEYS
fn KEYS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(KEYS, 0)
}
/// Retrieves first TerminalNode corresponding to token LAST
/// Returns `None` if there is no child corresponding to token LAST
fn LAST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LAST, 0)
}
/// Retrieves first TerminalNode corresponding to token LATERAL
/// Returns `None` if there is no child corresponding to token LATERAL
fn LATERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LATERAL, 0)
}
/// Retrieves first TerminalNode corresponding to token LAZY
/// Returns `None` if there is no child corresponding to token LAZY
fn LAZY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LAZY, 0)
}
/// Retrieves first TerminalNode corresponding to token LIKE
/// Returns `None` if there is no child corresponding to token LIKE
fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIKE, 0)
}
/// Retrieves first TerminalNode corresponding to token LIMIT
/// Returns `None` if there is no child corresponding to token LIMIT
fn LIMIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIMIT, 0)
}
/// Retrieves first TerminalNode corresponding to token LINES
/// Returns `None` if there is no child corresponding to token LINES
fn LINES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LINES, 0)
}
/// Retrieves first TerminalNode corresponding to token LIST
/// Returns `None` if there is no child corresponding to token LIST
fn LIST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIST, 0)
}
/// Retrieves first TerminalNode corresponding to token LOAD
/// Returns `None` if there is no child corresponding to token LOAD
fn LOAD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOAD, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCAL
/// Returns `None` if there is no child corresponding to token LOCAL
fn LOCAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCAL, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCATION
/// Returns `None` if there is no child corresponding to token LOCATION
fn LOCATION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCATION, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCK
/// Returns `None` if there is no child corresponding to token LOCK
fn LOCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCK, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCKS
/// Returns `None` if there is no child corresponding to token LOCKS
fn LOCKS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCKS, 0)
}
/// Retrieves first TerminalNode corresponding to token LOGICAL
/// Returns `None` if there is no child corresponding to token LOGICAL
fn LOGICAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOGICAL, 0)
}
/// Retrieves first TerminalNode corresponding to token MACRO
/// Returns `None` if there is no child corresponding to token MACRO
fn MACRO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MACRO, 0)
}
/// Retrieves first TerminalNode corresponding to token MAP
/// Returns `None` if there is no child corresponding to token MAP
fn MAP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MAP, 0)
}
/// Retrieves first TerminalNode corresponding to token MATCHED
/// Returns `None` if there is no child corresponding to token MATCHED
fn MATCHED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MATCHED, 0)
}
/// Retrieves first TerminalNode corresponding to token MERGE
/// Returns `None` if there is no child corresponding to token MERGE
fn MERGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MERGE, 0)
}
/// Retrieves first TerminalNode corresponding to token MSCK
/// Returns `None` if there is no child corresponding to token MSCK
fn MSCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MSCK, 0)
}
/// Retrieves first TerminalNode corresponding to token NAMESPACE
/// Returns `None` if there is no child corresponding to token NAMESPACE
fn NAMESPACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NAMESPACE, 0)
}
/// Retrieves first TerminalNode corresponding to token NAMESPACES
/// Returns `None` if there is no child corresponding to token NAMESPACES
fn NAMESPACES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NAMESPACES, 0)
}
/// Retrieves first TerminalNode corresponding to token NO
/// Returns `None` if there is no child corresponding to token NO
fn NO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NO, 0)
}
/// Retrieves first TerminalNode corresponding to token NULLS
/// Returns `None` if there is no child corresponding to token NULLS
fn NULLS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULLS, 0)
}
/// Retrieves first TerminalNode corresponding to token OF
/// Returns `None` if there is no child corresponding to token OF
fn OF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OF, 0)
}
/// Retrieves first TerminalNode corresponding to token OPTION
/// Returns `None` if there is no child corresponding to token OPTION
fn OPTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OPTION, 0)
}
/// Retrieves first TerminalNode corresponding to token OPTIONS
/// Returns `None` if there is no child corresponding to token OPTIONS
fn OPTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OPTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token OUT
/// Returns `None` if there is no child corresponding to token OUT
fn OUT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OUT, 0)
}
/// Retrieves first TerminalNode corresponding to token OUTPUTFORMAT
/// Returns `None` if there is no child corresponding to token OUTPUTFORMAT
fn OUTPUTFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OUTPUTFORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token OVER
/// Returns `None` if there is no child corresponding to token OVER
fn OVER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OVER, 0)
}
/// Retrieves first TerminalNode corresponding to token OVERLAY
/// Returns `None` if there is no child corresponding to token OVERLAY
fn OVERLAY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OVERLAY, 0)
}
/// Retrieves first TerminalNode corresponding to token OVERWRITE
/// Returns `None` if there is no child corresponding to token OVERWRITE
fn OVERWRITE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OVERWRITE, 0)
}
/// Retrieves first TerminalNode corresponding to token PARTITION
/// Returns `None` if there is no child corresponding to token PARTITION
fn PARTITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITION, 0)
}
/// Retrieves first TerminalNode corresponding to token PARTITIONED
/// Returns `None` if there is no child corresponding to token PARTITIONED
fn PARTITIONED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITIONED, 0)
}
/// Retrieves first TerminalNode corresponding to token PARTITIONS
/// Returns `None` if there is no child corresponding to token PARTITIONS
fn PARTITIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token PERCENTLIT
/// Returns `None` if there is no child corresponding to token PERCENTLIT
fn PERCENTLIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PERCENTLIT, 0)
}
/// Retrieves first TerminalNode corresponding to token PIVOT
/// Returns `None` if there is no child corresponding to token PIVOT
fn PIVOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PIVOT, 0)
}
/// Retrieves first TerminalNode corresponding to token PLACING
/// Returns `None` if there is no child corresponding to token PLACING
fn PLACING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PLACING, 0)
}
/// Retrieves first TerminalNode corresponding to token POSITION
/// Returns `None` if there is no child corresponding to token POSITION
fn POSITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(POSITION, 0)
}
/// Retrieves first TerminalNode corresponding to token PRECEDING
/// Returns `None` if there is no child corresponding to token PRECEDING
fn PRECEDING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PRECEDING, 0)
}
/// Retrieves first TerminalNode corresponding to token PRINCIPALS
/// Returns `None` if there is no child corresponding to token PRINCIPALS
fn PRINCIPALS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PRINCIPALS, 0)
}
/// Retrieves first TerminalNode corresponding to token PROPERTIES
/// Returns `None` if there is no child corresponding to token PROPERTIES
fn PROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token PURGE
/// Returns `None` if there is no child corresponding to token PURGE
fn PURGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PURGE, 0)
}
/// Retrieves first TerminalNode corresponding to token QUERY
/// Returns `None` if there is no child corresponding to token QUERY
fn QUERY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(QUERY, 0)
}
/// Retrieves first TerminalNode corresponding to token RANGE
/// Returns `None` if there is no child corresponding to token RANGE
fn RANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RANGE, 0)
}
/// Retrieves first TerminalNode corresponding to token RECORDREADER
/// Returns `None` if there is no child corresponding to token RECORDREADER
fn RECORDREADER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECORDREADER, 0)
}
/// Retrieves first TerminalNode corresponding to token RECORDWRITER
/// Returns `None` if there is no child corresponding to token RECORDWRITER
fn RECORDWRITER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECORDWRITER, 0)
}
/// Retrieves first TerminalNode corresponding to token RECOVER
/// Returns `None` if there is no child corresponding to token RECOVER
fn RECOVER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECOVER, 0)
}
/// Retrieves first TerminalNode corresponding to token REDUCE
/// Returns `None` if there is no child corresponding to token REDUCE
fn REDUCE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REDUCE, 0)
}
/// Retrieves first TerminalNode corresponding to token REFRESH
/// Returns `None` if there is no child corresponding to token REFRESH
fn REFRESH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REFRESH, 0)
}
/// Retrieves first TerminalNode corresponding to token RENAME
/// Returns `None` if there is no child corresponding to token RENAME
fn RENAME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RENAME, 0)
}
/// Retrieves first TerminalNode corresponding to token REPAIR
/// Returns `None` if there is no child corresponding to token REPAIR
fn REPAIR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REPAIR, 0)
}
/// Retrieves first TerminalNode corresponding to token REPLACE
/// Returns `None` if there is no child corresponding to token REPLACE
fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REPLACE, 0)
}
/// Retrieves first TerminalNode corresponding to token RESET
/// Returns `None` if there is no child corresponding to token RESET
fn RESET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RESET, 0)
}
/// Retrieves first TerminalNode corresponding to token RESTRICT
/// Returns `None` if there is no child corresponding to token RESTRICT
fn RESTRICT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RESTRICT, 0)
}
/// Retrieves first TerminalNode corresponding to token REVOKE
/// Returns `None` if there is no child corresponding to token REVOKE
fn REVOKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REVOKE, 0)
}
/// Retrieves first TerminalNode corresponding to token RLIKE
/// Returns `None` if there is no child corresponding to token RLIKE
fn RLIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RLIKE, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLE
/// Returns `None` if there is no child corresponding to token ROLE
fn ROLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLE, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLES
/// Returns `None` if there is no child corresponding to token ROLES
fn ROLES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLES, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLLBACK
/// Returns `None` if there is no child corresponding to token ROLLBACK
fn ROLLBACK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLLBACK, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLLUP
/// Returns `None` if there is no child corresponding to token ROLLUP
fn ROLLUP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLLUP, 0)
}
/// Retrieves first TerminalNode corresponding to token ROW
/// Returns `None` if there is no child corresponding to token ROW
fn ROW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROW, 0)
}
/// Retrieves first TerminalNode corresponding to token ROWS
/// Returns `None` if there is no child corresponding to token ROWS
fn ROWS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROWS, 0)
}
/// Retrieves first TerminalNode corresponding to token SCHEMA
/// Returns `None` if there is no child corresponding to token SCHEMA
fn SCHEMA(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SCHEMA, 0)
}
/// Retrieves first TerminalNode corresponding to token SEMI
/// Returns `None` if there is no child corresponding to token SEMI
fn SEMI(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SEMI, 0)
}
/// Retrieves first TerminalNode corresponding to token SEPARATED
/// Returns `None` if there is no child corresponding to token SEPARATED
fn SEPARATED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SEPARATED, 0)
}
/// Retrieves first TerminalNode corresponding to token SERDE
/// Returns `None` if there is no child corresponding to token SERDE
fn SERDE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SERDE, 0)
}
/// Retrieves first TerminalNode corresponding to token SERDEPROPERTIES
/// Returns `None` if there is no child corresponding to token SERDEPROPERTIES
fn SERDEPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SERDEPROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token SET
/// Returns `None` if there is no child corresponding to token SET
fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SET, 0)
}
/// Retrieves first TerminalNode corresponding to token SETMINUS
/// Returns `None` if there is no child corresponding to token SETMINUS
fn SETMINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SETMINUS, 0)
}
/// Retrieves first TerminalNode corresponding to token SETS
/// Returns `None` if there is no child corresponding to token SETS
fn SETS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SETS, 0)
}
/// Retrieves first TerminalNode corresponding to token SHOW
/// Returns `None` if there is no child corresponding to token SHOW
fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SHOW, 0)
}
/// Retrieves first TerminalNode corresponding to token SKEWED
/// Returns `None` if there is no child corresponding to token SKEWED
fn SKEWED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SKEWED, 0)
}
/// Retrieves first TerminalNode corresponding to token SORT
/// Returns `None` if there is no child corresponding to token SORT
fn SORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SORT, 0)
}
/// Retrieves first TerminalNode corresponding to token SORTED
/// Returns `None` if there is no child corresponding to token SORTED
fn SORTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SORTED, 0)
}
/// Retrieves first TerminalNode corresponding to token START
/// Returns `None` if there is no child corresponding to token START
fn START(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(START, 0)
}
/// Retrieves first TerminalNode corresponding to token STATISTICS
/// Returns `None` if there is no child corresponding to token STATISTICS
fn STATISTICS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STATISTICS, 0)
}
/// Retrieves first TerminalNode corresponding to token STORED
/// Returns `None` if there is no child corresponding to token STORED
fn STORED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STORED, 0)
}
/// Retrieves first TerminalNode corresponding to token STRATIFY
/// Returns `None` if there is no child corresponding to token STRATIFY
fn STRATIFY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRATIFY, 0)
}
/// Retrieves first TerminalNode corresponding to token STRUCT
/// Returns `None` if there is no child corresponding to token STRUCT
fn STRUCT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRUCT, 0)
}
/// Retrieves first TerminalNode corresponding to token SUBSTR
/// Returns `None` if there is no child corresponding to token SUBSTR
fn SUBSTR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SUBSTR, 0)
}
/// Retrieves first TerminalNode corresponding to token SUBSTRING
/// Returns `None` if there is no child corresponding to token SUBSTRING
fn SUBSTRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SUBSTRING, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLES
/// Returns `None` if there is no child corresponding to token TABLES
fn TABLES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLES, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLESAMPLE
/// Returns `None` if there is no child corresponding to token TABLESAMPLE
fn TABLESAMPLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLESAMPLE, 0)
}
/// Retrieves first TerminalNode corresponding to token TBLPROPERTIES
/// Returns `None` if there is no child corresponding to token TBLPROPERTIES
fn TBLPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TBLPROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token TEMPORARY
/// Returns `None` if there is no child corresponding to token TEMPORARY
fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TEMPORARY, 0)
}
/// Retrieves first TerminalNode corresponding to token TERMINATED
/// Returns `None` if there is no child corresponding to token TERMINATED
fn TERMINATED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TERMINATED, 0)
}
/// Retrieves first TerminalNode corresponding to token TOUCH
/// Returns `None` if there is no child corresponding to token TOUCH
fn TOUCH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TOUCH, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSACTION
/// Returns `None` if there is no child corresponding to token TRANSACTION
fn TRANSACTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSACTION, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSACTIONS
/// Returns `None` if there is no child corresponding to token TRANSACTIONS
fn TRANSACTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSACTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSFORM
/// Returns `None` if there is no child corresponding to token TRANSFORM
fn TRANSFORM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSFORM, 0)
}
/// Retrieves first TerminalNode corresponding to token TRIM
/// Returns `None` if there is no child corresponding to token TRIM
fn TRIM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRIM, 0)
}
/// Retrieves first TerminalNode corresponding to token TRUE
/// Returns `None` if there is no child corresponding to token TRUE
fn TRUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRUE, 0)
}
/// Retrieves first TerminalNode corresponding to token TRUNCATE
/// Returns `None` if there is no child corresponding to token TRUNCATE
fn TRUNCATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRUNCATE, 0)
}
/// Retrieves first TerminalNode corresponding to token TYPE
/// Returns `None` if there is no child corresponding to token TYPE
fn TYPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TYPE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNARCHIVE
/// Returns `None` if there is no child corresponding to token UNARCHIVE
fn UNARCHIVE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNARCHIVE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNBOUNDED
/// Returns `None` if there is no child corresponding to token UNBOUNDED
fn UNBOUNDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNBOUNDED, 0)
}
/// Retrieves first TerminalNode corresponding to token UNCACHE
/// Returns `None` if there is no child corresponding to token UNCACHE
fn UNCACHE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNCACHE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNLOCK
/// Returns `None` if there is no child corresponding to token UNLOCK
fn UNLOCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNLOCK, 0)
}
/// Retrieves first TerminalNode corresponding to token UNSET
/// Returns `None` if there is no child corresponding to token UNSET
fn UNSET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNSET, 0)
}
/// Retrieves first TerminalNode corresponding to token UPDATE
/// Returns `None` if there is no child corresponding to token UPDATE
fn UPDATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UPDATE, 0)
}
/// Retrieves first TerminalNode corresponding to token USE
/// Returns `None` if there is no child corresponding to token USE
fn USE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(USE, 0)
}
/// Retrieves first TerminalNode corresponding to token VALUES
/// Returns `None` if there is no child corresponding to token VALUES
fn VALUES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VALUES, 0)
}
/// Retrieves first TerminalNode corresponding to token VIEW
/// Returns `None` if there is no child corresponding to token VIEW
fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VIEW, 0)
}
/// Retrieves first TerminalNode corresponding to token VIEWS
/// Returns `None` if there is no child corresponding to token VIEWS
fn VIEWS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VIEWS, 0)
}
/// Retrieves first TerminalNode corresponding to token WINDOW
/// Returns `None` if there is no child corresponding to token WINDOW
fn WINDOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WINDOW, 0)
}
/// Retrieves first TerminalNode corresponding to token ZONE
/// Returns `None` if there is no child corresponding to token ZONE
fn ZONE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ZONE, 0)
}

}

impl<'input> AnsiNonReservedContextAttrs<'input> for AnsiNonReservedContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn ansiNonReserved(&mut self,)
	-> Result<Rc<AnsiNonReservedContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = AnsiNonReservedContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 268, RULE_ansiNonReserved);
        let mut _localctx: Rc<AnsiNonReservedContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(3024);
			_la = recog.base.input.la(1);
			if { !(((((_la - 11)) & !0x3f) == 0 && ((1usize << (_la - 11)) & ((1usize << (ADD - 11)) | (1usize << (AFTER - 11)) | (1usize << (ALTER - 11)) | (1usize << (ANALYZE - 11)) | (1usize << (ANTI - 11)) | (1usize << (ARCHIVE - 11)) | (1usize << (ARRAY - 11)) | (1usize << (ASC - 11)) | (1usize << (AT - 11)) | (1usize << (BETWEEN - 11)) | (1usize << (BUCKET - 11)) | (1usize << (BUCKETS - 11)) | (1usize << (BY - 11)) | (1usize << (CACHE - 11)) | (1usize << (CASCADE - 11)) | (1usize << (CHANGE - 11)) | (1usize << (CLEAR - 11)) | (1usize << (CLUSTER - 11)) | (1usize << (CLUSTERED - 11)) | (1usize << (CODEGEN - 11)) | (1usize << (COLLECTION - 11)))) != 0) || ((((_la - 43)) & !0x3f) == 0 && ((1usize << (_la - 43)) & ((1usize << (COLUMNS - 43)) | (1usize << (COMMENT - 43)) | (1usize << (COMMIT - 43)) | (1usize << (COMPACT - 43)) | (1usize << (COMPACTIONS - 43)) | (1usize << (COMPUTE - 43)) | (1usize << (CONCATENATE - 43)) | (1usize << (COST - 43)) | (1usize << (CUBE - 43)) | (1usize << (CURRENT - 43)) | (1usize << (DATA - 43)) | (1usize << (DATABASE - 43)) | (1usize << (DATABASES - 43)) | (1usize << (DBPROPERTIES - 43)) | (1usize << (DEFINED - 43)) | (1usize << (DELETE - 43)) | (1usize << (DELIMITED - 43)) | (1usize << (DESC - 43)) | (1usize << (DESCRIBE - 43)) | (1usize << (DFS - 43)) | (1usize << (DIRECTORIES - 43)) | (1usize << (DIRECTORY - 43)) | (1usize << (DISTRIBUTE - 43)) | (1usize << (DIV - 43)))) != 0) || ((((_la - 75)) & !0x3f) == 0 && ((1usize << (_la - 75)) & ((1usize << (DROP - 75)) | (1usize << (ESCAPED - 75)) | (1usize << (EXCHANGE - 75)) | (1usize << (EXISTS - 75)) | (1usize << (EXPLAIN - 75)) | (1usize << (EXPORT - 75)) | (1usize << (EXTENDED - 75)) | (1usize << (EXTERNAL - 75)) | (1usize << (EXTRACT - 75)) | (1usize << (FIELDS - 75)) | (1usize << (FILEFORMAT - 75)) | (1usize << (FIRST - 75)) | (1usize << (FOLLOWING - 75)) | (1usize << (FORMAT - 75)) | (1usize << (FORMATTED - 75)) | (1usize << (FUNCTION - 75)) | (1usize << (FUNCTIONS - 75)) | (1usize << (GLOBAL - 75)) | (1usize << (GROUPING - 75)))) != 0) || ((((_la - 108)) & !0x3f) == 0 && ((1usize << (_la - 108)) & ((1usize << (IF - 108)) | (1usize << (IGNORE - 108)) | (1usize << (IMPORT - 108)) | (1usize << (INDEX - 108)) | (1usize << (INDEXES - 108)) | (1usize << (INPATH - 108)) | (1usize << (INPUTFORMAT - 108)) | (1usize << (INSERT - 108)) | (1usize << (INTERVAL - 108)) | (1usize << (ITEMS - 108)) | (1usize << (KEYS - 108)) | (1usize << (LAST - 108)) | (1usize << (LATERAL - 108)) | (1usize << (LAZY - 108)) | (1usize << (LIKE - 108)) | (1usize << (LIMIT - 108)) | (1usize << (LINES - 108)) | (1usize << (LIST - 108)) | (1usize << (LOAD - 108)) | (1usize << (LOCAL - 108)) | (1usize << (LOCATION - 108)) | (1usize << (LOCK - 108)) | (1usize << (LOCKS - 108)) | (1usize << (LOGICAL - 108)))) != 0) || ((((_la - 140)) & !0x3f) == 0 && ((1usize << (_la - 140)) & ((1usize << (MACRO - 140)) | (1usize << (MAP - 140)) | (1usize << (MATCHED - 140)) | (1usize << (MERGE - 140)) | (1usize << (MSCK - 140)) | (1usize << (NAMESPACE - 140)) | (1usize << (NAMESPACES - 140)) | (1usize << (NO - 140)) | (1usize << (NULLS - 140)) | (1usize << (OF - 140)) | (1usize << (OPTION - 140)) | (1usize << (OPTIONS - 140)) | (1usize << (OUT - 140)) | (1usize << (OUTPUTFORMAT - 140)) | (1usize << (OVER - 140)) | (1usize << (OVERLAY - 140)) | (1usize << (OVERWRITE - 140)) | (1usize << (PARTITION - 140)) | (1usize << (PARTITIONED - 140)) | (1usize << (PARTITIONS - 140)) | (1usize << (PERCENTLIT - 140)) | (1usize << (PIVOT - 140)) | (1usize << (PLACING - 140)))) != 0) || ((((_la - 172)) & !0x3f) == 0 && ((1usize << (_la - 172)) & ((1usize << (POSITION - 172)) | (1usize << (PRECEDING - 172)) | (1usize << (PRINCIPALS - 172)) | (1usize << (PROPERTIES - 172)) | (1usize << (PURGE - 172)) | (1usize << (QUERY - 172)) | (1usize << (RANGE - 172)) | (1usize << (RECORDREADER - 172)) | (1usize << (RECORDWRITER - 172)) | (1usize << (RECOVER - 172)) | (1usize << (REDUCE - 172)) | (1usize << (REFRESH - 172)) | (1usize << (RENAME - 172)) | (1usize << (REPAIR - 172)) | (1usize << (REPLACE - 172)) | (1usize << (RESET - 172)) | (1usize << (RESTRICT - 172)) | (1usize << (REVOKE - 172)) | (1usize << (RLIKE - 172)) | (1usize << (ROLE - 172)) | (1usize << (ROLES - 172)) | (1usize << (ROLLBACK - 172)) | (1usize << (ROLLUP - 172)) | (1usize << (ROW - 172)) | (1usize << (ROWS - 172)) | (1usize << (SCHEMA - 172)) | (1usize << (SEMI - 172)) | (1usize << (SEPARATED - 172)))) != 0) || ((((_la - 204)) & !0x3f) == 0 && ((1usize << (_la - 204)) & ((1usize << (SERDE - 204)) | (1usize << (SERDEPROPERTIES - 204)) | (1usize << (SET - 204)) | (1usize << (SETMINUS - 204)) | (1usize << (SETS - 204)) | (1usize << (SHOW - 204)) | (1usize << (SKEWED - 204)) | (1usize << (SORT - 204)) | (1usize << (SORTED - 204)) | (1usize << (START - 204)) | (1usize << (STATISTICS - 204)) | (1usize << (STORED - 204)) | (1usize << (STRATIFY - 204)) | (1usize << (STRUCT - 204)) | (1usize << (SUBSTR - 204)) | (1usize << (SUBSTRING - 204)) | (1usize << (TABLES - 204)) | (1usize << (TABLESAMPLE - 204)) | (1usize << (TBLPROPERTIES - 204)) | (1usize << (TEMPORARY - 204)) | (1usize << (TERMINATED - 204)) | (1usize << (TOUCH - 204)) | (1usize << (TRANSACTION - 204)) | (1usize << (TRANSACTIONS - 204)) | (1usize << (TRANSFORM - 204)))) != 0) || ((((_la - 236)) & !0x3f) == 0 && ((1usize << (_la - 236)) & ((1usize << (TRIM - 236)) | (1usize << (TRUE - 236)) | (1usize << (TRUNCATE - 236)) | (1usize << (TYPE - 236)) | (1usize << (UNARCHIVE - 236)) | (1usize << (UNBOUNDED - 236)) | (1usize << (UNCACHE - 236)) | (1usize << (UNLOCK - 236)) | (1usize << (UNSET - 236)) | (1usize << (UPDATE - 236)) | (1usize << (USE - 236)) | (1usize << (VALUES - 236)) | (1usize << (VIEW - 236)) | (1usize << (VIEWS - 236)) | (1usize << (WINDOW - 236)) | (1usize << (ZONE - 236)))) != 0)) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- strictNonReserved ----------------
pub type StrictNonReservedContextAll<'input> = StrictNonReservedContext<'input>;


pub type StrictNonReservedContext<'input> = BaseParserRuleContext<'input,StrictNonReservedContextExt<'input>>;

#[derive(Clone)]
pub struct StrictNonReservedContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for StrictNonReservedContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for StrictNonReservedContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_strictNonReserved(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_strictNonReserved(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for StrictNonReservedContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_strictNonReserved }
	//fn type_rule_index() -> usize where Self: Sized { RULE_strictNonReserved }
}
crate::tid!{StrictNonReservedContextExt<'a>}

impl<'input> StrictNonReservedContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<StrictNonReservedContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,StrictNonReservedContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait StrictNonReservedContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<StrictNonReservedContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token ANTI
/// Returns `None` if there is no child corresponding to token ANTI
fn ANTI(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ANTI, 0)
}
/// Retrieves first TerminalNode corresponding to token CROSS
/// Returns `None` if there is no child corresponding to token CROSS
fn CROSS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CROSS, 0)
}
/// Retrieves first TerminalNode corresponding to token EXCEPT
/// Returns `None` if there is no child corresponding to token EXCEPT
fn EXCEPT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXCEPT, 0)
}
/// Retrieves first TerminalNode corresponding to token FULL
/// Returns `None` if there is no child corresponding to token FULL
fn FULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FULL, 0)
}
/// Retrieves first TerminalNode corresponding to token INNER
/// Returns `None` if there is no child corresponding to token INNER
fn INNER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INNER, 0)
}
/// Retrieves first TerminalNode corresponding to token INTERSECT
/// Returns `None` if there is no child corresponding to token INTERSECT
fn INTERSECT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTERSECT, 0)
}
/// Retrieves first TerminalNode corresponding to token JOIN
/// Returns `None` if there is no child corresponding to token JOIN
fn JOIN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(JOIN, 0)
}
/// Retrieves first TerminalNode corresponding to token LEFT
/// Returns `None` if there is no child corresponding to token LEFT
fn LEFT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LEFT, 0)
}
/// Retrieves first TerminalNode corresponding to token NATURAL
/// Returns `None` if there is no child corresponding to token NATURAL
fn NATURAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NATURAL, 0)
}
/// Retrieves first TerminalNode corresponding to token ON
/// Returns `None` if there is no child corresponding to token ON
fn ON(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ON, 0)
}
/// Retrieves first TerminalNode corresponding to token RIGHT
/// Returns `None` if there is no child corresponding to token RIGHT
fn RIGHT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RIGHT, 0)
}
/// Retrieves first TerminalNode corresponding to token SEMI
/// Returns `None` if there is no child corresponding to token SEMI
fn SEMI(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SEMI, 0)
}
/// Retrieves first TerminalNode corresponding to token SETMINUS
/// Returns `None` if there is no child corresponding to token SETMINUS
fn SETMINUS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SETMINUS, 0)
}
/// Retrieves first TerminalNode corresponding to token UNION
/// Returns `None` if there is no child corresponding to token UNION
fn UNION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNION, 0)
}
/// Retrieves first TerminalNode corresponding to token USING
/// Returns `None` if there is no child corresponding to token USING
fn USING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(USING, 0)
}

}

impl<'input> StrictNonReservedContextAttrs<'input> for StrictNonReservedContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn strictNonReserved(&mut self,)
	-> Result<Rc<StrictNonReservedContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = StrictNonReservedContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 270, RULE_strictNonReserved);
        let mut _localctx: Rc<StrictNonReservedContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(3026);
			_la = recog.base.input.la(1);
			if { !(_la==ANTI || _la==CROSS || _la==EXCEPT || ((((_la - 100)) & !0x3f) == 0 && ((1usize << (_la - 100)) & ((1usize << (FULL - 100)) | (1usize << (INNER - 100)) | (1usize << (INTERSECT - 100)) | (1usize << (JOIN - 100)) | (1usize << (LEFT - 100)))) != 0) || _la==NATURAL || _la==ON || ((((_la - 192)) & !0x3f) == 0 && ((1usize << (_la - 192)) & ((1usize << (RIGHT - 192)) | (1usize << (SEMI - 192)) | (1usize << (SETMINUS - 192)))) != 0) || _la==UNION || _la==USING) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}
//------------------- nonReserved ----------------
pub type NonReservedContextAll<'input> = NonReservedContext<'input>;


pub type NonReservedContext<'input> = BaseParserRuleContext<'input,NonReservedContextExt<'input>>;

#[derive(Clone)]
pub struct NonReservedContextExt<'input>{
ph:PhantomData<&'input str>
}

impl<'input> SparkSqlParserContext<'input> for NonReservedContext<'input>{}

impl<'input,'a> Listenable<dyn SparkSqlListener<'input> + 'a> for NonReservedContext<'input>{
		fn enter(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.enter_every_rule(self);
			listener.enter_nonReserved(self);
		}fn exit(&self,listener: &mut (dyn SparkSqlListener<'input> + 'a)) {
			listener.exit_nonReserved(self);
			listener.exit_every_rule(self);
		}
}

impl<'input> CustomRuleContext<'input> for NonReservedContextExt<'input>{
	type TF = LocalTokenFactory<'input>;
	type Ctx = SparkSqlParserContextType;
	fn get_rule_index(&self) -> usize { RULE_nonReserved }
	//fn type_rule_index() -> usize where Self: Sized { RULE_nonReserved }
}
crate::tid!{NonReservedContextExt<'a>}

impl<'input> NonReservedContextExt<'input>{
	fn new(parent: Option<Rc<dyn SparkSqlParserContext<'input> + 'input > >, invoking_state: isize) -> Rc<NonReservedContextAll<'input>> {
		Rc::new(
			BaseParserRuleContext::new_parser_ctx(parent, invoking_state,NonReservedContextExt{
				ph:PhantomData
			}),
		)
	}
}

pub trait NonReservedContextAttrs<'input>: SparkSqlParserContext<'input> + BorrowMut<NonReservedContextExt<'input>>{

/// Retrieves first TerminalNode corresponding to token ADD
/// Returns `None` if there is no child corresponding to token ADD
fn ADD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ADD, 0)
}
/// Retrieves first TerminalNode corresponding to token AFTER
/// Returns `None` if there is no child corresponding to token AFTER
fn AFTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AFTER, 0)
}
/// Retrieves first TerminalNode corresponding to token ALL
/// Returns `None` if there is no child corresponding to token ALL
fn ALL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ALL, 0)
}
/// Retrieves first TerminalNode corresponding to token ALTER
/// Returns `None` if there is no child corresponding to token ALTER
fn ALTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ALTER, 0)
}
/// Retrieves first TerminalNode corresponding to token ANALYZE
/// Returns `None` if there is no child corresponding to token ANALYZE
fn ANALYZE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ANALYZE, 0)
}
/// Retrieves first TerminalNode corresponding to token AND
/// Returns `None` if there is no child corresponding to token AND
fn AND(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AND, 0)
}
/// Retrieves first TerminalNode corresponding to token ANY
/// Returns `None` if there is no child corresponding to token ANY
fn ANY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ANY, 0)
}
/// Retrieves first TerminalNode corresponding to token ARCHIVE
/// Returns `None` if there is no child corresponding to token ARCHIVE
fn ARCHIVE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ARCHIVE, 0)
}
/// Retrieves first TerminalNode corresponding to token ARRAY
/// Returns `None` if there is no child corresponding to token ARRAY
fn ARRAY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ARRAY, 0)
}
/// Retrieves first TerminalNode corresponding to token AS
/// Returns `None` if there is no child corresponding to token AS
fn AS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AS, 0)
}
/// Retrieves first TerminalNode corresponding to token ASC
/// Returns `None` if there is no child corresponding to token ASC
fn ASC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ASC, 0)
}
/// Retrieves first TerminalNode corresponding to token AT
/// Returns `None` if there is no child corresponding to token AT
fn AT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AT, 0)
}
/// Retrieves first TerminalNode corresponding to token AUTHORIZATION
/// Returns `None` if there is no child corresponding to token AUTHORIZATION
fn AUTHORIZATION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(AUTHORIZATION, 0)
}
/// Retrieves first TerminalNode corresponding to token BETWEEN
/// Returns `None` if there is no child corresponding to token BETWEEN
fn BETWEEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BETWEEN, 0)
}
/// Retrieves first TerminalNode corresponding to token BOTH
/// Returns `None` if there is no child corresponding to token BOTH
fn BOTH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BOTH, 0)
}
/// Retrieves first TerminalNode corresponding to token BUCKET
/// Returns `None` if there is no child corresponding to token BUCKET
fn BUCKET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BUCKET, 0)
}
/// Retrieves first TerminalNode corresponding to token BUCKETS
/// Returns `None` if there is no child corresponding to token BUCKETS
fn BUCKETS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BUCKETS, 0)
}
/// Retrieves first TerminalNode corresponding to token BY
/// Returns `None` if there is no child corresponding to token BY
fn BY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(BY, 0)
}
/// Retrieves first TerminalNode corresponding to token CACHE
/// Returns `None` if there is no child corresponding to token CACHE
fn CACHE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CACHE, 0)
}
/// Retrieves first TerminalNode corresponding to token CASCADE
/// Returns `None` if there is no child corresponding to token CASCADE
fn CASCADE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CASCADE, 0)
}
/// Retrieves first TerminalNode corresponding to token CASE
/// Returns `None` if there is no child corresponding to token CASE
fn CASE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CASE, 0)
}
/// Retrieves first TerminalNode corresponding to token CAST
/// Returns `None` if there is no child corresponding to token CAST
fn CAST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CAST, 0)
}
/// Retrieves first TerminalNode corresponding to token CHANGE
/// Returns `None` if there is no child corresponding to token CHANGE
fn CHANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CHANGE, 0)
}
/// Retrieves first TerminalNode corresponding to token CHECK
/// Returns `None` if there is no child corresponding to token CHECK
fn CHECK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CHECK, 0)
}
/// Retrieves first TerminalNode corresponding to token CLEAR
/// Returns `None` if there is no child corresponding to token CLEAR
fn CLEAR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLEAR, 0)
}
/// Retrieves first TerminalNode corresponding to token CLUSTER
/// Returns `None` if there is no child corresponding to token CLUSTER
fn CLUSTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLUSTER, 0)
}
/// Retrieves first TerminalNode corresponding to token CLUSTERED
/// Returns `None` if there is no child corresponding to token CLUSTERED
fn CLUSTERED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CLUSTERED, 0)
}
/// Retrieves first TerminalNode corresponding to token CODEGEN
/// Returns `None` if there is no child corresponding to token CODEGEN
fn CODEGEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CODEGEN, 0)
}
/// Retrieves first TerminalNode corresponding to token COLLATE
/// Returns `None` if there is no child corresponding to token COLLATE
fn COLLATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COLLATE, 0)
}
/// Retrieves first TerminalNode corresponding to token COLLECTION
/// Returns `None` if there is no child corresponding to token COLLECTION
fn COLLECTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COLLECTION, 0)
}
/// Retrieves first TerminalNode corresponding to token COLUMN
/// Returns `None` if there is no child corresponding to token COLUMN
fn COLUMN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COLUMN, 0)
}
/// Retrieves first TerminalNode corresponding to token COLUMNS
/// Returns `None` if there is no child corresponding to token COLUMNS
fn COLUMNS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COLUMNS, 0)
}
/// Retrieves first TerminalNode corresponding to token COMMENT
/// Returns `None` if there is no child corresponding to token COMMENT
fn COMMENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMMENT, 0)
}
/// Retrieves first TerminalNode corresponding to token COMMIT
/// Returns `None` if there is no child corresponding to token COMMIT
fn COMMIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMMIT, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPACT
/// Returns `None` if there is no child corresponding to token COMPACT
fn COMPACT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPACT, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPACTIONS
/// Returns `None` if there is no child corresponding to token COMPACTIONS
fn COMPACTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPACTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token COMPUTE
/// Returns `None` if there is no child corresponding to token COMPUTE
fn COMPUTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COMPUTE, 0)
}
/// Retrieves first TerminalNode corresponding to token CONCATENATE
/// Returns `None` if there is no child corresponding to token CONCATENATE
fn CONCATENATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CONCATENATE, 0)
}
/// Retrieves first TerminalNode corresponding to token CONSTRAINT
/// Returns `None` if there is no child corresponding to token CONSTRAINT
fn CONSTRAINT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CONSTRAINT, 0)
}
/// Retrieves first TerminalNode corresponding to token COST
/// Returns `None` if there is no child corresponding to token COST
fn COST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(COST, 0)
}
/// Retrieves first TerminalNode corresponding to token CREATE
/// Returns `None` if there is no child corresponding to token CREATE
fn CREATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CREATE, 0)
}
/// Retrieves first TerminalNode corresponding to token CUBE
/// Returns `None` if there is no child corresponding to token CUBE
fn CUBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CUBE, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT
/// Returns `None` if there is no child corresponding to token CURRENT
fn CURRENT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT_DATE
/// Returns `None` if there is no child corresponding to token CURRENT_DATE
fn CURRENT_DATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT_DATE, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT_TIME
/// Returns `None` if there is no child corresponding to token CURRENT_TIME
fn CURRENT_TIME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT_TIME, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT_TIMESTAMP
/// Returns `None` if there is no child corresponding to token CURRENT_TIMESTAMP
fn CURRENT_TIMESTAMP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT_TIMESTAMP, 0)
}
/// Retrieves first TerminalNode corresponding to token CURRENT_USER
/// Returns `None` if there is no child corresponding to token CURRENT_USER
fn CURRENT_USER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(CURRENT_USER, 0)
}
/// Retrieves first TerminalNode corresponding to token DATA
/// Returns `None` if there is no child corresponding to token DATA
fn DATA(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATA, 0)
}
/// Retrieves first TerminalNode corresponding to token DATABASE
/// Returns `None` if there is no child corresponding to token DATABASE
fn DATABASE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATABASE, 0)
}
/// Retrieves first TerminalNode corresponding to token DATABASES
/// Returns `None` if there is no child corresponding to token DATABASES
fn DATABASES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DATABASES, 0)
}
/// Retrieves first TerminalNode corresponding to token DBPROPERTIES
/// Returns `None` if there is no child corresponding to token DBPROPERTIES
fn DBPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DBPROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token DEFINED
/// Returns `None` if there is no child corresponding to token DEFINED
fn DEFINED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DEFINED, 0)
}
/// Retrieves first TerminalNode corresponding to token DELETE
/// Returns `None` if there is no child corresponding to token DELETE
fn DELETE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DELETE, 0)
}
/// Retrieves first TerminalNode corresponding to token DELIMITED
/// Returns `None` if there is no child corresponding to token DELIMITED
fn DELIMITED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DELIMITED, 0)
}
/// Retrieves first TerminalNode corresponding to token DESC
/// Returns `None` if there is no child corresponding to token DESC
fn DESC(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DESC, 0)
}
/// Retrieves first TerminalNode corresponding to token DESCRIBE
/// Returns `None` if there is no child corresponding to token DESCRIBE
fn DESCRIBE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DESCRIBE, 0)
}
/// Retrieves first TerminalNode corresponding to token DFS
/// Returns `None` if there is no child corresponding to token DFS
fn DFS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DFS, 0)
}
/// Retrieves first TerminalNode corresponding to token DIRECTORIES
/// Returns `None` if there is no child corresponding to token DIRECTORIES
fn DIRECTORIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIRECTORIES, 0)
}
/// Retrieves first TerminalNode corresponding to token DIRECTORY
/// Returns `None` if there is no child corresponding to token DIRECTORY
fn DIRECTORY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIRECTORY, 0)
}
/// Retrieves first TerminalNode corresponding to token DISTINCT
/// Returns `None` if there is no child corresponding to token DISTINCT
fn DISTINCT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DISTINCT, 0)
}
/// Retrieves first TerminalNode corresponding to token DISTRIBUTE
/// Returns `None` if there is no child corresponding to token DISTRIBUTE
fn DISTRIBUTE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DISTRIBUTE, 0)
}
/// Retrieves first TerminalNode corresponding to token DIV
/// Returns `None` if there is no child corresponding to token DIV
fn DIV(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DIV, 0)
}
/// Retrieves first TerminalNode corresponding to token DROP
/// Returns `None` if there is no child corresponding to token DROP
fn DROP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(DROP, 0)
}
/// Retrieves first TerminalNode corresponding to token ELSE
/// Returns `None` if there is no child corresponding to token ELSE
fn ELSE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ELSE, 0)
}
/// Retrieves first TerminalNode corresponding to token END
/// Returns `None` if there is no child corresponding to token END
fn END(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(END, 0)
}
/// Retrieves first TerminalNode corresponding to token ESCAPE
/// Returns `None` if there is no child corresponding to token ESCAPE
fn ESCAPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ESCAPE, 0)
}
/// Retrieves first TerminalNode corresponding to token ESCAPED
/// Returns `None` if there is no child corresponding to token ESCAPED
fn ESCAPED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ESCAPED, 0)
}
/// Retrieves first TerminalNode corresponding to token EXCHANGE
/// Returns `None` if there is no child corresponding to token EXCHANGE
fn EXCHANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXCHANGE, 0)
}
/// Retrieves first TerminalNode corresponding to token EXISTS
/// Returns `None` if there is no child corresponding to token EXISTS
fn EXISTS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXISTS, 0)
}
/// Retrieves first TerminalNode corresponding to token EXPLAIN
/// Returns `None` if there is no child corresponding to token EXPLAIN
fn EXPLAIN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXPLAIN, 0)
}
/// Retrieves first TerminalNode corresponding to token EXPORT
/// Returns `None` if there is no child corresponding to token EXPORT
fn EXPORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXPORT, 0)
}
/// Retrieves first TerminalNode corresponding to token EXTENDED
/// Returns `None` if there is no child corresponding to token EXTENDED
fn EXTENDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXTENDED, 0)
}
/// Retrieves first TerminalNode corresponding to token EXTERNAL
/// Returns `None` if there is no child corresponding to token EXTERNAL
fn EXTERNAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXTERNAL, 0)
}
/// Retrieves first TerminalNode corresponding to token EXTRACT
/// Returns `None` if there is no child corresponding to token EXTRACT
fn EXTRACT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(EXTRACT, 0)
}
/// Retrieves first TerminalNode corresponding to token FALSE
/// Returns `None` if there is no child corresponding to token FALSE
fn FALSE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FALSE, 0)
}
/// Retrieves first TerminalNode corresponding to token FETCH
/// Returns `None` if there is no child corresponding to token FETCH
fn FETCH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FETCH, 0)
}
/// Retrieves first TerminalNode corresponding to token FILTER
/// Returns `None` if there is no child corresponding to token FILTER
fn FILTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FILTER, 0)
}
/// Retrieves first TerminalNode corresponding to token FIELDS
/// Returns `None` if there is no child corresponding to token FIELDS
fn FIELDS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FIELDS, 0)
}
/// Retrieves first TerminalNode corresponding to token FILEFORMAT
/// Returns `None` if there is no child corresponding to token FILEFORMAT
fn FILEFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FILEFORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token FIRST
/// Returns `None` if there is no child corresponding to token FIRST
fn FIRST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FIRST, 0)
}
/// Retrieves first TerminalNode corresponding to token FOLLOWING
/// Returns `None` if there is no child corresponding to token FOLLOWING
fn FOLLOWING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FOLLOWING, 0)
}
/// Retrieves first TerminalNode corresponding to token FOR
/// Returns `None` if there is no child corresponding to token FOR
fn FOR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FOR, 0)
}
/// Retrieves first TerminalNode corresponding to token FOREIGN
/// Returns `None` if there is no child corresponding to token FOREIGN
fn FOREIGN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FOREIGN, 0)
}
/// Retrieves first TerminalNode corresponding to token FORMAT
/// Returns `None` if there is no child corresponding to token FORMAT
fn FORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token FORMATTED
/// Returns `None` if there is no child corresponding to token FORMATTED
fn FORMATTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FORMATTED, 0)
}
/// Retrieves first TerminalNode corresponding to token FROM
/// Returns `None` if there is no child corresponding to token FROM
fn FROM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FROM, 0)
}
/// Retrieves first TerminalNode corresponding to token FUNCTION
/// Returns `None` if there is no child corresponding to token FUNCTION
fn FUNCTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FUNCTION, 0)
}
/// Retrieves first TerminalNode corresponding to token FUNCTIONS
/// Returns `None` if there is no child corresponding to token FUNCTIONS
fn FUNCTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(FUNCTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token GLOBAL
/// Returns `None` if there is no child corresponding to token GLOBAL
fn GLOBAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GLOBAL, 0)
}
/// Retrieves first TerminalNode corresponding to token GRANT
/// Returns `None` if there is no child corresponding to token GRANT
fn GRANT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GRANT, 0)
}
/// Retrieves first TerminalNode corresponding to token GROUP
/// Returns `None` if there is no child corresponding to token GROUP
fn GROUP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GROUP, 0)
}
/// Retrieves first TerminalNode corresponding to token GROUPING
/// Returns `None` if there is no child corresponding to token GROUPING
fn GROUPING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(GROUPING, 0)
}
/// Retrieves first TerminalNode corresponding to token HAVING
/// Returns `None` if there is no child corresponding to token HAVING
fn HAVING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(HAVING, 0)
}
/// Retrieves first TerminalNode corresponding to token IF
/// Returns `None` if there is no child corresponding to token IF
fn IF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IF, 0)
}
/// Retrieves first TerminalNode corresponding to token IGNORE
/// Returns `None` if there is no child corresponding to token IGNORE
fn IGNORE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IGNORE, 0)
}
/// Retrieves first TerminalNode corresponding to token IMPORT
/// Returns `None` if there is no child corresponding to token IMPORT
fn IMPORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IMPORT, 0)
}
/// Retrieves first TerminalNode corresponding to token IN
/// Returns `None` if there is no child corresponding to token IN
fn IN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IN, 0)
}
/// Retrieves first TerminalNode corresponding to token INDEX
/// Returns `None` if there is no child corresponding to token INDEX
fn INDEX(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INDEX, 0)
}
/// Retrieves first TerminalNode corresponding to token INDEXES
/// Returns `None` if there is no child corresponding to token INDEXES
fn INDEXES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INDEXES, 0)
}
/// Retrieves first TerminalNode corresponding to token INPATH
/// Returns `None` if there is no child corresponding to token INPATH
fn INPATH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INPATH, 0)
}
/// Retrieves first TerminalNode corresponding to token INPUTFORMAT
/// Returns `None` if there is no child corresponding to token INPUTFORMAT
fn INPUTFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INPUTFORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token INSERT
/// Returns `None` if there is no child corresponding to token INSERT
fn INSERT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INSERT, 0)
}
/// Retrieves first TerminalNode corresponding to token INTERVAL
/// Returns `None` if there is no child corresponding to token INTERVAL
fn INTERVAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTERVAL, 0)
}
/// Retrieves first TerminalNode corresponding to token INTO
/// Returns `None` if there is no child corresponding to token INTO
fn INTO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(INTO, 0)
}
/// Retrieves first TerminalNode corresponding to token IS
/// Returns `None` if there is no child corresponding to token IS
fn IS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(IS, 0)
}
/// Retrieves first TerminalNode corresponding to token ITEMS
/// Returns `None` if there is no child corresponding to token ITEMS
fn ITEMS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ITEMS, 0)
}
/// Retrieves first TerminalNode corresponding to token KEYS
/// Returns `None` if there is no child corresponding to token KEYS
fn KEYS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(KEYS, 0)
}
/// Retrieves first TerminalNode corresponding to token LAST
/// Returns `None` if there is no child corresponding to token LAST
fn LAST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LAST, 0)
}
/// Retrieves first TerminalNode corresponding to token LATERAL
/// Returns `None` if there is no child corresponding to token LATERAL
fn LATERAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LATERAL, 0)
}
/// Retrieves first TerminalNode corresponding to token LAZY
/// Returns `None` if there is no child corresponding to token LAZY
fn LAZY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LAZY, 0)
}
/// Retrieves first TerminalNode corresponding to token LEADING
/// Returns `None` if there is no child corresponding to token LEADING
fn LEADING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LEADING, 0)
}
/// Retrieves first TerminalNode corresponding to token LIKE
/// Returns `None` if there is no child corresponding to token LIKE
fn LIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIKE, 0)
}
/// Retrieves first TerminalNode corresponding to token LIMIT
/// Returns `None` if there is no child corresponding to token LIMIT
fn LIMIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIMIT, 0)
}
/// Retrieves first TerminalNode corresponding to token LINES
/// Returns `None` if there is no child corresponding to token LINES
fn LINES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LINES, 0)
}
/// Retrieves first TerminalNode corresponding to token LIST
/// Returns `None` if there is no child corresponding to token LIST
fn LIST(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LIST, 0)
}
/// Retrieves first TerminalNode corresponding to token LOAD
/// Returns `None` if there is no child corresponding to token LOAD
fn LOAD(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOAD, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCAL
/// Returns `None` if there is no child corresponding to token LOCAL
fn LOCAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCAL, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCATION
/// Returns `None` if there is no child corresponding to token LOCATION
fn LOCATION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCATION, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCK
/// Returns `None` if there is no child corresponding to token LOCK
fn LOCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCK, 0)
}
/// Retrieves first TerminalNode corresponding to token LOCKS
/// Returns `None` if there is no child corresponding to token LOCKS
fn LOCKS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOCKS, 0)
}
/// Retrieves first TerminalNode corresponding to token LOGICAL
/// Returns `None` if there is no child corresponding to token LOGICAL
fn LOGICAL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(LOGICAL, 0)
}
/// Retrieves first TerminalNode corresponding to token MACRO
/// Returns `None` if there is no child corresponding to token MACRO
fn MACRO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MACRO, 0)
}
/// Retrieves first TerminalNode corresponding to token MAP
/// Returns `None` if there is no child corresponding to token MAP
fn MAP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MAP, 0)
}
/// Retrieves first TerminalNode corresponding to token MATCHED
/// Returns `None` if there is no child corresponding to token MATCHED
fn MATCHED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MATCHED, 0)
}
/// Retrieves first TerminalNode corresponding to token MERGE
/// Returns `None` if there is no child corresponding to token MERGE
fn MERGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MERGE, 0)
}
/// Retrieves first TerminalNode corresponding to token MSCK
/// Returns `None` if there is no child corresponding to token MSCK
fn MSCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(MSCK, 0)
}
/// Retrieves first TerminalNode corresponding to token NAMESPACE
/// Returns `None` if there is no child corresponding to token NAMESPACE
fn NAMESPACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NAMESPACE, 0)
}
/// Retrieves first TerminalNode corresponding to token NAMESPACES
/// Returns `None` if there is no child corresponding to token NAMESPACES
fn NAMESPACES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NAMESPACES, 0)
}
/// Retrieves first TerminalNode corresponding to token NO
/// Returns `None` if there is no child corresponding to token NO
fn NO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NO, 0)
}
/// Retrieves first TerminalNode corresponding to token NOT
/// Returns `None` if there is no child corresponding to token NOT
fn NOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NOT, 0)
}
/// Retrieves first TerminalNode corresponding to token NULL
/// Returns `None` if there is no child corresponding to token NULL
fn NULL(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULL, 0)
}
/// Retrieves first TerminalNode corresponding to token NULLS
/// Returns `None` if there is no child corresponding to token NULLS
fn NULLS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(NULLS, 0)
}
/// Retrieves first TerminalNode corresponding to token OF
/// Returns `None` if there is no child corresponding to token OF
fn OF(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OF, 0)
}
/// Retrieves first TerminalNode corresponding to token ONLY
/// Returns `None` if there is no child corresponding to token ONLY
fn ONLY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ONLY, 0)
}
/// Retrieves first TerminalNode corresponding to token OPTION
/// Returns `None` if there is no child corresponding to token OPTION
fn OPTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OPTION, 0)
}
/// Retrieves first TerminalNode corresponding to token OPTIONS
/// Returns `None` if there is no child corresponding to token OPTIONS
fn OPTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OPTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token OR
/// Returns `None` if there is no child corresponding to token OR
fn OR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OR, 0)
}
/// Retrieves first TerminalNode corresponding to token ORDER
/// Returns `None` if there is no child corresponding to token ORDER
fn ORDER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ORDER, 0)
}
/// Retrieves first TerminalNode corresponding to token OUT
/// Returns `None` if there is no child corresponding to token OUT
fn OUT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OUT, 0)
}
/// Retrieves first TerminalNode corresponding to token OUTER
/// Returns `None` if there is no child corresponding to token OUTER
fn OUTER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OUTER, 0)
}
/// Retrieves first TerminalNode corresponding to token OUTPUTFORMAT
/// Returns `None` if there is no child corresponding to token OUTPUTFORMAT
fn OUTPUTFORMAT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OUTPUTFORMAT, 0)
}
/// Retrieves first TerminalNode corresponding to token OVER
/// Returns `None` if there is no child corresponding to token OVER
fn OVER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OVER, 0)
}
/// Retrieves first TerminalNode corresponding to token OVERLAPS
/// Returns `None` if there is no child corresponding to token OVERLAPS
fn OVERLAPS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OVERLAPS, 0)
}
/// Retrieves first TerminalNode corresponding to token OVERLAY
/// Returns `None` if there is no child corresponding to token OVERLAY
fn OVERLAY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OVERLAY, 0)
}
/// Retrieves first TerminalNode corresponding to token OVERWRITE
/// Returns `None` if there is no child corresponding to token OVERWRITE
fn OVERWRITE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(OVERWRITE, 0)
}
/// Retrieves first TerminalNode corresponding to token PARTITION
/// Returns `None` if there is no child corresponding to token PARTITION
fn PARTITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITION, 0)
}
/// Retrieves first TerminalNode corresponding to token PARTITIONED
/// Returns `None` if there is no child corresponding to token PARTITIONED
fn PARTITIONED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITIONED, 0)
}
/// Retrieves first TerminalNode corresponding to token PARTITIONS
/// Returns `None` if there is no child corresponding to token PARTITIONS
fn PARTITIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PARTITIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token PERCENTLIT
/// Returns `None` if there is no child corresponding to token PERCENTLIT
fn PERCENTLIT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PERCENTLIT, 0)
}
/// Retrieves first TerminalNode corresponding to token PIVOT
/// Returns `None` if there is no child corresponding to token PIVOT
fn PIVOT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PIVOT, 0)
}
/// Retrieves first TerminalNode corresponding to token PLACING
/// Returns `None` if there is no child corresponding to token PLACING
fn PLACING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PLACING, 0)
}
/// Retrieves first TerminalNode corresponding to token POSITION
/// Returns `None` if there is no child corresponding to token POSITION
fn POSITION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(POSITION, 0)
}
/// Retrieves first TerminalNode corresponding to token PRECEDING
/// Returns `None` if there is no child corresponding to token PRECEDING
fn PRECEDING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PRECEDING, 0)
}
/// Retrieves first TerminalNode corresponding to token PRIMARY
/// Returns `None` if there is no child corresponding to token PRIMARY
fn PRIMARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PRIMARY, 0)
}
/// Retrieves first TerminalNode corresponding to token PRINCIPALS
/// Returns `None` if there is no child corresponding to token PRINCIPALS
fn PRINCIPALS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PRINCIPALS, 0)
}
/// Retrieves first TerminalNode corresponding to token PROPERTIES
/// Returns `None` if there is no child corresponding to token PROPERTIES
fn PROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token PURGE
/// Returns `None` if there is no child corresponding to token PURGE
fn PURGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(PURGE, 0)
}
/// Retrieves first TerminalNode corresponding to token QUERY
/// Returns `None` if there is no child corresponding to token QUERY
fn QUERY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(QUERY, 0)
}
/// Retrieves first TerminalNode corresponding to token RANGE
/// Returns `None` if there is no child corresponding to token RANGE
fn RANGE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RANGE, 0)
}
/// Retrieves first TerminalNode corresponding to token RECORDREADER
/// Returns `None` if there is no child corresponding to token RECORDREADER
fn RECORDREADER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECORDREADER, 0)
}
/// Retrieves first TerminalNode corresponding to token RECORDWRITER
/// Returns `None` if there is no child corresponding to token RECORDWRITER
fn RECORDWRITER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECORDWRITER, 0)
}
/// Retrieves first TerminalNode corresponding to token RECOVER
/// Returns `None` if there is no child corresponding to token RECOVER
fn RECOVER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RECOVER, 0)
}
/// Retrieves first TerminalNode corresponding to token REDUCE
/// Returns `None` if there is no child corresponding to token REDUCE
fn REDUCE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REDUCE, 0)
}
/// Retrieves first TerminalNode corresponding to token REFERENCES
/// Returns `None` if there is no child corresponding to token REFERENCES
fn REFERENCES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REFERENCES, 0)
}
/// Retrieves first TerminalNode corresponding to token REFRESH
/// Returns `None` if there is no child corresponding to token REFRESH
fn REFRESH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REFRESH, 0)
}
/// Retrieves first TerminalNode corresponding to token RENAME
/// Returns `None` if there is no child corresponding to token RENAME
fn RENAME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RENAME, 0)
}
/// Retrieves first TerminalNode corresponding to token REPAIR
/// Returns `None` if there is no child corresponding to token REPAIR
fn REPAIR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REPAIR, 0)
}
/// Retrieves first TerminalNode corresponding to token REPLACE
/// Returns `None` if there is no child corresponding to token REPLACE
fn REPLACE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REPLACE, 0)
}
/// Retrieves first TerminalNode corresponding to token RESET
/// Returns `None` if there is no child corresponding to token RESET
fn RESET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RESET, 0)
}
/// Retrieves first TerminalNode corresponding to token RESTRICT
/// Returns `None` if there is no child corresponding to token RESTRICT
fn RESTRICT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RESTRICT, 0)
}
/// Retrieves first TerminalNode corresponding to token REVOKE
/// Returns `None` if there is no child corresponding to token REVOKE
fn REVOKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(REVOKE, 0)
}
/// Retrieves first TerminalNode corresponding to token RLIKE
/// Returns `None` if there is no child corresponding to token RLIKE
fn RLIKE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(RLIKE, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLE
/// Returns `None` if there is no child corresponding to token ROLE
fn ROLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLE, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLES
/// Returns `None` if there is no child corresponding to token ROLES
fn ROLES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLES, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLLBACK
/// Returns `None` if there is no child corresponding to token ROLLBACK
fn ROLLBACK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLLBACK, 0)
}
/// Retrieves first TerminalNode corresponding to token ROLLUP
/// Returns `None` if there is no child corresponding to token ROLLUP
fn ROLLUP(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROLLUP, 0)
}
/// Retrieves first TerminalNode corresponding to token ROW
/// Returns `None` if there is no child corresponding to token ROW
fn ROW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROW, 0)
}
/// Retrieves first TerminalNode corresponding to token ROWS
/// Returns `None` if there is no child corresponding to token ROWS
fn ROWS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ROWS, 0)
}
/// Retrieves first TerminalNode corresponding to token SCHEMA
/// Returns `None` if there is no child corresponding to token SCHEMA
fn SCHEMA(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SCHEMA, 0)
}
/// Retrieves first TerminalNode corresponding to token SELECT
/// Returns `None` if there is no child corresponding to token SELECT
fn SELECT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SELECT, 0)
}
/// Retrieves first TerminalNode corresponding to token SEPARATED
/// Returns `None` if there is no child corresponding to token SEPARATED
fn SEPARATED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SEPARATED, 0)
}
/// Retrieves first TerminalNode corresponding to token SERDE
/// Returns `None` if there is no child corresponding to token SERDE
fn SERDE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SERDE, 0)
}
/// Retrieves first TerminalNode corresponding to token SERDEPROPERTIES
/// Returns `None` if there is no child corresponding to token SERDEPROPERTIES
fn SERDEPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SERDEPROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token SESSION_USER
/// Returns `None` if there is no child corresponding to token SESSION_USER
fn SESSION_USER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SESSION_USER, 0)
}
/// Retrieves first TerminalNode corresponding to token SET
/// Returns `None` if there is no child corresponding to token SET
fn SET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SET, 0)
}
/// Retrieves first TerminalNode corresponding to token SETS
/// Returns `None` if there is no child corresponding to token SETS
fn SETS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SETS, 0)
}
/// Retrieves first TerminalNode corresponding to token SHOW
/// Returns `None` if there is no child corresponding to token SHOW
fn SHOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SHOW, 0)
}
/// Retrieves first TerminalNode corresponding to token SKEWED
/// Returns `None` if there is no child corresponding to token SKEWED
fn SKEWED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SKEWED, 0)
}
/// Retrieves first TerminalNode corresponding to token SOME
/// Returns `None` if there is no child corresponding to token SOME
fn SOME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SOME, 0)
}
/// Retrieves first TerminalNode corresponding to token SORT
/// Returns `None` if there is no child corresponding to token SORT
fn SORT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SORT, 0)
}
/// Retrieves first TerminalNode corresponding to token SORTED
/// Returns `None` if there is no child corresponding to token SORTED
fn SORTED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SORTED, 0)
}
/// Retrieves first TerminalNode corresponding to token START
/// Returns `None` if there is no child corresponding to token START
fn START(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(START, 0)
}
/// Retrieves first TerminalNode corresponding to token STATISTICS
/// Returns `None` if there is no child corresponding to token STATISTICS
fn STATISTICS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STATISTICS, 0)
}
/// Retrieves first TerminalNode corresponding to token STORED
/// Returns `None` if there is no child corresponding to token STORED
fn STORED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STORED, 0)
}
/// Retrieves first TerminalNode corresponding to token STRATIFY
/// Returns `None` if there is no child corresponding to token STRATIFY
fn STRATIFY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRATIFY, 0)
}
/// Retrieves first TerminalNode corresponding to token STRUCT
/// Returns `None` if there is no child corresponding to token STRUCT
fn STRUCT(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(STRUCT, 0)
}
/// Retrieves first TerminalNode corresponding to token SUBSTR
/// Returns `None` if there is no child corresponding to token SUBSTR
fn SUBSTR(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SUBSTR, 0)
}
/// Retrieves first TerminalNode corresponding to token SUBSTRING
/// Returns `None` if there is no child corresponding to token SUBSTRING
fn SUBSTRING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(SUBSTRING, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLE
/// Returns `None` if there is no child corresponding to token TABLE
fn TABLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLE, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLES
/// Returns `None` if there is no child corresponding to token TABLES
fn TABLES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLES, 0)
}
/// Retrieves first TerminalNode corresponding to token TABLESAMPLE
/// Returns `None` if there is no child corresponding to token TABLESAMPLE
fn TABLESAMPLE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TABLESAMPLE, 0)
}
/// Retrieves first TerminalNode corresponding to token TBLPROPERTIES
/// Returns `None` if there is no child corresponding to token TBLPROPERTIES
fn TBLPROPERTIES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TBLPROPERTIES, 0)
}
/// Retrieves first TerminalNode corresponding to token TEMPORARY
/// Returns `None` if there is no child corresponding to token TEMPORARY
fn TEMPORARY(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TEMPORARY, 0)
}
/// Retrieves first TerminalNode corresponding to token TERMINATED
/// Returns `None` if there is no child corresponding to token TERMINATED
fn TERMINATED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TERMINATED, 0)
}
/// Retrieves first TerminalNode corresponding to token THEN
/// Returns `None` if there is no child corresponding to token THEN
fn THEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(THEN, 0)
}
/// Retrieves first TerminalNode corresponding to token TIME
/// Returns `None` if there is no child corresponding to token TIME
fn TIME(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TIME, 0)
}
/// Retrieves first TerminalNode corresponding to token TO
/// Returns `None` if there is no child corresponding to token TO
fn TO(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TO, 0)
}
/// Retrieves first TerminalNode corresponding to token TOUCH
/// Returns `None` if there is no child corresponding to token TOUCH
fn TOUCH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TOUCH, 0)
}
/// Retrieves first TerminalNode corresponding to token TRAILING
/// Returns `None` if there is no child corresponding to token TRAILING
fn TRAILING(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRAILING, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSACTION
/// Returns `None` if there is no child corresponding to token TRANSACTION
fn TRANSACTION(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSACTION, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSACTIONS
/// Returns `None` if there is no child corresponding to token TRANSACTIONS
fn TRANSACTIONS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSACTIONS, 0)
}
/// Retrieves first TerminalNode corresponding to token TRANSFORM
/// Returns `None` if there is no child corresponding to token TRANSFORM
fn TRANSFORM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRANSFORM, 0)
}
/// Retrieves first TerminalNode corresponding to token TRIM
/// Returns `None` if there is no child corresponding to token TRIM
fn TRIM(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRIM, 0)
}
/// Retrieves first TerminalNode corresponding to token TRUE
/// Returns `None` if there is no child corresponding to token TRUE
fn TRUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRUE, 0)
}
/// Retrieves first TerminalNode corresponding to token TRUNCATE
/// Returns `None` if there is no child corresponding to token TRUNCATE
fn TRUNCATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TRUNCATE, 0)
}
/// Retrieves first TerminalNode corresponding to token TYPE
/// Returns `None` if there is no child corresponding to token TYPE
fn TYPE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(TYPE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNARCHIVE
/// Returns `None` if there is no child corresponding to token UNARCHIVE
fn UNARCHIVE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNARCHIVE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNBOUNDED
/// Returns `None` if there is no child corresponding to token UNBOUNDED
fn UNBOUNDED(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNBOUNDED, 0)
}
/// Retrieves first TerminalNode corresponding to token UNCACHE
/// Returns `None` if there is no child corresponding to token UNCACHE
fn UNCACHE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNCACHE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNIQUE
/// Returns `None` if there is no child corresponding to token UNIQUE
fn UNIQUE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNIQUE, 0)
}
/// Retrieves first TerminalNode corresponding to token UNKNOWN
/// Returns `None` if there is no child corresponding to token UNKNOWN
fn UNKNOWN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNKNOWN, 0)
}
/// Retrieves first TerminalNode corresponding to token UNLOCK
/// Returns `None` if there is no child corresponding to token UNLOCK
fn UNLOCK(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNLOCK, 0)
}
/// Retrieves first TerminalNode corresponding to token UNSET
/// Returns `None` if there is no child corresponding to token UNSET
fn UNSET(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UNSET, 0)
}
/// Retrieves first TerminalNode corresponding to token UPDATE
/// Returns `None` if there is no child corresponding to token UPDATE
fn UPDATE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(UPDATE, 0)
}
/// Retrieves first TerminalNode corresponding to token USE
/// Returns `None` if there is no child corresponding to token USE
fn USE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(USE, 0)
}
/// Retrieves first TerminalNode corresponding to token USER
/// Returns `None` if there is no child corresponding to token USER
fn USER(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(USER, 0)
}
/// Retrieves first TerminalNode corresponding to token VALUES
/// Returns `None` if there is no child corresponding to token VALUES
fn VALUES(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VALUES, 0)
}
/// Retrieves first TerminalNode corresponding to token VIEW
/// Returns `None` if there is no child corresponding to token VIEW
fn VIEW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VIEW, 0)
}
/// Retrieves first TerminalNode corresponding to token VIEWS
/// Returns `None` if there is no child corresponding to token VIEWS
fn VIEWS(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(VIEWS, 0)
}
/// Retrieves first TerminalNode corresponding to token WHEN
/// Returns `None` if there is no child corresponding to token WHEN
fn WHEN(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WHEN, 0)
}
/// Retrieves first TerminalNode corresponding to token WHERE
/// Returns `None` if there is no child corresponding to token WHERE
fn WHERE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WHERE, 0)
}
/// Retrieves first TerminalNode corresponding to token WINDOW
/// Returns `None` if there is no child corresponding to token WINDOW
fn WINDOW(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WINDOW, 0)
}
/// Retrieves first TerminalNode corresponding to token WITH
/// Returns `None` if there is no child corresponding to token WITH
fn WITH(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(WITH, 0)
}
/// Retrieves first TerminalNode corresponding to token ZONE
/// Returns `None` if there is no child corresponding to token ZONE
fn ZONE(&self) -> Option<Rc<TerminalNode<'input,SparkSqlParserContextType>>> where Self:Sized{
	self.get_token(ZONE, 0)
}

}

impl<'input> NonReservedContextAttrs<'input> for NonReservedContext<'input>{}

impl<'input, I, H> SparkSqlParser<'input, I, H>
where
    I: TokenStream<'input, TF = LocalTokenFactory<'input> > + TidAble<'input>,
    H: ErrorStrategy<'input,BaseParserType<'input,I>>
{
	pub fn nonReserved(&mut self,)
	-> Result<Rc<NonReservedContextAll<'input>>,ANTLRError> {
		let mut recog = self;
		let _parentctx = recog.ctx.take();
		let mut _localctx = NonReservedContextExt::new(_parentctx.clone(), recog.base.get_state());
        recog.base.enter_rule(_localctx.clone(), 272, RULE_nonReserved);
        let mut _localctx: Rc<NonReservedContextAll> = _localctx;
		let mut _la: isize = -1;
		let result: Result<(), ANTLRError> = (|| {

			//recog.base.enter_outer_alt(_localctx.clone(), 1);
			recog.base.enter_outer_alt(None, 1);
			{
			recog.base.set_state(3028);
			_la = recog.base.input.la(1);
			if { !(((((_la - 11)) & !0x3f) == 0 && ((1usize << (_la - 11)) & ((1usize << (ADD - 11)) | (1usize << (AFTER - 11)) | (1usize << (ALL - 11)) | (1usize << (ALTER - 11)) | (1usize << (ANALYZE - 11)) | (1usize << (AND - 11)) | (1usize << (ANY - 11)) | (1usize << (ARCHIVE - 11)) | (1usize << (ARRAY - 11)) | (1usize << (AS - 11)) | (1usize << (ASC - 11)) | (1usize << (AT - 11)) | (1usize << (AUTHORIZATION - 11)) | (1usize << (BETWEEN - 11)) | (1usize << (BOTH - 11)) | (1usize << (BUCKET - 11)) | (1usize << (BUCKETS - 11)) | (1usize << (BY - 11)) | (1usize << (CACHE - 11)) | (1usize << (CASCADE - 11)) | (1usize << (CASE - 11)) | (1usize << (CAST - 11)) | (1usize << (CHANGE - 11)) | (1usize << (CHECK - 11)) | (1usize << (CLEAR - 11)) | (1usize << (CLUSTER - 11)) | (1usize << (CLUSTERED - 11)) | (1usize << (CODEGEN - 11)) | (1usize << (COLLATE - 11)) | (1usize << (COLLECTION - 11)) | (1usize << (COLUMN - 11)))) != 0) || ((((_la - 43)) & !0x3f) == 0 && ((1usize << (_la - 43)) & ((1usize << (COLUMNS - 43)) | (1usize << (COMMENT - 43)) | (1usize << (COMMIT - 43)) | (1usize << (COMPACT - 43)) | (1usize << (COMPACTIONS - 43)) | (1usize << (COMPUTE - 43)) | (1usize << (CONCATENATE - 43)) | (1usize << (CONSTRAINT - 43)) | (1usize << (COST - 43)) | (1usize << (CREATE - 43)) | (1usize << (CUBE - 43)) | (1usize << (CURRENT - 43)) | (1usize << (CURRENT_DATE - 43)) | (1usize << (CURRENT_TIME - 43)) | (1usize << (CURRENT_TIMESTAMP - 43)) | (1usize << (CURRENT_USER - 43)) | (1usize << (DATA - 43)) | (1usize << (DATABASE - 43)) | (1usize << (DATABASES - 43)) | (1usize << (DBPROPERTIES - 43)) | (1usize << (DEFINED - 43)) | (1usize << (DELETE - 43)) | (1usize << (DELIMITED - 43)) | (1usize << (DESC - 43)) | (1usize << (DESCRIBE - 43)) | (1usize << (DFS - 43)) | (1usize << (DIRECTORIES - 43)) | (1usize << (DIRECTORY - 43)) | (1usize << (DISTINCT - 43)) | (1usize << (DISTRIBUTE - 43)) | (1usize << (DIV - 43)))) != 0) || ((((_la - 75)) & !0x3f) == 0 && ((1usize << (_la - 75)) & ((1usize << (DROP - 75)) | (1usize << (ELSE - 75)) | (1usize << (END - 75)) | (1usize << (ESCAPE - 75)) | (1usize << (ESCAPED - 75)) | (1usize << (EXCHANGE - 75)) | (1usize << (EXISTS - 75)) | (1usize << (EXPLAIN - 75)) | (1usize << (EXPORT - 75)) | (1usize << (EXTENDED - 75)) | (1usize << (EXTERNAL - 75)) | (1usize << (EXTRACT - 75)) | (1usize << (FALSE - 75)) | (1usize << (FETCH - 75)) | (1usize << (FIELDS - 75)) | (1usize << (FILTER - 75)) | (1usize << (FILEFORMAT - 75)) | (1usize << (FIRST - 75)) | (1usize << (FOLLOWING - 75)) | (1usize << (FOR - 75)) | (1usize << (FOREIGN - 75)) | (1usize << (FORMAT - 75)) | (1usize << (FORMATTED - 75)) | (1usize << (FROM - 75)) | (1usize << (FUNCTION - 75)) | (1usize << (FUNCTIONS - 75)) | (1usize << (GLOBAL - 75)) | (1usize << (GRANT - 75)) | (1usize << (GROUP - 75)) | (1usize << (GROUPING - 75)))) != 0) || ((((_la - 107)) & !0x3f) == 0 && ((1usize << (_la - 107)) & ((1usize << (HAVING - 107)) | (1usize << (IF - 107)) | (1usize << (IGNORE - 107)) | (1usize << (IMPORT - 107)) | (1usize << (IN - 107)) | (1usize << (INDEX - 107)) | (1usize << (INDEXES - 107)) | (1usize << (INPATH - 107)) | (1usize << (INPUTFORMAT - 107)) | (1usize << (INSERT - 107)) | (1usize << (INTERVAL - 107)) | (1usize << (INTO - 107)) | (1usize << (IS - 107)) | (1usize << (ITEMS - 107)) | (1usize << (KEYS - 107)) | (1usize << (LAST - 107)) | (1usize << (LATERAL - 107)) | (1usize << (LAZY - 107)) | (1usize << (LEADING - 107)) | (1usize << (LIKE - 107)) | (1usize << (LIMIT - 107)) | (1usize << (LINES - 107)) | (1usize << (LIST - 107)) | (1usize << (LOAD - 107)) | (1usize << (LOCAL - 107)) | (1usize << (LOCATION - 107)) | (1usize << (LOCK - 107)) | (1usize << (LOCKS - 107)))) != 0) || ((((_la - 139)) & !0x3f) == 0 && ((1usize << (_la - 139)) & ((1usize << (LOGICAL - 139)) | (1usize << (MACRO - 139)) | (1usize << (MAP - 139)) | (1usize << (MATCHED - 139)) | (1usize << (MERGE - 139)) | (1usize << (MSCK - 139)) | (1usize << (NAMESPACE - 139)) | (1usize << (NAMESPACES - 139)) | (1usize << (NO - 139)) | (1usize << (NOT - 139)) | (1usize << (NULL - 139)) | (1usize << (NULLS - 139)) | (1usize << (OF - 139)) | (1usize << (ONLY - 139)) | (1usize << (OPTION - 139)) | (1usize << (OPTIONS - 139)) | (1usize << (OR - 139)) | (1usize << (ORDER - 139)) | (1usize << (OUT - 139)) | (1usize << (OUTER - 139)) | (1usize << (OUTPUTFORMAT - 139)) | (1usize << (OVER - 139)) | (1usize << (OVERLAPS - 139)) | (1usize << (OVERLAY - 139)) | (1usize << (OVERWRITE - 139)) | (1usize << (PARTITION - 139)) | (1usize << (PARTITIONED - 139)) | (1usize << (PARTITIONS - 139)) | (1usize << (PERCENTLIT - 139)) | (1usize << (PIVOT - 139)))) != 0) || ((((_la - 171)) & !0x3f) == 0 && ((1usize << (_la - 171)) & ((1usize << (PLACING - 171)) | (1usize << (POSITION - 171)) | (1usize << (PRECEDING - 171)) | (1usize << (PRIMARY - 171)) | (1usize << (PRINCIPALS - 171)) | (1usize << (PROPERTIES - 171)) | (1usize << (PURGE - 171)) | (1usize << (QUERY - 171)) | (1usize << (RANGE - 171)) | (1usize << (RECORDREADER - 171)) | (1usize << (RECORDWRITER - 171)) | (1usize << (RECOVER - 171)) | (1usize << (REDUCE - 171)) | (1usize << (REFERENCES - 171)) | (1usize << (REFRESH - 171)) | (1usize << (RENAME - 171)) | (1usize << (REPAIR - 171)) | (1usize << (REPLACE - 171)) | (1usize << (RESET - 171)) | (1usize << (RESTRICT - 171)) | (1usize << (REVOKE - 171)) | (1usize << (RLIKE - 171)) | (1usize << (ROLE - 171)) | (1usize << (ROLES - 171)) | (1usize << (ROLLBACK - 171)) | (1usize << (ROLLUP - 171)) | (1usize << (ROW - 171)) | (1usize << (ROWS - 171)) | (1usize << (SCHEMA - 171)) | (1usize << (SELECT - 171)))) != 0) || ((((_la - 203)) & !0x3f) == 0 && ((1usize << (_la - 203)) & ((1usize << (SEPARATED - 203)) | (1usize << (SERDE - 203)) | (1usize << (SERDEPROPERTIES - 203)) | (1usize << (SESSION_USER - 203)) | (1usize << (SET - 203)) | (1usize << (SETS - 203)) | (1usize << (SHOW - 203)) | (1usize << (SKEWED - 203)) | (1usize << (SOME - 203)) | (1usize << (SORT - 203)) | (1usize << (SORTED - 203)) | (1usize << (START - 203)) | (1usize << (STATISTICS - 203)) | (1usize << (STORED - 203)) | (1usize << (STRATIFY - 203)) | (1usize << (STRUCT - 203)) | (1usize << (SUBSTR - 203)) | (1usize << (SUBSTRING - 203)) | (1usize << (TABLE - 203)) | (1usize << (TABLES - 203)) | (1usize << (TABLESAMPLE - 203)) | (1usize << (TBLPROPERTIES - 203)) | (1usize << (TEMPORARY - 203)) | (1usize << (TERMINATED - 203)) | (1usize << (THEN - 203)) | (1usize << (TIME - 203)) | (1usize << (TO - 203)) | (1usize << (TOUCH - 203)) | (1usize << (TRAILING - 203)) | (1usize << (TRANSACTION - 203)) | (1usize << (TRANSACTIONS - 203)))) != 0) || ((((_la - 235)) & !0x3f) == 0 && ((1usize << (_la - 235)) & ((1usize << (TRANSFORM - 235)) | (1usize << (TRIM - 235)) | (1usize << (TRUE - 235)) | (1usize << (TRUNCATE - 235)) | (1usize << (TYPE - 235)) | (1usize << (UNARCHIVE - 235)) | (1usize << (UNBOUNDED - 235)) | (1usize << (UNCACHE - 235)) | (1usize << (UNIQUE - 235)) | (1usize << (UNKNOWN - 235)) | (1usize << (UNLOCK - 235)) | (1usize << (UNSET - 235)) | (1usize << (UPDATE - 235)) | (1usize << (USE - 235)) | (1usize << (USER - 235)) | (1usize << (VALUES - 235)) | (1usize << (VIEW - 235)) | (1usize << (VIEWS - 235)) | (1usize << (WHEN - 235)) | (1usize << (WHERE - 235)) | (1usize << (WINDOW - 235)) | (1usize << (WITH - 235)) | (1usize << (ZONE - 235)))) != 0)) } {
				recog.err_handler.recover_inline(&mut recog.base)?;

			}
			else {
				if  recog.base.input.la(1)==TOKEN_EOF { recog.base.matched_eof = true };
				recog.err_handler.report_match(&mut recog.base);
				recog.base.consume(&mut recog.err_handler);
			}
			}
			Ok(())
		})();
		match result {
		Ok(_)=>{},
        Err(e @ ANTLRError::FallThrough(_)) => return Err(e),
		Err(ref re) => {
				//_localctx.exception = re;
				recog.err_handler.report_error(&mut recog.base, re);
				recog.err_handler.recover(&mut recog.base, re)?;
			}
		}
		recog.base.exit_rule();

		Ok(_localctx)
	}
}

lazy_static! {
    static ref _ATN: Arc<ATN> =
        Arc::new(ATNDeserializer::new(None).deserialize(_serializedATN.chars()));
    static ref _decision_to_DFA: Arc<Vec<crate::RwLock<DFA>>> = {
        let mut dfa = Vec::new();
        let size = _ATN.decision_to_state.len();
        for i in 0..size {
            dfa.push(DFA::new(
                _ATN.clone(),
                _ATN.get_decision_state(i),
                i as isize,
            ).into())
        }
        Arc::new(dfa)
    };
}



const _serializedATN:&'static str =
	"\x03\u{608b}\u{a72a}\u{8133}\u{b9ed}\u{417c}\u{3be7}\u{7786}\u{5964}\x03\
	\u{129}\u{bd9}\x04\x02\x09\x02\x04\x03\x09\x03\x04\x04\x09\x04\x04\x05\x09\
	\x05\x04\x06\x09\x06\x04\x07\x09\x07\x04\x08\x09\x08\x04\x09\x09\x09\x04\
	\x0a\x09\x0a\x04\x0b\x09\x0b\x04\x0c\x09\x0c\x04\x0d\x09\x0d\x04\x0e\x09\
	\x0e\x04\x0f\x09\x0f\x04\x10\x09\x10\x04\x11\x09\x11\x04\x12\x09\x12\x04\
	\x13\x09\x13\x04\x14\x09\x14\x04\x15\x09\x15\x04\x16\x09\x16\x04\x17\x09\
	\x17\x04\x18\x09\x18\x04\x19\x09\x19\x04\x1a\x09\x1a\x04\x1b\x09\x1b\x04\
	\x1c\x09\x1c\x04\x1d\x09\x1d\x04\x1e\x09\x1e\x04\x1f\x09\x1f\x04\x20\x09\
	\x20\x04\x21\x09\x21\x04\x22\x09\x22\x04\x23\x09\x23\x04\x24\x09\x24\x04\
	\x25\x09\x25\x04\x26\x09\x26\x04\x27\x09\x27\x04\x28\x09\x28\x04\x29\x09\
	\x29\x04\x2a\x09\x2a\x04\x2b\x09\x2b\x04\x2c\x09\x2c\x04\x2d\x09\x2d\x04\
	\x2e\x09\x2e\x04\x2f\x09\x2f\x04\x30\x09\x30\x04\x31\x09\x31\x04\x32\x09\
	\x32\x04\x33\x09\x33\x04\x34\x09\x34\x04\x35\x09\x35\x04\x36\x09\x36\x04\
	\x37\x09\x37\x04\x38\x09\x38\x04\x39\x09\x39\x04\x3a\x09\x3a\x04\x3b\x09\
	\x3b\x04\x3c\x09\x3c\x04\x3d\x09\x3d\x04\x3e\x09\x3e\x04\x3f\x09\x3f\x04\
	\x40\x09\x40\x04\x41\x09\x41\x04\x42\x09\x42\x04\x43\x09\x43\x04\x44\x09\
	\x44\x04\x45\x09\x45\x04\x46\x09\x46\x04\x47\x09\x47\x04\x48\x09\x48\x04\
	\x49\x09\x49\x04\x4a\x09\x4a\x04\x4b\x09\x4b\x04\x4c\x09\x4c\x04\x4d\x09\
	\x4d\x04\x4e\x09\x4e\x04\x4f\x09\x4f\x04\x50\x09\x50\x04\x51\x09\x51\x04\
	\x52\x09\x52\x04\x53\x09\x53\x04\x54\x09\x54\x04\x55\x09\x55\x04\x56\x09\
	\x56\x04\x57\x09\x57\x04\x58\x09\x58\x04\x59\x09\x59\x04\x5a\x09\x5a\x04\
	\x5b\x09\x5b\x04\x5c\x09\x5c\x04\x5d\x09\x5d\x04\x5e\x09\x5e\x04\x5f\x09\
	\x5f\x04\x60\x09\x60\x04\x61\x09\x61\x04\x62\x09\x62\x04\x63\x09\x63\x04\
	\x64\x09\x64\x04\x65\x09\x65\x04\x66\x09\x66\x04\x67\x09\x67\x04\x68\x09\
	\x68\x04\x69\x09\x69\x04\x6a\x09\x6a\x04\x6b\x09\x6b\x04\x6c\x09\x6c\x04\
	\x6d\x09\x6d\x04\x6e\x09\x6e\x04\x6f\x09\x6f\x04\x70\x09\x70\x04\x71\x09\
	\x71\x04\x72\x09\x72\x04\x73\x09\x73\x04\x74\x09\x74\x04\x75\x09\x75\x04\
	\x76\x09\x76\x04\x77\x09\x77\x04\x78\x09\x78\x04\x79\x09\x79\x04\x7a\x09\
	\x7a\x04\x7b\x09\x7b\x04\x7c\x09\x7c\x04\x7d\x09\x7d\x04\x7e\x09\x7e\x04\
	\x7f\x09\x7f\x04\u{80}\x09\u{80}\x04\u{81}\x09\u{81}\x04\u{82}\x09\u{82}\
	\x04\u{83}\x09\u{83}\x04\u{84}\x09\u{84}\x04\u{85}\x09\u{85}\x04\u{86}\x09\
	\u{86}\x04\u{87}\x09\u{87}\x04\u{88}\x09\u{88}\x04\u{89}\x09\u{89}\x04\u{8a}\
	\x09\u{8a}\x03\x02\x03\x02\x03\x02\x03\x03\x03\x03\x05\x03\u{11a}\x0a\x03\
	\x03\x03\x07\x03\u{11d}\x0a\x03\x0c\x03\x0e\x03\u{120}\x0b\x03\x03\x04\x03\
	\x04\x03\x05\x03\x05\x03\x05\x03\x06\x03\x06\x03\x06\x03\x07\x03\x07\x03\
	\x07\x03\x08\x03\x08\x03\x08\x03\x09\x03\x09\x03\x09\x03\x0a\x03\x0a\x05\
	\x0a\u{135}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{13a}\x0a\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{142}\x0a\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x07\x0a\u{14a}\x0a\x0a\x0c\x0a\
	\x0e\x0a\u{14d}\x0b\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x05\x0a\u{160}\x0a\x0a\x03\x0a\x03\x0a\x05\x0a\u{164}\x0a\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{16a}\x0a\x0a\x03\x0a\x05\
	\x0a\u{16d}\x0a\x0a\x03\x0a\x05\x0a\u{170}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x05\x0a\u{177}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\
	\u{17c}\x0a\x0a\x03\x0a\x05\x0a\u{17f}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{186}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{192}\x0a\x0a\x03\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x07\x0a\u{19b}\x0a\
	\x0a\x0c\x0a\x0e\x0a\u{19e}\x0b\x0a\x03\x0a\x05\x0a\u{1a1}\x0a\x0a\x03\x0a\
	\x05\x0a\u{1a4}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\
	\u{1ab}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x07\x0a\u{1b6}\x0a\x0a\x0c\x0a\x0e\x0a\u{1b9}\x0b\x0a\x03\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{1c0}\x0a\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{1c5}\x0a\x0a\x03\x0a\x05\x0a\u{1c8}\x0a\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{1ce}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{1d9}\x0a\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\
	\u{219}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x05\x0a\u{222}\x0a\x0a\x03\x0a\x03\x0a\x05\x0a\u{226}\x0a\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x03\x0a\x05\x0a\u{22c}\x0a\x0a\x03\x0a\x03\x0a\x05\x0a\u{230}\
	\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{235}\x0a\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x05\x0a\u{23b}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{247}\x0a\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{24f}\x0a\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{255}\x0a\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x05\x0a\u{262}\x0a\x0a\x03\x0a\x06\x0a\u{265}\x0a\x0a\x0d\x0a\x0e\x0a\u{266}\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{277}\x0a\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x07\x0a\u{27c}\x0a\x0a\x0c\x0a\x0e\x0a\u{27f}\x0b\x0a\x03\
	\x0a\x05\x0a\u{282}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{288}\
	\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{297}\x0a\x0a\x03\x0a\
	\x03\x0a\x05\x0a\u{29b}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\
	\u{2a1}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{2a7}\x0a\x0a\x03\
	\x0a\x05\x0a\u{2aa}\x0a\x0a\x03\x0a\x05\x0a\u{2ad}\x0a\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x05\x0a\u{2b3}\x0a\x0a\x03\x0a\x03\x0a\x05\x0a\u{2b7}\x0a\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x07\x0a\u{2bf}\x0a\
	\x0a\x0c\x0a\x0e\x0a\u{2c2}\x0b\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{2ca}\x0a\x0a\x03\x0a\x05\x0a\u{2cd}\x0a\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{2d6}\x0a\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{2db}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x05\x0a\u{2e1}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x05\x0a\u{2e8}\x0a\x0a\x03\x0a\x05\x0a\u{2eb}\x0a\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{2f1}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x03\x0a\x07\x0a\u{2fa}\x0a\x0a\x0c\x0a\x0e\x0a\u{2fd}\x0b\x0a\
	\x05\x0a\u{2ff}\x0a\x0a\x03\x0a\x03\x0a\x05\x0a\u{303}\x0a\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{308}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{30d}\
	\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{314}\x0a\x0a\
	\x03\x0a\x05\x0a\u{317}\x0a\x0a\x03\x0a\x05\x0a\u{31a}\x0a\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{321}\x0a\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x05\x0a\u{326}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{32f}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{337}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\
	\x0a\u{33d}\x0a\x0a\x03\x0a\x05\x0a\u{340}\x0a\x0a\x03\x0a\x05\x0a\u{343}\
	\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{349}\x0a\x0a\x03\x0a\
	\x03\x0a\x05\x0a\u{34d}\x0a\x0a\x03\x0a\x03\x0a\x05\x0a\u{351}\x0a\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{355}\x0a\x0a\x05\x0a\u{357}\x0a\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{35f}\x0a\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{367}\x0a\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x05\x0a\u{36d}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x05\x0a\u{373}\x0a\x0a\x03\x0a\x05\x0a\u{376}\x0a\x0a\x03\x0a\x03\x0a\x05\
	\x0a\u{37a}\x0a\x0a\x03\x0a\x05\x0a\u{37d}\x0a\x0a\x03\x0a\x03\x0a\x05\x0a\
	\u{381}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x07\x0a\
	\u{39b}\x0a\x0a\x0c\x0a\x0e\x0a\u{39e}\x0b\x0a\x05\x0a\u{3a0}\x0a\x0a\x03\
	\x0a\x03\x0a\x05\x0a\u{3a4}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\
	\x0a\u{3aa}\x0a\x0a\x03\x0a\x05\x0a\u{3ad}\x0a\x0a\x03\x0a\x05\x0a\u{3b0}\
	\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{3b6}\x0a\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{3be}\x0a\x0a\x03\x0a\
	\x03\x0a\x03\x0a\x05\x0a\u{3c3}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x05\x0a\u{3c9}\x0a\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x05\x0a\u{3cf}\x0a\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x07\
	\x0a\u{3d9}\x0a\x0a\x0c\x0a\x0e\x0a\u{3dc}\x0b\x0a\x05\x0a\u{3de}\x0a\x0a\
	\x03\x0a\x03\x0a\x03\x0a\x07\x0a\u{3e3}\x0a\x0a\x0c\x0a\x0e\x0a\u{3e6}\x0b\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x03\
	\x0a\x03\x0a\x03\x0a\x03\x0a\x07\x0a\u{3f4}\x0a\x0a\x0c\x0a\x0e\x0a\u{3f7}\
	\x0b\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\x07\x0a\u{3fd}\x0a\x0a\x0c\x0a\
	\x0e\x0a\u{400}\x0b\x0a\x05\x0a\u{402}\x0a\x0a\x03\x0a\x03\x0a\x07\x0a\u{406}\
	\x0a\x0a\x0c\x0a\x0e\x0a\u{409}\x0b\x0a\x03\x0a\x03\x0a\x03\x0a\x03\x0a\
	\x07\x0a\u{40f}\x0a\x0a\x0c\x0a\x0e\x0a\u{412}\x0b\x0a\x03\x0a\x03\x0a\x07\
	\x0a\u{416}\x0a\x0a\x0c\x0a\x0e\x0a\u{419}\x0b\x0a\x05\x0a\u{41b}\x0a\x0a\
	\x03\x0b\x03\x0b\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x05\x0c\
	\u{425}\x0a\x0c\x03\x0c\x03\x0c\x05\x0c\u{429}\x0a\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x05\x0c\u{430}\x0a\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x05\x0c\u{4a4}\x0a\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x05\x0c\u{4ac}\x0a\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x05\x0c\u{4b4}\x0a\x0c\x03\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x05\x0c\u{4bd}\x0a\x0c\x03\x0c\x03\x0c\x03\
	\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x03\x0c\x05\x0c\u{4c7}\x0a\x0c\x03\
	\x0d\x03\x0d\x05\x0d\u{4cb}\x0a\x0d\x03\x0d\x05\x0d\u{4ce}\x0a\x0d\x03\x0d\
	\x03\x0d\x03\x0d\x03\x0d\x05\x0d\u{4d4}\x0a\x0d\x03\x0d\x03\x0d\x03\x0e\
	\x03\x0e\x05\x0e\u{4da}\x0a\x0e\x03\x0e\x03\x0e\x03\x0e\x03\x0e\x03\x0f\
	\x03\x0f\x03\x0f\x03\x0f\x03\x0f\x03\x0f\x05\x0f\u{4e6}\x0a\x0f\x03\x0f\
	\x03\x0f\x03\x0f\x03\x0f\x03\x10\x03\x10\x03\x10\x03\x10\x03\x10\x03\x10\
	\x05\x10\u{4f2}\x0a\x10\x03\x10\x03\x10\x03\x10\x05\x10\u{4f7}\x0a\x10\x03\
	\x11\x03\x11\x03\x11\x03\x12\x03\x12\x03\x12\x03\x13\x05\x13\u{500}\x0a\
	\x13\x03\x13\x03\x13\x03\x13\x03\x14\x03\x14\x03\x14\x05\x14\u{508}\x0a\
	\x14\x03\x14\x03\x14\x03\x14\x03\x14\x03\x14\x05\x14\u{50f}\x0a\x14\x05\
	\x14\u{511}\x0a\x14\x03\x14\x03\x14\x03\x14\x05\x14\u{516}\x0a\x14\x03\x14\
	\x03\x14\x05\x14\u{51a}\x0a\x14\x03\x14\x03\x14\x03\x14\x05\x14\u{51f}\x0a\
	\x14\x03\x14\x03\x14\x03\x14\x05\x14\u{524}\x0a\x14\x03\x14\x03\x14\x03\
	\x14\x05\x14\u{529}\x0a\x14\x03\x14\x05\x14\u{52c}\x0a\x14\x03\x14\x03\x14\
	\x03\x14\x05\x14\u{531}\x0a\x14\x03\x14\x03\x14\x05\x14\u{535}\x0a\x14\x03\
	\x14\x03\x14\x03\x14\x05\x14\u{53a}\x0a\x14\x05\x14\u{53c}\x0a\x14\x03\x15\
	\x03\x15\x05\x15\u{540}\x0a\x15\x03\x16\x03\x16\x03\x16\x03\x16\x03\x16\
	\x07\x16\u{547}\x0a\x16\x0c\x16\x0e\x16\u{54a}\x0b\x16\x03\x16\x03\x16\x03\
	\x17\x03\x17\x03\x17\x05\x17\u{551}\x0a\x17\x03\x18\x03\x18\x03\x19\x03\
	\x19\x03\x19\x03\x19\x03\x19\x05\x19\u{55a}\x0a\x19\x03\x1a\x03\x1a\x03\
	\x1a\x07\x1a\u{55f}\x0a\x1a\x0c\x1a\x0e\x1a\u{562}\x0b\x1a\x03\x1b\x03\x1b\
	\x03\x1b\x03\x1b\x07\x1b\u{568}\x0a\x1b\x0c\x1b\x0e\x1b\u{56b}\x0b\x1b\x03\
	\x1c\x03\x1c\x05\x1c\u{56f}\x0a\x1c\x03\x1c\x05\x1c\u{572}\x0a\x1c\x03\x1c\
	\x03\x1c\x03\x1c\x03\x1c\x03\x1d\x03\x1d\x03\x1d\x03\x1e\x03\x1e\x03\x1e\
	\x03\x1e\x03\x1e\x03\x1e\x03\x1e\x03\x1e\x03\x1e\x03\x1e\x07\x1e\u{585}\
	\x0a\x1e\x0c\x1e\x0e\x1e\u{588}\x0b\x1e\x03\x1f\x03\x1f\x03\x1f\x03\x1f\
	\x07\x1f\u{58e}\x0a\x1f\x0c\x1f\x0e\x1f\u{591}\x0b\x1f\x03\x1f\x03\x1f\x03\
	\x20\x03\x20\x05\x20\u{597}\x0a\x20\x03\x20\x05\x20\u{59a}\x0a\x20\x03\x21\
	\x03\x21\x03\x21\x07\x21\u{59f}\x0a\x21\x0c\x21\x0e\x21\u{5a2}\x0b\x21\x03\
	\x21\x05\x21\u{5a5}\x0a\x21\x03\x22\x03\x22\x03\x22\x03\x22\x05\x22\u{5ab}\
	\x0a\x22\x03\x23\x03\x23\x03\x23\x03\x23\x07\x23\u{5b1}\x0a\x23\x0c\x23\
	\x0e\x23\u{5b4}\x0b\x23\x03\x23\x03\x23\x03\x24\x03\x24\x03\x24\x03\x24\
	\x07\x24\u{5bc}\x0a\x24\x0c\x24\x0e\x24\u{5bf}\x0b\x24\x03\x24\x03\x24\x03\
	\x25\x03\x25\x03\x25\x03\x25\x03\x25\x03\x25\x05\x25\u{5c9}\x0a\x25\x03\
	\x26\x03\x26\x03\x26\x03\x26\x03\x26\x05\x26\u{5d0}\x0a\x26\x03\x27\x03\
	\x27\x03\x27\x03\x27\x05\x27\u{5d6}\x0a\x27\x03\x28\x03\x28\x03\x28\x03\
	\x29\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\x06\x29\u{5e1}\x0a\x29\x0d\
	\x29\x0e\x29\u{5e2}\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\x05\x29\u{5ea}\
	\x0a\x29\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\x05\x29\u{5f1}\x0a\x29\
	\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\x03\x29\
	\x03\x29\x05\x29\u{5fd}\x0a\x29\x03\x29\x03\x29\x03\x29\x03\x29\x07\x29\
	\u{603}\x0a\x29\x0c\x29\x0e\x29\u{606}\x0b\x29\x03\x29\x07\x29\u{609}\x0a\
	\x29\x0c\x29\x0e\x29\u{60c}\x0b\x29\x05\x29\u{60e}\x0a\x29\x03\x2a\x03\x2a\
	\x03\x2a\x03\x2a\x03\x2a\x07\x2a\u{615}\x0a\x2a\x0c\x2a\x0e\x2a\u{618}\x0b\
	\x2a\x05\x2a\u{61a}\x0a\x2a\x03\x2a\x03\x2a\x03\x2a\x03\x2a\x03\x2a\x07\
	\x2a\u{621}\x0a\x2a\x0c\x2a\x0e\x2a\u{624}\x0b\x2a\x05\x2a\u{626}\x0a\x2a\
	\x03\x2a\x03\x2a\x03\x2a\x03\x2a\x03\x2a\x07\x2a\u{62d}\x0a\x2a\x0c\x2a\
	\x0e\x2a\u{630}\x0b\x2a\x05\x2a\u{632}\x0a\x2a\x03\x2a\x03\x2a\x03\x2a\x03\
	\x2a\x03\x2a\x07\x2a\u{639}\x0a\x2a\x0c\x2a\x0e\x2a\u{63c}\x0b\x2a\x05\x2a\
	\u{63e}\x0a\x2a\x03\x2a\x05\x2a\u{641}\x0a\x2a\x03\x2a\x03\x2a\x03\x2a\x05\
	\x2a\u{646}\x0a\x2a\x05\x2a\u{648}\x0a\x2a\x03\x2b\x03\x2b\x03\x2b\x03\x2c\
	\x03\x2c\x03\x2c\x03\x2c\x03\x2c\x03\x2c\x03\x2c\x05\x2c\u{654}\x0a\x2c\
	\x03\x2c\x03\x2c\x03\x2c\x03\x2c\x03\x2c\x05\x2c\u{65b}\x0a\x2c\x03\x2c\
	\x03\x2c\x03\x2c\x03\x2c\x03\x2c\x05\x2c\u{662}\x0a\x2c\x03\x2c\x07\x2c\
	\u{665}\x0a\x2c\x0c\x2c\x0e\x2c\u{668}\x0b\x2c\x03\x2d\x03\x2d\x03\x2d\x03\
	\x2d\x03\x2d\x03\x2d\x03\x2d\x03\x2d\x03\x2d\x05\x2d\u{673}\x0a\x2d\x03\
	\x2e\x03\x2e\x05\x2e\u{677}\x0a\x2e\x03\x2e\x03\x2e\x05\x2e\u{67b}\x0a\x2e\
	\x03\x2f\x03\x2f\x06\x2f\u{67f}\x0a\x2f\x0d\x2f\x0e\x2f\u{680}\x03\x30\x03\
	\x30\x05\x30\u{685}\x0a\x30\x03\x30\x03\x30\x03\x30\x03\x30\x07\x30\u{68b}\
	\x0a\x30\x0c\x30\x0e\x30\u{68e}\x0b\x30\x03\x30\x05\x30\u{691}\x0a\x30\x03\
	\x30\x05\x30\u{694}\x0a\x30\x03\x30\x05\x30\u{697}\x0a\x30\x03\x30\x05\x30\
	\u{69a}\x0a\x30\x03\x30\x03\x30\x05\x30\u{69e}\x0a\x30\x03\x31\x03\x31\x05\
	\x31\u{6a2}\x0a\x31\x03\x31\x05\x31\u{6a5}\x0a\x31\x03\x31\x03\x31\x05\x31\
	\u{6a9}\x0a\x31\x03\x31\x07\x31\u{6ac}\x0a\x31\x0c\x31\x0e\x31\u{6af}\x0b\
	\x31\x03\x31\x05\x31\u{6b2}\x0a\x31\x03\x31\x05\x31\u{6b5}\x0a\x31\x03\x31\
	\x05\x31\u{6b8}\x0a\x31\x03\x31\x05\x31\u{6bb}\x0a\x31\x05\x31\u{6bd}\x0a\
	\x31\x03\x32\x03\x32\x03\x32\x03\x32\x03\x32\x03\x32\x03\x32\x03\x32\x03\
	\x32\x03\x32\x05\x32\u{6c9}\x0a\x32\x03\x32\x05\x32\u{6cc}\x0a\x32\x03\x32\
	\x03\x32\x05\x32\u{6d0}\x0a\x32\x03\x32\x03\x32\x03\x32\x03\x32\x03\x32\
	\x03\x32\x03\x32\x03\x32\x05\x32\u{6da}\x0a\x32\x03\x32\x03\x32\x05\x32\
	\u{6de}\x0a\x32\x05\x32\u{6e0}\x0a\x32\x03\x32\x05\x32\u{6e3}\x0a\x32\x03\
	\x32\x03\x32\x05\x32\u{6e7}\x0a\x32\x03\x33\x03\x33\x07\x33\u{6eb}\x0a\x33\
	\x0c\x33\x0e\x33\u{6ee}\x0b\x33\x03\x33\x05\x33\u{6f1}\x0a\x33\x03\x33\x03\
	\x33\x03\x34\x03\x34\x03\x34\x03\x35\x03\x35\x03\x35\x03\x35\x05\x35\u{6fc}\
	\x0a\x35\x03\x35\x03\x35\x03\x35\x03\x36\x03\x36\x03\x36\x03\x36\x03\x36\
	\x05\x36\u{706}\x0a\x36\x03\x36\x03\x36\x03\x36\x03\x37\x03\x37\x03\x37\
	\x03\x37\x03\x37\x03\x37\x03\x37\x05\x37\u{712}\x0a\x37\x03\x38\x03\x38\
	\x03\x38\x03\x38\x03\x38\x03\x38\x03\x38\x03\x38\x03\x38\x03\x38\x03\x38\
	\x07\x38\u{71f}\x0a\x38\x0c\x38\x0e\x38\u{722}\x0b\x38\x03\x38\x03\x38\x05\
	\x38\u{726}\x0a\x38\x03\x39\x03\x39\x03\x39\x07\x39\u{72b}\x0a\x39\x0c\x39\
	\x0e\x39\u{72e}\x0b\x39\x03\x3a\x03\x3a\x03\x3a\x03\x3a\x03\x3b\x03\x3b\
	\x03\x3b\x03\x3c\x03\x3c\x03\x3c\x03\x3d\x03\x3d\x03\x3d\x05\x3d\u{73d}\
	\x0a\x3d\x03\x3d\x07\x3d\u{740}\x0a\x3d\x0c\x3d\x0e\x3d\u{743}\x0b\x3d\x03\
	\x3d\x03\x3d\x03\x3e\x03\x3e\x03\x3e\x03\x3e\x03\x3e\x03\x3e\x07\x3e\u{74d}\
	\x0a\x3e\x0c\x3e\x0e\x3e\u{750}\x0b\x3e\x03\x3e\x03\x3e\x05\x3e\u{754}\x0a\
	\x3e\x03\x3f\x03\x3f\x03\x3f\x03\x3f\x07\x3f\u{75a}\x0a\x3f\x0c\x3f\x0e\
	\x3f\u{75d}\x0b\x3f\x03\x3f\x07\x3f\u{760}\x0a\x3f\x0c\x3f\x0e\x3f\u{763}\
	\x0b\x3f\x03\x3f\x05\x3f\u{766}\x0a\x3f\x03\x40\x03\x40\x03\x40\x03\x40\
	\x03\x40\x07\x40\u{76d}\x0a\x40\x0c\x40\x0e\x40\u{770}\x0b\x40\x03\x40\x03\
	\x40\x03\x40\x03\x40\x03\x40\x03\x40\x03\x40\x03\x40\x03\x40\x03\x40\x07\
	\x40\u{77c}\x0a\x40\x0c\x40\x0e\x40\u{77f}\x0b\x40\x03\x40\x03\x40\x05\x40\
	\u{783}\x0a\x40\x03\x40\x03\x40\x03\x40\x03\x40\x03\x40\x03\x40\x03\x40\
	\x03\x40\x07\x40\u{78d}\x0a\x40\x0c\x40\x0e\x40\u{790}\x0b\x40\x03\x40\x03\
	\x40\x05\x40\u{794}\x0a\x40\x03\x41\x03\x41\x03\x41\x03\x41\x07\x41\u{79a}\
	\x0a\x41\x0c\x41\x0e\x41\u{79d}\x0b\x41\x05\x41\u{79f}\x0a\x41\x03\x41\x03\
	\x41\x05\x41\u{7a3}\x0a\x41\x03\x42\x03\x42\x03\x42\x03\x42\x03\x42\x03\
	\x42\x03\x42\x03\x42\x03\x42\x03\x42\x07\x42\u{7af}\x0a\x42\x0c\x42\x0e\
	\x42\u{7b2}\x0b\x42\x03\x42\x03\x42\x03\x42\x03\x43\x03\x43\x03\x43\x03\
	\x43\x03\x43\x07\x43\u{7bc}\x0a\x43\x0c\x43\x0e\x43\u{7bf}\x0b\x43\x03\x43\
	\x03\x43\x05\x43\u{7c3}\x0a\x43\x03\x44\x03\x44\x05\x44\u{7c7}\x0a\x44\x03\
	\x44\x05\x44\u{7ca}\x0a\x44\x03\x45\x03\x45\x03\x45\x05\x45\u{7cf}\x0a\x45\
	\x03\x45\x03\x45\x03\x45\x03\x45\x03\x45\x07\x45\u{7d6}\x0a\x45\x0c\x45\
	\x0e\x45\u{7d9}\x0b\x45\x05\x45\u{7db}\x0a\x45\x03\x45\x03\x45\x03\x45\x05\
	\x45\u{7e0}\x0a\x45\x03\x45\x03\x45\x03\x45\x07\x45\u{7e5}\x0a\x45\x0c\x45\
	\x0e\x45\u{7e8}\x0b\x45\x05\x45\u{7ea}\x0a\x45\x03\x46\x03\x46\x03\x47\x03\
	\x47\x07\x47\u{7f0}\x0a\x47\x0c\x47\x0e\x47\u{7f3}\x0b\x47\x03\x48\x03\x48\
	\x03\x48\x03\x48\x05\x48\u{7f9}\x0a\x48\x03\x48\x03\x48\x03\x48\x03\x48\
	\x03\x48\x05\x48\u{800}\x0a\x48\x03\x49\x05\x49\u{803}\x0a\x49\x03\x49\x03\
	\x49\x03\x49\x05\x49\u{808}\x0a\x49\x03\x49\x05\x49\u{80b}\x0a\x49\x03\x49\
	\x03\x49\x03\x49\x05\x49\u{810}\x0a\x49\x03\x49\x03\x49\x05\x49\u{814}\x0a\
	\x49\x03\x49\x05\x49\u{817}\x0a\x49\x03\x49\x05\x49\u{81a}\x0a\x49\x03\x4a\
	\x03\x4a\x03\x4a\x03\x4a\x05\x4a\u{820}\x0a\x4a\x03\x4b\x03\x4b\x03\x4b\
	\x05\x4b\u{825}\x0a\x4b\x03\x4b\x03\x4b\x03\x4c\x05\x4c\u{82a}\x0a\x4c\x03\
	\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x03\
	\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x03\x4c\x05\x4c\u{83c}\x0a\
	\x4c\x05\x4c\u{83e}\x0a\x4c\x03\x4c\x05\x4c\u{841}\x0a\x4c\x03\x4d\x03\x4d\
	\x03\x4d\x03\x4d\x03\x4e\x03\x4e\x03\x4e\x07\x4e\u{84a}\x0a\x4e\x0c\x4e\
	\x0e\x4e\u{84d}\x0b\x4e\x03\x4f\x03\x4f\x03\x4f\x03\x4f\x07\x4f\u{853}\x0a\
	\x4f\x0c\x4f\x0e\x4f\u{856}\x0b\x4f\x03\x4f\x03\x4f\x03\x50\x03\x50\x05\
	\x50\u{85c}\x0a\x50\x03\x51\x03\x51\x03\x51\x03\x51\x07\x51\u{862}\x0a\x51\
	\x0c\x51\x0e\x51\u{865}\x0b\x51\x03\x51\x03\x51\x03\x52\x03\x52\x05\x52\
	\u{86b}\x0a\x52\x03\x53\x03\x53\x05\x53\u{86f}\x0a\x53\x03\x53\x03\x53\x03\
	\x53\x03\x53\x03\x53\x03\x53\x05\x53\u{877}\x0a\x53\x03\x53\x03\x53\x03\
	\x53\x03\x53\x03\x53\x03\x53\x05\x53\u{87f}\x0a\x53\x03\x53\x03\x53\x03\
	\x53\x03\x53\x05\x53\u{885}\x0a\x53\x03\x54\x03\x54\x03\x54\x03\x54\x07\
	\x54\u{88b}\x0a\x54\x0c\x54\x0e\x54\u{88e}\x0b\x54\x03\x54\x03\x54\x03\x55\
	\x03\x55\x03\x55\x03\x55\x03\x55\x07\x55\u{897}\x0a\x55\x0c\x55\x0e\x55\
	\u{89a}\x0b\x55\x05\x55\u{89c}\x0a\x55\x03\x55\x03\x55\x03\x55\x03\x56\x05\
	\x56\u{8a2}\x0a\x56\x03\x56\x03\x56\x05\x56\u{8a6}\x0a\x56\x05\x56\u{8a8}\
	\x0a\x56\x03\x57\x03\x57\x03\x57\x03\x57\x03\x57\x03\x57\x03\x57\x05\x57\
	\u{8b1}\x0a\x57\x03\x57\x03\x57\x03\x57\x03\x57\x03\x57\x03\x57\x03\x57\
	\x03\x57\x03\x57\x03\x57\x05\x57\u{8bd}\x0a\x57\x05\x57\u{8bf}\x0a\x57\x03\
	\x57\x03\x57\x03\x57\x03\x57\x03\x57\x05\x57\u{8c6}\x0a\x57\x03\x57\x03\
	\x57\x03\x57\x03\x57\x03\x57\x05\x57\u{8cd}\x0a\x57\x03\x57\x03\x57\x03\
	\x57\x03\x57\x05\x57\u{8d3}\x0a\x57\x03\x57\x03\x57\x03\x57\x03\x57\x05\
	\x57\u{8d9}\x0a\x57\x05\x57\u{8db}\x0a\x57\x03\x58\x03\x58\x03\x58\x07\x58\
	\u{8e0}\x0a\x58\x0c\x58\x0e\x58\u{8e3}\x0b\x58\x03\x59\x03\x59\x03\x59\x07\
	\x59\u{8e8}\x0a\x59\x0c\x59\x0e\x59\u{8eb}\x0b\x59\x03\x5a\x03\x5a\x03\x5a\
	\x05\x5a\u{8f0}\x0a\x5a\x03\x5a\x03\x5a\x03\x5b\x03\x5b\x05\x5b\u{8f6}\x0a\
	\x5b\x03\x5b\x03\x5b\x05\x5b\u{8fa}\x0a\x5b\x05\x5b\u{8fc}\x0a\x5b\x03\x5c\
	\x03\x5c\x03\x5c\x07\x5c\u{901}\x0a\x5c\x0c\x5c\x0e\x5c\u{904}\x0b\x5c\x03\
	\x5d\x03\x5d\x03\x5d\x03\x5d\x07\x5d\u{90a}\x0a\x5d\x0c\x5d\x0e\x5d\u{90d}\
	\x0b\x5d\x03\x5d\x03\x5d\x03\x5e\x03\x5e\x03\x5e\x03\x5e\x03\x5e\x03\x5e\
	\x07\x5e\u{917}\x0a\x5e\x0c\x5e\x0e\x5e\u{91a}\x0b\x5e\x03\x5e\x03\x5e\x05\
	\x5e\u{91e}\x0a\x5e\x03\x5f\x03\x5f\x05\x5f\u{922}\x0a\x5f\x03\x60\x03\x60\
	\x03\x61\x03\x61\x03\x61\x03\x61\x03\x61\x03\x61\x03\x61\x03\x61\x03\x61\
	\x03\x61\x05\x61\u{930}\x0a\x61\x05\x61\u{932}\x0a\x61\x03\x61\x03\x61\x03\
	\x61\x03\x61\x03\x61\x03\x61\x07\x61\u{93a}\x0a\x61\x0c\x61\x0e\x61\u{93d}\
	\x0b\x61\x03\x62\x05\x62\u{940}\x0a\x62\x03\x62\x03\x62\x03\x62\x03\x62\
	\x03\x62\x03\x62\x05\x62\u{948}\x0a\x62\x03\x62\x03\x62\x03\x62\x03\x62\
	\x03\x62\x07\x62\u{94f}\x0a\x62\x0c\x62\x0e\x62\u{952}\x0b\x62\x03\x62\x03\
	\x62\x03\x62\x05\x62\u{957}\x0a\x62\x03\x62\x03\x62\x03\x62\x03\x62\x03\
	\x62\x03\x62\x05\x62\u{95f}\x0a\x62\x03\x62\x03\x62\x03\x62\x05\x62\u{964}\
	\x0a\x62\x03\x62\x03\x62\x03\x62\x03\x62\x03\x62\x03\x62\x03\x62\x03\x62\
	\x07\x62\u{96e}\x0a\x62\x0c\x62\x0e\x62\u{971}\x0b\x62\x03\x62\x03\x62\x05\
	\x62\u{975}\x0a\x62\x03\x62\x05\x62\u{978}\x0a\x62\x03\x62\x03\x62\x03\x62\
	\x03\x62\x05\x62\u{97e}\x0a\x62\x03\x62\x03\x62\x05\x62\u{982}\x0a\x62\x03\
	\x62\x03\x62\x03\x62\x05\x62\u{987}\x0a\x62\x03\x62\x03\x62\x03\x62\x05\
	\x62\u{98c}\x0a\x62\x03\x62\x03\x62\x03\x62\x05\x62\u{991}\x0a\x62\x03\x63\
	\x03\x63\x03\x63\x03\x63\x05\x63\u{997}\x0a\x63\x03\x63\x03\x63\x03\x63\
	\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\
	\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\x03\x63\x07\x63\u{9ac}\
	\x0a\x63\x0c\x63\x0e\x63\u{9af}\x0b\x63\x03\x64\x03\x64\x03\x64\x03\x64\
	\x06\x64\u{9b5}\x0a\x64\x0d\x64\x0e\x64\u{9b6}\x03\x64\x03\x64\x05\x64\u{9bb}\
	\x0a\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x06\x64\u{9c2}\x0a\x64\
	\x0d\x64\x0e\x64\u{9c3}\x03\x64\x03\x64\x05\x64\u{9c8}\x0a\x64\x03\x64\x03\
	\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\
	\x64\x03\x64\x03\x64\x03\x64\x07\x64\u{9d8}\x0a\x64\x0c\x64\x0e\x64\u{9db}\
	\x0b\x64\x05\x64\u{9dd}\x0a\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x05\x64\u{9e5}\x0a\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x05\x64\u{9ee}\x0a\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x06\x64\u{a03}\x0a\x64\
	\x0d\x64\x0e\x64\u{a04}\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x03\x64\x05\x64\u{a10}\x0a\x64\x03\x64\x03\x64\x03\x64\
	\x07\x64\u{a15}\x0a\x64\x0c\x64\x0e\x64\u{a18}\x0b\x64\x05\x64\u{a1a}\x0a\
	\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x05\x64\u{a23}\
	\x0a\x64\x03\x64\x03\x64\x05\x64\u{a27}\x0a\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x06\x64\u{a31}\x0a\x64\x0d\x64\
	\x0e\x64\u{a32}\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x05\x64\u{a4c}\
	\x0a\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x05\x64\u{a53}\x0a\x64\
	\x03\x64\x05\x64\u{a56}\x0a\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\
	\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x05\x64\
	\u{a65}\x0a\x64\x03\x64\x03\x64\x05\x64\u{a69}\x0a\x64\x03\x64\x03\x64\x03\
	\x64\x03\x64\x03\x64\x03\x64\x03\x64\x03\x64\x07\x64\u{a73}\x0a\x64\x0c\
	\x64\x0e\x64\u{a76}\x0b\x64\x03\x65\x03\x65\x03\x65\x03\x65\x03\x65\x03\
	\x65\x03\x65\x03\x65\x06\x65\u{a80}\x0a\x65\x0d\x65\x0e\x65\u{a81}\x05\x65\
	\u{a84}\x0a\x65\x03\x66\x03\x66\x03\x67\x03\x67\x03\x68\x03\x68\x03\x69\
	\x03\x69\x03\x6a\x03\x6a\x03\x6a\x05\x6a\u{a91}\x0a\x6a\x03\x6b\x03\x6b\
	\x05\x6b\u{a95}\x0a\x6b\x03\x6c\x03\x6c\x03\x6c\x06\x6c\u{a9a}\x0a\x6c\x0d\
	\x6c\x0e\x6c\u{a9b}\x03\x6d\x03\x6d\x03\x6d\x05\x6d\u{aa1}\x0a\x6d\x03\x6e\
	\x03\x6e\x03\x6e\x03\x6e\x03\x6e\x03\x6f\x05\x6f\u{aa9}\x0a\x6f\x03\x6f\
	\x03\x6f\x05\x6f\u{aad}\x0a\x6f\x03\x70\x03\x70\x03\x70\x05\x70\u{ab2}\x0a\
	\x70\x03\x71\x03\x71\x03\x71\x03\x71\x03\x71\x03\x71\x03\x71\x03\x71\x03\
	\x71\x03\x71\x03\x71\x03\x71\x03\x71\x03\x71\x03\x71\x05\x71\u{ac3}\x0a\
	\x71\x03\x71\x03\x71\x05\x71\u{ac7}\x0a\x71\x03\x71\x03\x71\x03\x71\x03\
	\x71\x03\x71\x07\x71\u{ace}\x0a\x71\x0c\x71\x0e\x71\u{ad1}\x0b\x71\x03\x71\
	\x05\x71\u{ad4}\x0a\x71\x05\x71\u{ad6}\x0a\x71\x03\x72\x03\x72\x03\x72\x07\
	\x72\u{adb}\x0a\x72\x0c\x72\x0e\x72\u{ade}\x0b\x72\x03\x73\x03\x73\x03\x73\
	\x03\x73\x05\x73\u{ae4}\x0a\x73\x03\x73\x05\x73\u{ae7}\x0a\x73\x03\x73\x05\
	\x73\u{aea}\x0a\x73\x03\x74\x03\x74\x03\x74\x07\x74\u{aef}\x0a\x74\x0c\x74\
	\x0e\x74\u{af2}\x0b\x74\x03\x75\x03\x75\x03\x75\x03\x75\x05\x75\u{af8}\x0a\
	\x75\x03\x75\x05\x75\u{afb}\x0a\x75\x03\x76\x03\x76\x03\x76\x07\x76\u{b00}\
	\x0a\x76\x0c\x76\x0e\x76\u{b03}\x0b\x76\x03\x77\x03\x77\x03\x77\x03\x77\
	\x03\x77\x05\x77\u{b0a}\x0a\x77\x03\x77\x05\x77\u{b0d}\x0a\x77\x03\x78\x03\
	\x78\x03\x78\x03\x78\x03\x78\x03\x79\x03\x79\x03\x79\x03\x79\x07\x79\u{b18}\
	\x0a\x79\x0c\x79\x0e\x79\u{b1b}\x0b\x79\x03\x7a\x03\x7a\x03\x7a\x03\x7a\
	\x03\x7b\x03\x7b\x03\x7b\x03\x7b\x03\x7b\x03\x7b\x03\x7b\x03\x7b\x03\x7b\
	\x03\x7b\x03\x7b\x07\x7b\u{b2c}\x0a\x7b\x0c\x7b\x0e\x7b\u{b2f}\x0b\x7b\x03\
	\x7b\x03\x7b\x03\x7b\x03\x7b\x03\x7b\x07\x7b\u{b36}\x0a\x7b\x0c\x7b\x0e\
	\x7b\u{b39}\x0b\x7b\x05\x7b\u{b3b}\x0a\x7b\x03\x7b\x03\x7b\x03\x7b\x03\x7b\
	\x03\x7b\x07\x7b\u{b42}\x0a\x7b\x0c\x7b\x0e\x7b\u{b45}\x0b\x7b\x05\x7b\u{b47}\
	\x0a\x7b\x05\x7b\u{b49}\x0a\x7b\x03\x7b\x05\x7b\u{b4c}\x0a\x7b\x03\x7b\x05\
	\x7b\u{b4f}\x0a\x7b\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\
	\x7c\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\x7c\x03\
	\x7c\x05\x7c\u{b61}\x0a\x7c\x03\x7d\x03\x7d\x03\x7d\x03\x7d\x03\x7d\x03\
	\x7d\x03\x7d\x05\x7d\u{b6a}\x0a\x7d\x03\x7e\x03\x7e\x03\x7e\x07\x7e\u{b6f}\
	\x0a\x7e\x0c\x7e\x0e\x7e\u{b72}\x0b\x7e\x03\x7f\x03\x7f\x03\x7f\x03\x7f\
	\x05\x7f\u{b78}\x0a\x7f\x03\u{80}\x03\u{80}\x03\u{80}\x07\u{80}\u{b7d}\x0a\
	\u{80}\x0c\u{80}\x0e\u{80}\u{b80}\x0b\u{80}\x03\u{81}\x03\u{81}\x03\u{81}\
	\x03\u{82}\x03\u{82}\x06\u{82}\u{b87}\x0a\u{82}\x0d\u{82}\x0e\u{82}\u{b88}\
	\x03\u{82}\x05\u{82}\u{b8c}\x0a\u{82}\x03\u{83}\x03\u{83}\x03\u{83}\x05\
	\u{83}\u{b91}\x0a\u{83}\x03\u{84}\x03\u{84}\x03\u{84}\x03\u{84}\x03\u{84}\
	\x03\u{84}\x05\u{84}\u{b99}\x0a\u{84}\x03\u{85}\x03\u{85}\x03\u{86}\x03\
	\u{86}\x05\u{86}\u{b9f}\x0a\u{86}\x03\u{86}\x03\u{86}\x03\u{86}\x05\u{86}\
	\u{ba4}\x0a\u{86}\x03\u{86}\x03\u{86}\x03\u{86}\x05\u{86}\u{ba9}\x0a\u{86}\
	\x03\u{86}\x03\u{86}\x05\u{86}\u{bad}\x0a\u{86}\x03\u{86}\x03\u{86}\x05\
	\u{86}\u{bb1}\x0a\u{86}\x03\u{86}\x03\u{86}\x05\u{86}\u{bb5}\x0a\u{86}\x03\
	\u{86}\x03\u{86}\x05\u{86}\u{bb9}\x0a\u{86}\x03\u{86}\x03\u{86}\x05\u{86}\
	\u{bbd}\x0a\u{86}\x03\u{86}\x03\u{86}\x05\u{86}\u{bc1}\x0a\u{86}\x03\u{86}\
	\x03\u{86}\x05\u{86}\u{bc5}\x0a\u{86}\x03\u{86}\x05\u{86}\u{bc8}\x0a\u{86}\
	\x03\u{87}\x03\u{87}\x03\u{87}\x03\u{87}\x03\u{87}\x03\u{87}\x03\u{87}\x05\
	\u{87}\u{bd1}\x0a\u{87}\x03\u{88}\x03\u{88}\x03\u{89}\x03\u{89}\x03\u{8a}\
	\x03\u{8a}\x03\u{8a}\x0a\u{39c}\u{3da}\u{3e4}\u{3f5}\u{3fe}\u{407}\u{410}\
	\u{417}\x06\x56\u{c0}\u{c4}\u{c6}\u{8b}\x02\x04\x06\x08\x0a\x0c\x0e\x10\
	\x12\x14\x16\x18\x1a\x1c\x1e\x20\x22\x24\x26\x28\x2a\x2c\x2e\x30\x32\x34\
	\x36\x38\x3a\x3c\x3e\x40\x42\x44\x46\x48\x4a\x4c\x4e\x50\x52\x54\x56\x58\
	\x5a\x5c\x5e\x60\x62\x64\x66\x68\x6a\x6c\x6e\x70\x72\x74\x76\x78\x7a\x7c\
	\x7e\u{80}\u{82}\u{84}\u{86}\u{88}\u{8a}\u{8c}\u{8e}\u{90}\u{92}\u{94}\u{96}\
	\u{98}\u{9a}\u{9c}\u{9e}\u{a0}\u{a2}\u{a4}\u{a6}\u{a8}\u{aa}\u{ac}\u{ae}\
	\u{b0}\u{b2}\u{b4}\u{b6}\u{b8}\u{ba}\u{bc}\u{be}\u{c0}\u{c2}\u{c4}\u{c6}\
	\u{c8}\u{ca}\u{cc}\u{ce}\u{d0}\u{d2}\u{d4}\u{d6}\u{d8}\u{da}\u{dc}\u{de}\
	\u{e0}\u{e2}\u{e4}\u{e6}\u{e8}\u{ea}\u{ec}\u{ee}\u{f0}\u{f2}\u{f4}\u{f6}\
	\u{f8}\u{fa}\u{fc}\u{fe}\u{100}\u{102}\u{104}\u{106}\u{108}\u{10a}\u{10c}\
	\u{10e}\u{110}\u{112}\x02\x2d\x04\x02\x41\x41\u{b2}\u{b2}\x04\x02\x21\x21\
	\u{c0}\u{c0}\x04\x02\x40\x40\u{94}\u{94}\x04\x02\x65\x65\x71\x71\x03\x02\
	\x2c\x2d\x04\x02\u{e0}\u{e0}\u{ff}\u{ff}\x04\x02\x10\x10\x24\x24\x07\x02\
	\x29\x29\x35\x35\x57\x57\x64\x64\u{8d}\u{8d}\x03\x02\x45\x46\x04\x02\x57\
	\x57\x64\x64\x04\x02\u{98}\u{98}\u{119}\u{119}\x04\x02\x0d\x0d\u{87}\u{87}\
	\x04\x02\u{89}\u{89}\u{119}\u{119}\x05\x02\x3f\x3f\u{93}\u{93}\u{ca}\u{ca}\
	\x06\x02\x52\x52\x78\x78\u{d2}\u{d2}\u{f5}\u{f5}\x05\x02\x52\x52\u{d2}\u{d2}\
	\u{f5}\u{f5}\x04\x02\x18\x18\x45\x45\x04\x02\x5f\x5f\x7f\x7f\x04\x02\x0f\
	\x0f\x4a\x4a\x04\x02\u{11d}\u{11d}\u{11f}\u{11f}\x05\x02\x0f\x0f\x14\x14\
	\u{d6}\u{d6}\x05\x02\x5a\x5a\u{ef}\u{ef}\u{f7}\u{f7}\x04\x02\u{10e}\u{10f}\
	\u{113}\u{113}\x04\x02\x4c\x4c\u{110}\u{112}\x04\x02\u{10e}\u{10f}\u{116}\
	\u{116}\x04\x02\x3a\x3a\x3c\x3c\x03\x02\u{de}\u{df}\x04\x02\x05\x05\x65\
	\x65\x04\x02\x05\x05\x61\x61\x05\x02\x1c\x1c\u{82}\u{82}\u{ea}\u{ea}\x03\
	\x02\u{106}\u{10d}\x04\x02\x4c\x4c\u{10e}\u{117}\x06\x02\x12\x12\x71\x71\
	\u{97}\u{97}\u{9f}\u{9f}\x04\x02\x5a\x5a\u{ef}\u{ef}\x03\x02\u{10e}\u{10f}\
	\x04\x02\x4b\x4b\u{a8}\u{a8}\x04\x02\u{a0}\u{a0}\u{d7}\u{d7}\x04\x02\x60\
	\x60\u{af}\u{af}\x03\x02\u{11e}\u{11f}\x04\x02\x4d\x4d\u{d1}\u{d1}\x32\x02\
	\x0d\x0e\x10\x11\x13\x13\x15\x16\x18\x19\x1b\x1b\x1d\x21\x24\x24\x26\x29\
	\x2b\x2b\x2d\x33\x35\x35\x38\x39\x3e\x49\x4b\x4d\x51\x51\x53\x59\x5c\x5c\
	\x5e\x60\x63\x64\x67\x69\x6c\x6c\x6e\x70\x72\x73\x75\x77\x79\x79\x7c\x7c\
	\x7e\u{81}\u{84}\u{94}\u{96}\u{96}\u{99}\u{9a}\u{9d}\u{9e}\u{a1}\u{a1}\u{a3}\
	\u{a4}\u{a6}\u{af}\u{b1}\u{b9}\u{bb}\u{c1}\u{c3}\u{ca}\u{cc}\u{cf}\u{d1}\
	\u{d5}\u{d7}\u{df}\u{e1}\u{e5}\u{e9}\u{e9}\u{eb}\u{f4}\u{f8}\u{fb}\u{fe}\
	\u{100}\u{103}\u{103}\u{105}\u{105}\x11\x02\x13\x13\x37\x37\x52\x52\x66\
	\x66\x74\x74\x78\x78\x7d\x7d\u{83}\u{83}\u{95}\u{95}\u{9b}\u{9b}\u{c2}\u{c2}\
	\u{cc}\u{cc}\u{d2}\u{d2}\u{f5}\u{f5}\u{fd}\u{fd}\x12\x02\x0d\x12\x14\x36\
	\x38\x51\x53\x65\x67\x73\x75\x77\x79\x7c\x7e\u{82}\u{84}\u{94}\u{96}\u{9a}\
	\u{9c}\u{c1}\u{c3}\u{cb}\u{cd}\u{d1}\u{d3}\u{f4}\u{f6}\u{fc}\u{fe}\u{105}\
	\x02\u{db3}\x02\u{114}\x03\x02\x02\x02\x04\u{11e}\x03\x02\x02\x02\x06\u{121}\
	\x03\x02\x02\x02\x08\u{123}\x03\x02\x02\x02\x0a\u{126}\x03\x02\x02\x02\x0c\
	\u{129}\x03\x02\x02\x02\x0e\u{12c}\x03\x02\x02\x02\x10\u{12f}\x03\x02\x02\
	\x02\x12\u{41a}\x03\x02\x02\x02\x14\u{41c}\x03\x02\x02\x02\x16\u{4c6}\x03\
	\x02\x02\x02\x18\u{4c8}\x03\x02\x02\x02\x1a\u{4d9}\x03\x02\x02\x02\x1c\u{4df}\
	\x03\x02\x02\x02\x1e\u{4eb}\x03\x02\x02\x02\x20\u{4f8}\x03\x02\x02\x02\x22\
	\u{4fb}\x03\x02\x02\x02\x24\u{4ff}\x03\x02\x02\x02\x26\u{53b}\x03\x02\x02\
	\x02\x28\u{53d}\x03\x02\x02\x02\x2a\u{541}\x03\x02\x02\x02\x2c\u{54d}\x03\
	\x02\x02\x02\x2e\u{552}\x03\x02\x02\x02\x30\u{559}\x03\x02\x02\x02\x32\u{55b}\
	\x03\x02\x02\x02\x34\u{563}\x03\x02\x02\x02\x36\u{56c}\x03\x02\x02\x02\x38\
	\u{577}\x03\x02\x02\x02\x3a\u{586}\x03\x02\x02\x02\x3c\u{589}\x03\x02\x02\
	\x02\x3e\u{594}\x03\x02\x02\x02\x40\u{5a4}\x03\x02\x02\x02\x42\u{5aa}\x03\
	\x02\x02\x02\x44\u{5ac}\x03\x02\x02\x02\x46\u{5b7}\x03\x02\x02\x02\x48\u{5c8}\
	\x03\x02\x02\x02\x4a\u{5cf}\x03\x02\x02\x02\x4c\u{5d1}\x03\x02\x02\x02\x4e\
	\u{5d7}\x03\x02\x02\x02\x50\u{60d}\x03\x02\x02\x02\x52\u{619}\x03\x02\x02\
	\x02\x54\u{649}\x03\x02\x02\x02\x56\u{64c}\x03\x02\x02\x02\x58\u{672}\x03\
	\x02\x02\x02\x5a\u{674}\x03\x02\x02\x02\x5c\u{67c}\x03\x02\x02\x02\x5e\u{69d}\
	\x03\x02\x02\x02\x60\u{6bc}\x03\x02\x02\x02\x62\u{6c8}\x03\x02\x02\x02\x64\
	\u{6e8}\x03\x02\x02\x02\x66\u{6f4}\x03\x02\x02\x02\x68\u{6f7}\x03\x02\x02\
	\x02\x6a\u{700}\x03\x02\x02\x02\x6c\u{711}\x03\x02\x02\x02\x6e\u{725}\x03\
	\x02\x02\x02\x70\u{727}\x03\x02\x02\x02\x72\u{72f}\x03\x02\x02\x02\x74\u{733}\
	\x03\x02\x02\x02\x76\u{736}\x03\x02\x02\x02\x78\u{739}\x03\x02\x02\x02\x7a\
	\u{753}\x03\x02\x02\x02\x7c\u{755}\x03\x02\x02\x02\x7e\u{793}\x03\x02\x02\
	\x02\u{80}\u{7a2}\x03\x02\x02\x02\u{82}\u{7a4}\x03\x02\x02\x02\u{84}\u{7c2}\
	\x03\x02\x02\x02\u{86}\u{7c4}\x03\x02\x02\x02\u{88}\u{7cb}\x03\x02\x02\x02\
	\u{8a}\u{7eb}\x03\x02\x02\x02\u{8c}\u{7ed}\x03\x02\x02\x02\u{8e}\u{7ff}\
	\x03\x02\x02\x02\u{90}\u{819}\x03\x02\x02\x02\u{92}\u{81f}\x03\x02\x02\x02\
	\u{94}\u{821}\x03\x02\x02\x02\u{96}\u{840}\x03\x02\x02\x02\u{98}\u{842}\
	\x03\x02\x02\x02\u{9a}\u{846}\x03\x02\x02\x02\u{9c}\u{84e}\x03\x02\x02\x02\
	\u{9e}\u{859}\x03\x02\x02\x02\u{a0}\u{85d}\x03\x02\x02\x02\u{a2}\u{868}\
	\x03\x02\x02\x02\u{a4}\u{884}\x03\x02\x02\x02\u{a6}\u{886}\x03\x02\x02\x02\
	\u{a8}\u{891}\x03\x02\x02\x02\u{aa}\u{8a7}\x03\x02\x02\x02\u{ac}\u{8da}\
	\x03\x02\x02\x02\u{ae}\u{8dc}\x03\x02\x02\x02\u{b0}\u{8e4}\x03\x02\x02\x02\
	\u{b2}\u{8ef}\x03\x02\x02\x02\u{b4}\u{8f3}\x03\x02\x02\x02\u{b6}\u{8fd}\
	\x03\x02\x02\x02\u{b8}\u{905}\x03\x02\x02\x02\u{ba}\u{91d}\x03\x02\x02\x02\
	\u{bc}\u{921}\x03\x02\x02\x02\u{be}\u{923}\x03\x02\x02\x02\u{c0}\u{931}\
	\x03\x02\x02\x02\u{c2}\u{990}\x03\x02\x02\x02\u{c4}\u{996}\x03\x02\x02\x02\
	\u{c6}\u{a68}\x03\x02\x02\x02\u{c8}\u{a83}\x03\x02\x02\x02\u{ca}\u{a85}\
	\x03\x02\x02\x02\u{cc}\u{a87}\x03\x02\x02\x02\u{ce}\u{a89}\x03\x02\x02\x02\
	\u{d0}\u{a8b}\x03\x02\x02\x02\u{d2}\u{a8d}\x03\x02\x02\x02\u{d4}\u{a92}\
	\x03\x02\x02\x02\u{d6}\u{a99}\x03\x02\x02\x02\u{d8}\u{a9d}\x03\x02\x02\x02\
	\u{da}\u{aa2}\x03\x02\x02\x02\u{dc}\u{aac}\x03\x02\x02\x02\u{de}\u{ab1}\
	\x03\x02\x02\x02\u{e0}\u{ad5}\x03\x02\x02\x02\u{e2}\u{ad7}\x03\x02\x02\x02\
	\u{e4}\u{adf}\x03\x02\x02\x02\u{e6}\u{aeb}\x03\x02\x02\x02\u{e8}\u{af3}\
	\x03\x02\x02\x02\u{ea}\u{afc}\x03\x02\x02\x02\u{ec}\u{b04}\x03\x02\x02\x02\
	\u{ee}\u{b0e}\x03\x02\x02\x02\u{f0}\u{b13}\x03\x02\x02\x02\u{f2}\u{b1c}\
	\x03\x02\x02\x02\u{f4}\u{b4e}\x03\x02\x02\x02\u{f6}\u{b60}\x03\x02\x02\x02\
	\u{f8}\u{b69}\x03\x02\x02\x02\u{fa}\u{b6b}\x03\x02\x02\x02\u{fc}\u{b77}\
	\x03\x02\x02\x02\u{fe}\u{b79}\x03\x02\x02\x02\u{100}\u{b81}\x03\x02\x02\
	\x02\u{102}\u{b8b}\x03\x02\x02\x02\u{104}\u{b90}\x03\x02\x02\x02\u{106}\
	\u{b98}\x03\x02\x02\x02\u{108}\u{b9a}\x03\x02\x02\x02\u{10a}\u{bc7}\x03\
	\x02\x02\x02\u{10c}\u{bd0}\x03\x02\x02\x02\u{10e}\u{bd2}\x03\x02\x02\x02\
	\u{110}\u{bd4}\x03\x02\x02\x02\u{112}\u{bd6}\x03\x02\x02\x02\u{114}\u{115}\
	\x05\x04\x03\x02\u{115}\u{116}\x07\x02\x02\x03\u{116}\x03\x03\x02\x02\x02\
	\u{117}\u{119}\x05\x12\x0a\x02\u{118}\u{11a}\x07\u{118}\x02\x02\u{119}\u{118}\
	\x03\x02\x02\x02\u{119}\u{11a}\x03\x02\x02\x02\u{11a}\u{11d}\x03\x02\x02\
	\x02\u{11b}\u{11d}\x05\x06\x04\x02\u{11c}\u{117}\x03\x02\x02\x02\u{11c}\
	\u{11b}\x03\x02\x02\x02\u{11d}\u{120}\x03\x02\x02\x02\u{11e}\u{11c}\x03\
	\x02\x02\x02\u{11e}\u{11f}\x03\x02\x02\x02\u{11f}\x05\x03\x02\x02\x02\u{120}\
	\u{11e}\x03\x02\x02\x02\u{121}\u{122}\x07\u{118}\x02\x02\u{122}\x07\x03\
	\x02\x02\x02\u{123}\u{124}\x05\u{b4}\x5b\x02\u{124}\u{125}\x07\x02\x02\x03\
	\u{125}\x09\x03\x02\x02\x02\u{126}\u{127}\x05\u{b2}\x5a\x02\u{127}\u{128}\
	\x07\x02\x02\x03\u{128}\x0b\x03\x02\x02\x02\u{129}\u{12a}\x05\u{b0}\x59\
	\x02\u{12a}\u{12b}\x07\x02\x02\x03\u{12b}\x0d\x03\x02\x02\x02\u{12c}\u{12d}\
	\x05\u{e0}\x71\x02\u{12d}\u{12e}\x07\x02\x02\x03\u{12e}\x0f\x03\x02\x02\
	\x02\u{12f}\u{130}\x05\u{e6}\x74\x02\u{130}\u{131}\x07\x02\x02\x03\u{131}\
	\x11\x03\x02\x02\x02\u{132}\u{41b}\x05\x24\x13\x02\u{133}\u{135}\x05\x34\
	\x1b\x02\u{134}\u{133}\x03\x02\x02\x02\u{134}\u{135}\x03\x02\x02\x02\u{135}\
	\u{136}\x03\x02\x02\x02\u{136}\u{41b}\x05\x50\x29\x02\u{137}\u{139}\x07\
	\u{fb}\x02\x02\u{138}\u{13a}\x07\u{93}\x02\x02\u{139}\u{138}\x03\x02\x02\
	\x02\u{139}\u{13a}\x03\x02\x02\x02\u{13a}\u{13b}\x03\x02\x02\x02\u{13b}\
	\u{41b}\x05\u{b0}\x59\x02\u{13c}\u{13d}\x07\x36\x02\x02\u{13d}\u{141}\x05\
	\x2e\x18\x02\u{13e}\u{13f}\x07\x6e\x02\x02\u{13f}\u{140}\x07\u{97}\x02\x02\
	\u{140}\u{142}\x07\x54\x02\x02\u{141}\u{13e}\x03\x02\x02\x02\u{141}\u{142}\
	\x03\x02\x02\x02\u{142}\u{143}\x03\x02\x02\x02\u{143}\u{14b}\x05\u{b0}\x59\
	\x02\u{144}\u{14a}\x05\x22\x12\x02\u{145}\u{14a}\x05\x20\x11\x02\u{146}\
	\u{147}\x07\u{104}\x02\x02\u{147}\u{148}\x09\x02\x02\x02\u{148}\u{14a}\x05\
	\x3c\x1f\x02\u{149}\u{144}\x03\x02\x02\x02\u{149}\u{145}\x03\x02\x02\x02\
	\u{149}\u{146}\x03\x02\x02\x02\u{14a}\u{14d}\x03\x02\x02\x02\u{14b}\u{149}\
	\x03\x02\x02\x02\u{14b}\u{14c}\x03\x02\x02\x02\u{14c}\u{41b}\x03\x02\x02\
	\x02\u{14d}\u{14b}\x03\x02\x02\x02\u{14e}\u{14f}\x07\x10\x02\x02\u{14f}\
	\u{150}\x05\x2e\x18\x02\u{150}\u{151}\x05\u{b0}\x59\x02\u{151}\u{152}\x07\
	\u{d1}\x02\x02\u{152}\u{153}\x09\x02\x02\x02\u{153}\u{154}\x05\x3c\x1f\x02\
	\u{154}\u{41b}\x03\x02\x02\x02\u{155}\u{156}\x07\x10\x02\x02\u{156}\u{157}\
	\x05\x2e\x18\x02\u{157}\u{158}\x05\u{b0}\x59\x02\u{158}\u{159}\x07\u{d1}\
	\x02\x02\u{159}\u{15a}\x05\x20\x11\x02\u{15a}\u{41b}\x03\x02\x02\x02\u{15b}\
	\u{15c}\x07\x4d\x02\x02\u{15c}\u{15f}\x05\x2e\x18\x02\u{15d}\u{15e}\x07\
	\x6e\x02\x02\u{15e}\u{160}\x07\x54\x02\x02\u{15f}\u{15d}\x03\x02\x02\x02\
	\u{15f}\u{160}\x03\x02\x02\x02\u{160}\u{161}\x03\x02\x02\x02\u{161}\u{163}\
	\x05\u{b0}\x59\x02\u{162}\u{164}\x09\x03\x02\x02\u{163}\u{162}\x03\x02\x02\
	\x02\u{163}\u{164}\x03\x02\x02\x02\u{164}\u{41b}\x03\x02\x02\x02\u{165}\
	\u{166}\x07\u{d4}\x02\x02\u{166}\u{169}\x09\x04\x02\x02\u{167}\u{168}\x09\
	\x05\x02\x02\u{168}\u{16a}\x05\u{b0}\x59\x02\u{169}\u{167}\x03\x02\x02\x02\
	\u{169}\u{16a}\x03\x02\x02\x02\u{16a}\u{16f}\x03\x02\x02\x02\u{16b}\u{16d}\
	\x07\u{84}\x02\x02\u{16c}\u{16b}\x03\x02\x02\x02\u{16c}\u{16d}\x03\x02\x02\
	\x02\u{16d}\u{16e}\x03\x02\x02\x02\u{16e}\u{170}\x07\u{119}\x02\x02\u{16f}\
	\u{16c}\x03\x02\x02\x02\u{16f}\u{170}\x03\x02\x02\x02\u{170}\u{41b}\x03\
	\x02\x02\x02\u{171}\u{176}\x05\x18\x0d\x02\u{172}\u{173}\x07\x03\x02\x02\
	\u{173}\u{174}\x05\u{e6}\x74\x02\u{174}\u{175}\x07\x04\x02\x02\u{175}\u{177}\
	\x03\x02\x02\x02\u{176}\u{172}\x03\x02\x02\x02\u{176}\u{177}\x03\x02\x02\
	\x02\u{177}\u{178}\x03\x02\x02\x02\u{178}\u{179}\x05\x38\x1d\x02\u{179}\
	\u{17e}\x05\x3a\x1e\x02\u{17a}\u{17c}\x07\x17\x02\x02\u{17b}\u{17a}\x03\
	\x02\x02\x02\u{17b}\u{17c}\x03\x02\x02\x02\u{17c}\u{17d}\x03\x02\x02\x02\
	\u{17d}\u{17f}\x05\x24\x13\x02\u{17e}\u{17b}\x03\x02\x02\x02\u{17e}\u{17f}\
	\x03\x02\x02\x02\u{17f}\u{41b}\x03\x02\x02\x02\u{180}\u{185}\x05\x18\x0d\
	\x02\u{181}\u{182}\x07\x03\x02\x02\u{182}\u{183}\x05\u{e6}\x74\x02\u{183}\
	\u{184}\x07\x04\x02\x02\u{184}\u{186}\x03\x02\x02\x02\u{185}\u{181}\x03\
	\x02\x02\x02\u{185}\u{186}\x03\x02\x02\x02\u{186}\u{19c}\x03\x02\x02\x02\
	\u{187}\u{19b}\x05\x22\x12\x02\u{188}\u{189}\x07\u{a9}\x02\x02\u{189}\u{18a}\
	\x07\x1f\x02\x02\u{18a}\u{18b}\x07\x03\x02\x02\u{18b}\u{18c}\x05\u{e6}\x74\
	\x02\u{18c}\u{18d}\x07\x04\x02\x02\u{18d}\u{192}\x03\x02\x02\x02\u{18e}\
	\u{18f}\x07\u{a9}\x02\x02\u{18f}\u{190}\x07\x1f\x02\x02\u{190}\u{192}\x05\
	\u{98}\x4d\x02\u{191}\u{188}\x03\x02\x02\x02\u{191}\u{18e}\x03\x02\x02\x02\
	\u{192}\u{19b}\x03\x02\x02\x02\u{193}\u{19b}\x05\x1c\x0f\x02\u{194}\u{19b}\
	\x05\x1e\x10\x02\u{195}\u{19b}\x05\u{ac}\x57\x02\u{196}\u{19b}\x05\x48\x25\
	\x02\u{197}\u{19b}\x05\x20\x11\x02\u{198}\u{199}\x07\u{e3}\x02\x02\u{199}\
	\u{19b}\x05\x3c\x1f\x02\u{19a}\u{187}\x03\x02\x02\x02\u{19a}\u{191}\x03\
	\x02\x02\x02\u{19a}\u{193}\x03\x02\x02\x02\u{19a}\u{194}\x03\x02\x02\x02\
	\u{19a}\u{195}\x03\x02\x02\x02\u{19a}\u{196}\x03\x02\x02\x02\u{19a}\u{197}\
	\x03\x02\x02\x02\u{19a}\u{198}\x03\x02\x02\x02\u{19b}\u{19e}\x03\x02\x02\
	\x02\u{19c}\u{19a}\x03\x02\x02\x02\u{19c}\u{19d}\x03\x02\x02\x02\u{19d}\
	\u{1a3}\x03\x02\x02\x02\u{19e}\u{19c}\x03\x02\x02\x02\u{19f}\u{1a1}\x07\
	\x17\x02\x02\u{1a0}\u{19f}\x03\x02\x02\x02\u{1a0}\u{1a1}\x03\x02\x02\x02\
	\u{1a1}\u{1a2}\x03\x02\x02\x02\u{1a2}\u{1a4}\x05\x24\x13\x02\u{1a3}\u{1a0}\
	\x03\x02\x02\x02\u{1a3}\u{1a4}\x03\x02\x02\x02\u{1a4}\u{41b}\x03\x02\x02\
	\x02\u{1a5}\u{1a6}\x07\x36\x02\x02\u{1a6}\u{1aa}\x07\u{e0}\x02\x02\u{1a7}\
	\u{1a8}\x07\x6e\x02\x02\u{1a8}\u{1a9}\x07\u{97}\x02\x02\u{1a9}\u{1ab}\x07\
	\x54\x02\x02\u{1aa}\u{1a7}\x03\x02\x02\x02\u{1aa}\u{1ab}\x03\x02\x02\x02\
	\u{1ab}\u{1ac}\x03\x02\x02\x02\u{1ac}\u{1ad}\x05\u{b2}\x5a\x02\u{1ad}\u{1ae}\
	\x07\u{84}\x02\x02\u{1ae}\u{1b7}\x05\u{b2}\x5a\x02\u{1af}\u{1b6}\x05\x38\
	\x1d\x02\u{1b0}\u{1b6}\x05\u{ac}\x57\x02\u{1b1}\u{1b6}\x05\x48\x25\x02\u{1b2}\
	\u{1b6}\x05\x20\x11\x02\u{1b3}\u{1b4}\x07\u{e3}\x02\x02\u{1b4}\u{1b6}\x05\
	\x3c\x1f\x02\u{1b5}\u{1af}\x03\x02\x02\x02\u{1b5}\u{1b0}\x03\x02\x02\x02\
	\u{1b5}\u{1b1}\x03\x02\x02\x02\u{1b5}\u{1b2}\x03\x02\x02\x02\u{1b5}\u{1b3}\
	\x03\x02\x02\x02\u{1b6}\u{1b9}\x03\x02\x02\x02\u{1b7}\u{1b5}\x03\x02\x02\
	\x02\u{1b7}\u{1b8}\x03\x02\x02\x02\u{1b8}\u{41b}\x03\x02\x02\x02\u{1b9}\
	\u{1b7}\x03\x02\x02\x02\u{1ba}\u{1bf}\x05\x1a\x0e\x02\u{1bb}\u{1bc}\x07\
	\x03\x02\x02\u{1bc}\u{1bd}\x05\u{e6}\x74\x02\u{1bd}\u{1be}\x07\x04\x02\x02\
	\u{1be}\u{1c0}\x03\x02\x02\x02\u{1bf}\u{1bb}\x03\x02\x02\x02\u{1bf}\u{1c0}\
	\x03\x02\x02\x02\u{1c0}\u{1c1}\x03\x02\x02\x02\u{1c1}\u{1c2}\x05\x38\x1d\
	\x02\u{1c2}\u{1c7}\x05\x3a\x1e\x02\u{1c3}\u{1c5}\x07\x17\x02\x02\u{1c4}\
	\u{1c3}\x03\x02\x02\x02\u{1c4}\u{1c5}\x03\x02\x02\x02\u{1c5}\u{1c6}\x03\
	\x02\x02\x02\u{1c6}\u{1c8}\x05\x24\x13\x02\u{1c7}\u{1c4}\x03\x02\x02\x02\
	\u{1c7}\u{1c8}\x03\x02\x02\x02\u{1c8}\u{41b}\x03\x02\x02\x02\u{1c9}\u{1ca}\
	\x07\x11\x02\x02\u{1ca}\u{1cb}\x07\u{e0}\x02\x02\u{1cb}\u{1cd}\x05\u{b0}\
	\x59\x02\u{1cc}\u{1ce}\x05\x2a\x16\x02\u{1cd}\u{1cc}\x03\x02\x02\x02\u{1cd}\
	\u{1ce}\x03\x02\x02\x02\u{1ce}\u{1cf}\x03\x02\x02\x02\u{1cf}\u{1d0}\x07\
	\x32\x02\x02\u{1d0}\u{1d8}\x07\u{da}\x02\x02\u{1d1}\u{1d9}\x05\u{104}\u{83}\
	\x02\u{1d2}\u{1d3}\x07\x61\x02\x02\u{1d3}\u{1d4}\x07\x2d\x02\x02\u{1d4}\
	\u{1d9}\x05\u{9a}\x4e\x02\u{1d5}\u{1d6}\x07\x61\x02\x02\u{1d6}\u{1d7}\x07\
	\x0f\x02\x02\u{1d7}\u{1d9}\x07\x2d\x02\x02\u{1d8}\u{1d1}\x03\x02\x02\x02\
	\u{1d8}\u{1d2}\x03\x02\x02\x02\u{1d8}\u{1d5}\x03\x02\x02\x02\u{1d8}\u{1d9}\
	\x03\x02\x02\x02\u{1d9}\u{41b}\x03\x02\x02\x02\u{1da}\u{1db}\x07\x10\x02\
	\x02\u{1db}\u{1dc}\x07\u{e0}\x02\x02\u{1dc}\u{1dd}\x05\u{b0}\x59\x02\u{1dd}\
	\u{1de}\x07\x0d\x02\x02\u{1de}\u{1df}\x09\x06\x02\x02\u{1df}\u{1e0}\x05\
	\u{e2}\x72\x02\u{1e0}\u{41b}\x03\x02\x02\x02\u{1e1}\u{1e2}\x07\x10\x02\x02\
	\u{1e2}\u{1e3}\x07\u{e0}\x02\x02\u{1e3}\u{1e4}\x05\u{b0}\x59\x02\u{1e4}\
	\u{1e5}\x07\x0d\x02\x02\u{1e5}\u{1e6}\x09\x06\x02\x02\u{1e6}\u{1e7}\x07\
	\x03\x02\x02\u{1e7}\u{1e8}\x05\u{e2}\x72\x02\u{1e8}\u{1e9}\x07\x04\x02\x02\
	\u{1e9}\u{41b}\x03\x02\x02\x02\u{1ea}\u{1eb}\x07\x10\x02\x02\u{1eb}\u{1ec}\
	\x07\u{e0}\x02\x02\u{1ec}\u{1ed}\x05\u{b0}\x59\x02\u{1ed}\u{1ee}\x07\u{bc}\
	\x02\x02\u{1ee}\u{1ef}\x07\x2c\x02\x02\u{1ef}\u{1f0}\x05\u{b0}\x59\x02\u{1f0}\
	\u{1f1}\x07\u{e8}\x02\x02\u{1f1}\u{1f2}\x05\u{100}\u{81}\x02\u{1f2}\u{41b}\
	\x03\x02\x02\x02\u{1f3}\u{1f4}\x07\x10\x02\x02\u{1f4}\u{1f5}\x07\u{e0}\x02\
	\x02\u{1f5}\u{1f6}\x05\u{b0}\x59\x02\u{1f6}\u{1f7}\x07\x4d\x02\x02\u{1f7}\
	\u{1f8}\x09\x06\x02\x02\u{1f8}\u{1f9}\x07\x03\x02\x02\u{1f9}\u{1fa}\x05\
	\u{ae}\x58\x02\u{1fa}\u{1fb}\x07\x04\x02\x02\u{1fb}\u{41b}\x03\x02\x02\x02\
	\u{1fc}\u{1fd}\x07\x10\x02\x02\u{1fd}\u{1fe}\x07\u{e0}\x02\x02\u{1fe}\u{1ff}\
	\x05\u{b0}\x59\x02\u{1ff}\u{200}\x07\x4d\x02\x02\u{200}\u{201}\x09\x06\x02\
	\x02\u{201}\u{202}\x05\u{ae}\x58\x02\u{202}\u{41b}\x03\x02\x02\x02\u{203}\
	\u{204}\x07\x10\x02\x02\u{204}\u{205}\x09\x07\x02\x02\u{205}\u{206}\x05\
	\u{b0}\x59\x02\u{206}\u{207}\x07\u{bc}\x02\x02\u{207}\u{208}\x07\u{e8}\x02\
	\x02\u{208}\u{209}\x05\u{b0}\x59\x02\u{209}\u{41b}\x03\x02\x02\x02\u{20a}\
	\u{20b}\x07\x10\x02\x02\u{20b}\u{20c}\x09\x07\x02\x02\u{20c}\u{20d}\x05\
	\u{b0}\x59\x02\u{20d}\u{20e}\x07\u{d1}\x02\x02\u{20e}\u{20f}\x07\u{e3}\x02\
	\x02\u{20f}\u{210}\x05\x3c\x1f\x02\u{210}\u{41b}\x03\x02\x02\x02\u{211}\
	\u{212}\x07\x10\x02\x02\u{212}\u{213}\x09\x07\x02\x02\u{213}\u{214}\x05\
	\u{b0}\x59\x02\u{214}\u{215}\x07\u{f9}\x02\x02\u{215}\u{218}\x07\u{e3}\x02\
	\x02\u{216}\u{217}\x07\x6e\x02\x02\u{217}\u{219}\x07\x54\x02\x02\u{218}\
	\u{216}\x03\x02\x02\x02\u{218}\u{219}\x03\x02\x02\x02\u{219}\u{21a}\x03\
	\x02\x02\x02\u{21a}\u{21b}\x05\x3c\x1f\x02\u{21b}\u{41b}\x03\x02\x02\x02\
	\u{21c}\u{21d}\x07\x10\x02\x02\u{21d}\u{21e}\x07\u{e0}\x02\x02\u{21e}\u{21f}\
	\x05\u{b0}\x59\x02\u{21f}\u{221}\x09\x08\x02\x02\u{220}\u{222}\x07\x2c\x02\
	\x02\u{221}\u{220}\x03\x02\x02\x02\u{221}\u{222}\x03\x02\x02\x02\u{222}\
	\u{223}\x03\x02\x02\x02\u{223}\u{225}\x05\u{b0}\x59\x02\u{224}\u{226}\x05\
	\u{10c}\u{87}\x02\u{225}\u{224}\x03\x02\x02\x02\u{225}\u{226}\x03\x02\x02\
	\x02\u{226}\u{41b}\x03\x02\x02\x02\u{227}\u{228}\x07\x10\x02\x02\u{228}\
	\u{229}\x07\u{e0}\x02\x02\u{229}\u{22b}\x05\u{b0}\x59\x02\u{22a}\u{22c}\
	\x05\x2a\x16\x02\u{22b}\u{22a}\x03\x02\x02\x02\u{22b}\u{22c}\x03\x02\x02\
	\x02\u{22c}\u{22d}\x03\x02\x02\x02\u{22d}\u{22f}\x07\x24\x02\x02\u{22e}\
	\u{230}\x07\x2c\x02\x02\u{22f}\u{22e}\x03\x02\x02\x02\u{22f}\u{230}\x03\
	\x02\x02\x02\u{230}\u{231}\x03\x02\x02\x02\u{231}\u{232}\x05\u{b0}\x59\x02\
	\u{232}\u{234}\x05\u{e8}\x75\x02\u{233}\u{235}\x05\u{de}\x70\x02\u{234}\
	\u{233}\x03\x02\x02\x02\u{234}\u{235}\x03\x02\x02\x02\u{235}\u{41b}\x03\
	\x02\x02\x02\u{236}\u{237}\x07\x10\x02\x02\u{237}\u{238}\x07\u{e0}\x02\x02\
	\u{238}\u{23a}\x05\u{b0}\x59\x02\u{239}\u{23b}\x05\x2a\x16\x02\u{23a}\u{239}\
	\x03\x02\x02\x02\u{23a}\u{23b}\x03\x02\x02\x02\u{23b}\u{23c}\x03\x02\x02\
	\x02\u{23c}\u{23d}\x07\u{be}\x02\x02\u{23d}\u{23e}\x07\x2d\x02\x02\u{23e}\
	\u{23f}\x07\x03\x02\x02\u{23f}\u{240}\x05\u{e2}\x72\x02\u{240}\u{241}\x07\
	\x04\x02\x02\u{241}\u{41b}\x03\x02\x02\x02\u{242}\u{243}\x07\x10\x02\x02\
	\u{243}\u{244}\x07\u{e0}\x02\x02\u{244}\u{246}\x05\u{b0}\x59\x02\u{245}\
	\u{247}\x05\x2a\x16\x02\u{246}\u{245}\x03\x02\x02\x02\u{246}\u{247}\x03\
	\x02\x02\x02\u{247}\u{248}\x03\x02\x02\x02\u{248}\u{249}\x07\u{d1}\x02\x02\
	\u{249}\u{24a}\x07\u{ce}\x02\x02\u{24a}\u{24e}\x07\u{119}\x02\x02\u{24b}\
	\u{24c}\x07\u{104}\x02\x02\u{24c}\u{24d}\x07\u{cf}\x02\x02\u{24d}\u{24f}\
	\x05\x3c\x1f\x02\u{24e}\u{24b}\x03\x02\x02\x02\u{24e}\u{24f}\x03\x02\x02\
	\x02\u{24f}\u{41b}\x03\x02\x02\x02\u{250}\u{251}\x07\x10\x02\x02\u{251}\
	\u{252}\x07\u{e0}\x02\x02\u{252}\u{254}\x05\u{b0}\x59\x02\u{253}\u{255}\
	\x05\x2a\x16\x02\u{254}\u{253}\x03\x02\x02\x02\u{254}\u{255}\x03\x02\x02\
	\x02\u{255}\u{256}\x03\x02\x02\x02\u{256}\u{257}\x07\u{d1}\x02\x02\u{257}\
	\u{258}\x07\u{cf}\x02\x02\u{258}\u{259}\x05\x3c\x1f\x02\u{259}\u{41b}\x03\
	\x02\x02\x02\u{25a}\u{25b}\x07\x10\x02\x02\u{25b}\u{25c}\x09\x07\x02\x02\
	\u{25c}\u{25d}\x05\u{b0}\x59\x02\u{25d}\u{261}\x07\x0d\x02\x02\u{25e}\u{25f}\
	\x07\x6e\x02\x02\u{25f}\u{260}\x07\u{97}\x02\x02\u{260}\u{262}\x07\x54\x02\
	\x02\u{261}\u{25e}\x03\x02\x02\x02\u{261}\u{262}\x03\x02\x02\x02\u{262}\
	\u{264}\x03\x02\x02\x02\u{263}\u{265}\x05\x28\x15\x02\u{264}\u{263}\x03\
	\x02\x02\x02\u{265}\u{266}\x03\x02\x02\x02\u{266}\u{264}\x03\x02\x02\x02\
	\u{266}\u{267}\x03\x02\x02\x02\u{267}\u{41b}\x03\x02\x02\x02\u{268}\u{269}\
	\x07\x10\x02\x02\u{269}\u{26a}\x07\u{e0}\x02\x02\u{26a}\u{26b}\x05\u{b0}\
	\x59\x02\u{26b}\u{26c}\x05\x2a\x16\x02\u{26c}\u{26d}\x07\u{bc}\x02\x02\u{26d}\
	\u{26e}\x07\u{e8}\x02\x02\u{26e}\u{26f}\x05\x2a\x16\x02\u{26f}\u{41b}\x03\
	\x02\x02\x02\u{270}\u{271}\x07\x10\x02\x02\u{271}\u{272}\x09\x07\x02\x02\
	\u{272}\u{273}\x05\u{b0}\x59\x02\u{273}\u{276}\x07\x4d\x02\x02\u{274}\u{275}\
	\x07\x6e\x02\x02\u{275}\u{277}\x07\x54\x02\x02\u{276}\u{274}\x03\x02\x02\
	\x02\u{276}\u{277}\x03\x02\x02\x02\u{277}\u{278}\x03\x02\x02\x02\u{278}\
	\u{27d}\x05\x2a\x16\x02\u{279}\u{27a}\x07\x05\x02\x02\u{27a}\u{27c}\x05\
	\x2a\x16\x02\u{27b}\u{279}\x03\x02\x02\x02\u{27c}\u{27f}\x03\x02\x02\x02\
	\u{27d}\u{27b}\x03\x02\x02\x02\u{27d}\u{27e}\x03\x02\x02\x02\u{27e}\u{281}\
	\x03\x02\x02\x02\u{27f}\u{27d}\x03\x02\x02\x02\u{280}\u{282}\x07\u{b3}\x02\
	\x02\u{281}\u{280}\x03\x02\x02\x02\u{281}\u{282}\x03\x02\x02\x02\u{282}\
	\u{41b}\x03\x02\x02\x02\u{283}\u{284}\x07\x10\x02\x02\u{284}\u{285}\x07\
	\u{e0}\x02\x02\u{285}\u{287}\x05\u{b0}\x59\x02\u{286}\u{288}\x05\x2a\x16\
	\x02\u{287}\u{286}\x03\x02\x02\x02\u{287}\u{288}\x03\x02\x02\x02\u{288}\
	\u{289}\x03\x02\x02\x02\u{289}\u{28a}\x07\u{d1}\x02\x02\u{28a}\u{28b}\x05\
	\x20\x11\x02\u{28b}\u{41b}\x03\x02\x02\x02\u{28c}\u{28d}\x07\x10\x02\x02\
	\u{28d}\u{28e}\x07\u{e0}\x02\x02\u{28e}\u{28f}\x05\u{b0}\x59\x02\u{28f}\
	\u{290}\x07\u{b8}\x02\x02\u{290}\u{291}\x07\u{aa}\x02\x02\u{291}\u{41b}\
	\x03\x02\x02\x02\u{292}\u{293}\x07\x4d\x02\x02\u{293}\u{296}\x07\u{e0}\x02\
	\x02\u{294}\u{295}\x07\x6e\x02\x02\u{295}\u{297}\x07\x54\x02\x02\u{296}\
	\u{294}\x03\x02\x02\x02\u{296}\u{297}\x03\x02\x02\x02\u{297}\u{298}\x03\
	\x02\x02\x02\u{298}\u{29a}\x05\u{b0}\x59\x02\u{299}\u{29b}\x07\u{b3}\x02\
	\x02\u{29a}\u{299}\x03\x02\x02\x02\u{29a}\u{29b}\x03\x02\x02\x02\u{29b}\
	\u{41b}\x03\x02\x02\x02\u{29c}\u{29d}\x07\x4d\x02\x02\u{29d}\u{2a0}\x07\
	\u{ff}\x02\x02\u{29e}\u{29f}\x07\x6e\x02\x02\u{29f}\u{2a1}\x07\x54\x02\x02\
	\u{2a0}\u{29e}\x03\x02\x02\x02\u{2a0}\u{2a1}\x03\x02\x02\x02\u{2a1}\u{2a2}\
	\x03\x02\x02\x02\u{2a2}\u{41b}\x05\u{b0}\x59\x02\u{2a3}\u{2a6}\x07\x36\x02\
	\x02\u{2a4}\u{2a5}\x07\u{9f}\x02\x02\u{2a5}\u{2a7}\x07\u{be}\x02\x02\u{2a6}\
	\u{2a4}\x03\x02\x02\x02\u{2a6}\u{2a7}\x03\x02\x02\x02\u{2a7}\u{2ac}\x03\
	\x02\x02\x02\u{2a8}\u{2aa}\x07\x69\x02\x02\u{2a9}\u{2a8}\x03\x02\x02\x02\
	\u{2a9}\u{2aa}\x03\x02\x02\x02\u{2aa}\u{2ab}\x03\x02\x02\x02\u{2ab}\u{2ad}\
	\x07\u{e4}\x02\x02\u{2ac}\u{2a9}\x03\x02\x02\x02\u{2ac}\u{2ad}\x03\x02\x02\
	\x02\u{2ad}\u{2ae}\x03\x02\x02\x02\u{2ae}\u{2b2}\x07\u{ff}\x02\x02\u{2af}\
	\u{2b0}\x07\x6e\x02\x02\u{2b0}\u{2b1}\x07\u{97}\x02\x02\u{2b1}\u{2b3}\x07\
	\x54\x02\x02\u{2b2}\u{2af}\x03\x02\x02\x02\u{2b2}\u{2b3}\x03\x02\x02\x02\
	\u{2b3}\u{2b4}\x03\x02\x02\x02\u{2b4}\u{2b6}\x05\u{b0}\x59\x02\u{2b5}\u{2b7}\
	\x05\u{a0}\x51\x02\u{2b6}\u{2b5}\x03\x02\x02\x02\u{2b6}\u{2b7}\x03\x02\x02\
	\x02\u{2b7}\u{2c0}\x03\x02\x02\x02\u{2b8}\u{2bf}\x05\x22\x12\x02\u{2b9}\
	\u{2ba}\x07\u{a9}\x02\x02\u{2ba}\u{2bb}\x07\u{9b}\x02\x02\u{2bb}\u{2bf}\
	\x05\u{98}\x4d\x02\u{2bc}\u{2bd}\x07\u{e3}\x02\x02\u{2bd}\u{2bf}\x05\x3c\
	\x1f\x02\u{2be}\u{2b8}\x03\x02\x02\x02\u{2be}\u{2b9}\x03\x02\x02\x02\u{2be}\
	\u{2bc}\x03\x02\x02\x02\u{2bf}\u{2c2}\x03\x02\x02\x02\u{2c0}\u{2be}\x03\
	\x02\x02\x02\u{2c0}\u{2c1}\x03\x02\x02\x02\u{2c1}\u{2c3}\x03\x02\x02\x02\
	\u{2c2}\u{2c0}\x03\x02\x02\x02\u{2c3}\u{2c4}\x07\x17\x02\x02\u{2c4}\u{2c5}\
	\x05\x24\x13\x02\u{2c5}\u{41b}\x03\x02\x02\x02\u{2c6}\u{2c9}\x07\x36\x02\
	\x02\u{2c7}\u{2c8}\x07\u{9f}\x02\x02\u{2c8}\u{2ca}\x07\u{be}\x02\x02\u{2c9}\
	\u{2c7}\x03\x02\x02\x02\u{2c9}\u{2ca}\x03\x02\x02\x02\u{2ca}\u{2cc}\x03\
	\x02\x02\x02\u{2cb}\u{2cd}\x07\x69\x02\x02\u{2cc}\u{2cb}\x03\x02\x02\x02\
	\u{2cc}\u{2cd}\x03\x02\x02\x02\u{2cd}\u{2ce}\x03\x02\x02\x02\u{2ce}\u{2cf}\
	\x07\u{e4}\x02\x02\u{2cf}\u{2d0}\x07\u{ff}\x02\x02\u{2d0}\u{2d5}\x05\u{b2}\
	\x5a\x02\u{2d1}\u{2d2}\x07\x03\x02\x02\u{2d2}\u{2d3}\x05\u{e6}\x74\x02\u{2d3}\
	\u{2d4}\x07\x04\x02\x02\u{2d4}\u{2d6}\x03\x02\x02\x02\u{2d5}\u{2d1}\x03\
	\x02\x02\x02\u{2d5}\u{2d6}\x03\x02\x02\x02\u{2d6}\u{2d7}\x03\x02\x02\x02\
	\u{2d7}\u{2da}\x05\x38\x1d\x02\u{2d8}\u{2d9}\x07\u{9e}\x02\x02\u{2d9}\u{2db}\
	\x05\x3c\x1f\x02\u{2da}\u{2d8}\x03\x02\x02\x02\u{2da}\u{2db}\x03\x02\x02\
	\x02\u{2db}\u{41b}\x03\x02\x02\x02\u{2dc}\u{2dd}\x07\x10\x02\x02\u{2dd}\
	\u{2de}\x07\u{ff}\x02\x02\u{2de}\u{2e0}\x05\u{b0}\x59\x02\u{2df}\u{2e1}\
	\x07\x17\x02\x02\u{2e0}\u{2df}\x03\x02\x02\x02\u{2e0}\u{2e1}\x03\x02\x02\
	\x02\u{2e1}\u{2e2}\x03\x02\x02\x02\u{2e2}\u{2e3}\x05\x24\x13\x02\u{2e3}\
	\u{41b}\x03\x02\x02\x02\u{2e4}\u{2e7}\x07\x36\x02\x02\u{2e5}\u{2e6}\x07\
	\u{9f}\x02\x02\u{2e6}\u{2e8}\x07\u{be}\x02\x02\u{2e7}\u{2e5}\x03\x02\x02\
	\x02\u{2e7}\u{2e8}\x03\x02\x02\x02\u{2e8}\u{2ea}\x03\x02\x02\x02\u{2e9}\
	\u{2eb}\x07\u{e4}\x02\x02\u{2ea}\u{2e9}\x03\x02\x02\x02\u{2ea}\u{2eb}\x03\
	\x02\x02\x02\u{2eb}\u{2ec}\x03\x02\x02\x02\u{2ec}\u{2f0}\x07\x67\x02\x02\
	\u{2ed}\u{2ee}\x07\x6e\x02\x02\u{2ee}\u{2ef}\x07\u{97}\x02\x02\u{2ef}\u{2f1}\
	\x07\x54\x02\x02\u{2f0}\u{2ed}\x03\x02\x02\x02\u{2f0}\u{2f1}\x03\x02\x02\
	\x02\u{2f1}\u{2f2}\x03\x02\x02\x02\u{2f2}\u{2f3}\x05\u{b0}\x59\x02\u{2f3}\
	\u{2f4}\x07\x17\x02\x02\u{2f4}\u{2fe}\x07\u{119}\x02\x02\u{2f5}\u{2f6}\x07\
	\u{fd}\x02\x02\u{2f6}\u{2fb}\x05\x4e\x28\x02\u{2f7}\u{2f8}\x07\x05\x02\x02\
	\u{2f8}\u{2fa}\x05\x4e\x28\x02\u{2f9}\u{2f7}\x03\x02\x02\x02\u{2fa}\u{2fd}\
	\x03\x02\x02\x02\u{2fb}\u{2f9}\x03\x02\x02\x02\u{2fb}\u{2fc}\x03\x02\x02\
	\x02\u{2fc}\u{2ff}\x03\x02\x02\x02\u{2fd}\u{2fb}\x03\x02\x02\x02\u{2fe}\
	\u{2f5}\x03\x02\x02\x02\u{2fe}\u{2ff}\x03\x02\x02\x02\u{2ff}\u{41b}\x03\
	\x02\x02\x02\u{300}\u{302}\x07\x4d\x02\x02\u{301}\u{303}\x07\u{e4}\x02\x02\
	\u{302}\u{301}\x03\x02\x02\x02\u{302}\u{303}\x03\x02\x02\x02\u{303}\u{304}\
	\x03\x02\x02\x02\u{304}\u{307}\x07\x67\x02\x02\u{305}\u{306}\x07\x6e\x02\
	\x02\u{306}\u{308}\x07\x54\x02\x02\u{307}\u{305}\x03\x02\x02\x02\u{307}\
	\u{308}\x03\x02\x02\x02\u{308}\u{309}\x03\x02\x02\x02\u{309}\u{41b}\x05\
	\u{b0}\x59\x02\u{30a}\u{30c}\x07\x55\x02\x02\u{30b}\u{30d}\x09\x09\x02\x02\
	\u{30c}\u{30b}\x03\x02\x02\x02\u{30c}\u{30d}\x03\x02\x02\x02\u{30d}\u{30e}\
	\x03\x02\x02\x02\u{30e}\u{41b}\x05\x12\x0a\x02\u{30f}\u{310}\x07\u{d4}\x02\
	\x02\u{310}\u{313}\x07\u{e1}\x02\x02\u{311}\u{312}\x09\x05\x02\x02\u{312}\
	\u{314}\x05\u{b0}\x59\x02\u{313}\u{311}\x03\x02\x02\x02\u{313}\u{314}\x03\
	\x02\x02\x02\u{314}\u{319}\x03\x02\x02\x02\u{315}\u{317}\x07\u{84}\x02\x02\
	\u{316}\u{315}\x03\x02\x02\x02\u{316}\u{317}\x03\x02\x02\x02\u{317}\u{318}\
	\x03\x02\x02\x02\u{318}\u{31a}\x07\u{119}\x02\x02\u{319}\u{316}\x03\x02\
	\x02\x02\u{319}\u{31a}\x03\x02\x02\x02\u{31a}\u{41b}\x03\x02\x02\x02\u{31b}\
	\u{31c}\x07\u{d4}\x02\x02\u{31c}\u{31d}\x07\u{e0}\x02\x02\u{31d}\u{320}\
	\x07\x57\x02\x02\u{31e}\u{31f}\x09\x05\x02\x02\u{31f}\u{321}\x05\u{b0}\x59\
	\x02\u{320}\u{31e}\x03\x02\x02\x02\u{320}\u{321}\x03\x02\x02\x02\u{321}\
	\u{322}\x03\x02\x02\x02\u{322}\u{323}\x07\u{84}\x02\x02\u{323}\u{325}\x07\
	\u{119}\x02\x02\u{324}\u{326}\x05\x2a\x16\x02\u{325}\u{324}\x03\x02\x02\
	\x02\u{325}\u{326}\x03\x02\x02\x02\u{326}\u{41b}\x03\x02\x02\x02\u{327}\
	\u{328}\x07\u{d4}\x02\x02\u{328}\u{329}\x07\u{e3}\x02\x02\u{329}\u{32e}\
	\x05\u{b0}\x59\x02\u{32a}\u{32b}\x07\x03\x02\x02\u{32b}\u{32c}\x05\x40\x21\
	\x02\u{32c}\u{32d}\x07\x04\x02\x02\u{32d}\u{32f}\x03\x02\x02\x02\u{32e}\
	\u{32a}\x03\x02\x02\x02\u{32e}\u{32f}\x03\x02\x02\x02\u{32f}\u{41b}\x03\
	\x02\x02\x02\u{330}\u{331}\x07\u{d4}\x02\x02\u{331}\u{332}\x07\x2d\x02\x02\
	\u{332}\u{333}\x09\x05\x02\x02\u{333}\u{336}\x05\u{b0}\x59\x02\u{334}\u{335}\
	\x09\x05\x02\x02\u{335}\u{337}\x05\u{b0}\x59\x02\u{336}\u{334}\x03\x02\x02\
	\x02\u{336}\u{337}\x03\x02\x02\x02\u{337}\u{41b}\x03\x02\x02\x02\u{338}\
	\u{339}\x07\u{d4}\x02\x02\u{339}\u{33c}\x07\u{100}\x02\x02\u{33a}\u{33b}\
	\x09\x05\x02\x02\u{33b}\u{33d}\x05\u{b0}\x59\x02\u{33c}\u{33a}\x03\x02\x02\
	\x02\u{33c}\u{33d}\x03\x02\x02\x02\u{33d}\u{342}\x03\x02\x02\x02\u{33e}\
	\u{340}\x07\u{84}\x02\x02\u{33f}\u{33e}\x03\x02\x02\x02\u{33f}\u{340}\x03\
	\x02\x02\x02\u{340}\u{341}\x03\x02\x02\x02\u{341}\u{343}\x07\u{119}\x02\
	\x02\u{342}\u{33f}\x03\x02\x02\x02\u{342}\u{343}\x03\x02\x02\x02\u{343}\
	\u{41b}\x03\x02\x02\x02\u{344}\u{345}\x07\u{d4}\x02\x02\u{345}\u{346}\x07\
	\u{aa}\x02\x02\u{346}\u{348}\x05\u{b0}\x59\x02\u{347}\u{349}\x05\x2a\x16\
	\x02\u{348}\u{347}\x03\x02\x02\x02\u{348}\u{349}\x03\x02\x02\x02\u{349}\
	\u{41b}\x03\x02\x02\x02\u{34a}\u{34c}\x07\u{d4}\x02\x02\u{34b}\u{34d}\x05\
	\u{104}\u{83}\x02\u{34c}\u{34b}\x03\x02\x02\x02\u{34c}\u{34d}\x03\x02\x02\
	\x02\u{34d}\u{34e}\x03\x02\x02\x02\u{34e}\u{356}\x07\x68\x02\x02\u{34f}\
	\u{351}\x07\u{84}\x02\x02\u{350}\u{34f}\x03\x02\x02\x02\u{350}\u{351}\x03\
	\x02\x02\x02\u{351}\u{354}\x03\x02\x02\x02\u{352}\u{355}\x05\u{b0}\x59\x02\
	\u{353}\u{355}\x07\u{119}\x02\x02\u{354}\u{352}\x03\x02\x02\x02\u{354}\u{353}\
	\x03\x02\x02\x02\u{355}\u{357}\x03\x02\x02\x02\u{356}\u{350}\x03\x02\x02\
	\x02\u{356}\u{357}\x03\x02\x02\x02\u{357}\u{41b}\x03\x02\x02\x02\u{358}\
	\u{359}\x07\u{d4}\x02\x02\u{359}\u{35a}\x07\x36\x02\x02\u{35a}\u{35b}\x07\
	\u{e0}\x02\x02\u{35b}\u{35e}\x05\u{b0}\x59\x02\u{35c}\u{35d}\x07\x17\x02\
	\x02\u{35d}\u{35f}\x07\u{ce}\x02\x02\u{35e}\u{35c}\x03\x02\x02\x02\u{35e}\
	\u{35f}\x03\x02\x02\x02\u{35f}\u{41b}\x03\x02\x02\x02\u{360}\u{361}\x07\
	\u{d4}\x02\x02\u{361}\u{362}\x07\x39\x02\x02\u{362}\u{41b}\x07\u{93}\x02\
	\x02\u{363}\u{364}\x09\x0a\x02\x02\u{364}\u{366}\x07\x67\x02\x02\u{365}\
	\u{367}\x07\x57\x02\x02\u{366}\u{365}\x03\x02\x02\x02\u{366}\u{367}\x03\
	\x02\x02\x02\u{367}\u{368}\x03\x02\x02\x02\u{368}\u{41b}\x05\x30\x19\x02\
	\u{369}\u{36a}\x09\x0a\x02\x02\u{36a}\u{36c}\x05\x2e\x18\x02\u{36b}\u{36d}\
	\x07\x57\x02\x02\u{36c}\u{36b}\x03\x02\x02\x02\u{36c}\u{36d}\x03\x02\x02\
	\x02\u{36d}\u{36e}\x03\x02\x02\x02\u{36e}\u{36f}\x05\u{b0}\x59\x02\u{36f}\
	\u{41b}\x03\x02\x02\x02\u{370}\u{372}\x09\x0a\x02\x02\u{371}\u{373}\x07\
	\u{e0}\x02\x02\u{372}\u{371}\x03\x02\x02\x02\u{372}\u{373}\x03\x02\x02\x02\
	\u{373}\u{375}\x03\x02\x02\x02\u{374}\u{376}\x09\x0b\x02\x02\u{375}\u{374}\
	\x03\x02\x02\x02\u{375}\u{376}\x03\x02\x02\x02\u{376}\u{377}\x03\x02\x02\
	\x02\u{377}\u{379}\x05\u{b0}\x59\x02\u{378}\u{37a}\x05\x2a\x16\x02\u{379}\
	\u{378}\x03\x02\x02\x02\u{379}\u{37a}\x03\x02\x02\x02\u{37a}\u{37c}\x03\
	\x02\x02\x02\u{37b}\u{37d}\x05\x32\x1a\x02\u{37c}\u{37b}\x03\x02\x02\x02\
	\u{37c}\u{37d}\x03\x02\x02\x02\u{37d}\u{41b}\x03\x02\x02\x02\u{37e}\u{380}\
	\x09\x0a\x02\x02\u{37f}\u{381}\x07\u{b4}\x02\x02\u{380}\u{37f}\x03\x02\x02\
	\x02\u{380}\u{381}\x03\x02\x02\x02\u{381}\u{382}\x03\x02\x02\x02\u{382}\
	\u{41b}\x05\x24\x13\x02\u{383}\u{384}\x07\x2e\x02\x02\u{384}\u{385}\x07\
	\u{9b}\x02\x02\u{385}\u{386}\x05\x2e\x18\x02\u{386}\u{387}\x05\u{b0}\x59\
	\x02\u{387}\u{388}\x07\x7b\x02\x02\u{388}\u{389}\x09\x0c\x02\x02\u{389}\
	\u{41b}\x03\x02\x02\x02\u{38a}\u{38b}\x07\x2e\x02\x02\u{38b}\u{38c}\x07\
	\u{9b}\x02\x02\u{38c}\u{38d}\x07\u{e0}\x02\x02\u{38d}\u{38e}\x05\u{b0}\x59\
	\x02\u{38e}\u{38f}\x07\x7b\x02\x02\u{38f}\u{390}\x09\x0c\x02\x02\u{390}\
	\u{41b}\x03\x02\x02\x02\u{391}\u{392}\x07\u{bb}\x02\x02\u{392}\u{393}\x07\
	\u{e0}\x02\x02\u{393}\u{41b}\x05\u{b0}\x59\x02\u{394}\u{395}\x07\u{bb}\x02\
	\x02\u{395}\u{396}\x07\x67\x02\x02\u{396}\u{41b}\x05\u{b0}\x59\x02\u{397}\
	\u{39f}\x07\u{bb}\x02\x02\u{398}\u{3a0}\x07\u{119}\x02\x02\u{399}\u{39b}\
	\x0b\x02\x02\x02\u{39a}\u{399}\x03\x02\x02\x02\u{39b}\u{39e}\x03\x02\x02\
	\x02\u{39c}\u{39d}\x03\x02\x02\x02\u{39c}\u{39a}\x03\x02\x02\x02\u{39d}\
	\u{3a0}\x03\x02\x02\x02\u{39e}\u{39c}\x03\x02\x02\x02\u{39f}\u{398}\x03\
	\x02\x02\x02\u{39f}\u{39c}\x03\x02\x02\x02\u{3a0}\u{41b}\x03\x02\x02\x02\
	\u{3a1}\u{3a3}\x07\x20\x02\x02\u{3a2}\u{3a4}\x07\u{81}\x02\x02\u{3a3}\u{3a2}\
	\x03\x02\x02\x02\u{3a3}\u{3a4}\x03\x02\x02\x02\u{3a4}\u{3a5}\x03\x02\x02\
	\x02\u{3a5}\u{3a6}\x07\u{e0}\x02\x02\u{3a6}\u{3a9}\x05\u{b0}\x59\x02\u{3a7}\
	\u{3a8}\x07\u{9e}\x02\x02\u{3a8}\u{3aa}\x05\x3c\x1f\x02\u{3a9}\u{3a7}\x03\
	\x02\x02\x02\u{3a9}\u{3aa}\x03\x02\x02\x02\u{3aa}\u{3af}\x03\x02\x02\x02\
	\u{3ab}\u{3ad}\x07\x17\x02\x02\u{3ac}\u{3ab}\x03\x02\x02\x02\u{3ac}\u{3ad}\
	\x03\x02\x02\x02\u{3ad}\u{3ae}\x03\x02\x02\x02\u{3ae}\u{3b0}\x05\x24\x13\
	\x02\u{3af}\u{3ac}\x03\x02\x02\x02\u{3af}\u{3b0}\x03\x02\x02\x02\u{3b0}\
	\u{41b}\x03\x02\x02\x02\u{3b1}\u{3b2}\x07\u{f4}\x02\x02\u{3b2}\u{3b5}\x07\
	\u{e0}\x02\x02\u{3b3}\u{3b4}\x07\x6e\x02\x02\u{3b4}\u{3b6}\x07\x54\x02\x02\
	\u{3b5}\u{3b3}\x03\x02\x02\x02\u{3b5}\u{3b6}\x03\x02\x02\x02\u{3b6}\u{3b7}\
	\x03\x02\x02\x02\u{3b7}\u{41b}\x05\u{b0}\x59\x02\u{3b8}\u{3b9}\x07\x26\x02\
	\x02\u{3b9}\u{41b}\x07\x20\x02\x02\u{3ba}\u{3bb}\x07\u{88}\x02\x02\u{3bb}\
	\u{3bd}\x07\x3e\x02\x02\u{3bc}\u{3be}\x07\u{89}\x02\x02\u{3bd}\u{3bc}\x03\
	\x02\x02\x02\u{3bd}\u{3be}\x03\x02\x02\x02\u{3be}\u{3bf}\x03\x02\x02\x02\
	\u{3bf}\u{3c0}\x07\x75\x02\x02\u{3c0}\u{3c2}\x07\u{119}\x02\x02\u{3c1}\u{3c3}\
	\x07\u{a7}\x02\x02\u{3c2}\u{3c1}\x03\x02\x02\x02\u{3c2}\u{3c3}\x03\x02\x02\
	\x02\u{3c3}\u{3c4}\x03\x02\x02\x02\u{3c4}\u{3c5}\x07\x7a\x02\x02\u{3c5}\
	\u{3c6}\x07\u{e0}\x02\x02\u{3c6}\u{3c8}\x05\u{b0}\x59\x02\u{3c7}\u{3c9}\
	\x05\x2a\x16\x02\u{3c8}\u{3c7}\x03\x02\x02\x02\u{3c8}\u{3c9}\x03\x02\x02\
	\x02\u{3c9}\u{41b}\x03\x02\x02\x02\u{3ca}\u{3cb}\x07\u{f0}\x02\x02\u{3cb}\
	\u{3cc}\x07\u{e0}\x02\x02\u{3cc}\u{3ce}\x05\u{b0}\x59\x02\u{3cd}\u{3cf}\
	\x05\x2a\x16\x02\u{3ce}\u{3cd}\x03\x02\x02\x02\u{3ce}\u{3cf}\x03\x02\x02\
	\x02\u{3cf}\u{41b}\x03\x02\x02\x02\u{3d0}\u{3d1}\x07\u{92}\x02\x02\u{3d1}\
	\u{3d2}\x07\u{bd}\x02\x02\u{3d2}\u{3d3}\x07\u{e0}\x02\x02\u{3d3}\u{41b}\
	\x05\u{b0}\x59\x02\u{3d4}\u{3d5}\x09\x0d\x02\x02\u{3d5}\u{3dd}\x05\u{104}\
	\u{83}\x02\u{3d6}\u{3de}\x07\u{119}\x02\x02\u{3d7}\u{3d9}\x0b\x02\x02\x02\
	\u{3d8}\u{3d7}\x03\x02\x02\x02\u{3d9}\u{3dc}\x03\x02\x02\x02\u{3da}\u{3db}\
	\x03\x02\x02\x02\u{3da}\u{3d8}\x03\x02\x02\x02\u{3db}\u{3de}\x03\x02\x02\
	\x02\u{3dc}\u{3da}\x03\x02\x02\x02\u{3dd}\u{3d6}\x03\x02\x02\x02\u{3dd}\
	\u{3da}\x03\x02\x02\x02\u{3de}\u{41b}\x03\x02\x02\x02\u{3df}\u{3e0}\x07\
	\u{d1}\x02\x02\u{3e0}\u{3e4}\x07\u{c4}\x02\x02\u{3e1}\u{3e3}\x0b\x02\x02\
	\x02\u{3e2}\u{3e1}\x03\x02\x02\x02\u{3e3}\u{3e6}\x03\x02\x02\x02\u{3e4}\
	\u{3e5}\x03\x02\x02\x02\u{3e4}\u{3e2}\x03\x02\x02\x02\u{3e5}\u{41b}\x03\
	\x02\x02\x02\u{3e6}\u{3e4}\x03\x02\x02\x02\u{3e7}\u{3e8}\x07\u{d1}\x02\x02\
	\u{3e8}\u{3e9}\x07\u{e7}\x02\x02\u{3e9}\u{3ea}\x07\u{105}\x02\x02\u{3ea}\
	\u{41b}\x05\u{d2}\x6a\x02\u{3eb}\u{3ec}\x07\u{d1}\x02\x02\u{3ec}\u{3ed}\
	\x07\u{e7}\x02\x02\u{3ed}\u{3ee}\x07\u{105}\x02\x02\u{3ee}\u{41b}\x09\x0e\
	\x02\x02\u{3ef}\u{3f0}\x07\u{d1}\x02\x02\u{3f0}\u{3f1}\x07\u{e7}\x02\x02\
	\u{3f1}\u{3f5}\x07\u{105}\x02\x02\u{3f2}\u{3f4}\x0b\x02\x02\x02\u{3f3}\u{3f2}\
	\x03\x02\x02\x02\u{3f4}\u{3f7}\x03\x02\x02\x02\u{3f5}\u{3f6}\x03\x02\x02\
	\x02\u{3f5}\u{3f3}\x03\x02\x02\x02\u{3f6}\u{41b}\x03\x02\x02\x02\u{3f7}\
	\u{3f5}\x03\x02\x02\x02\u{3f8}\u{3f9}\x07\u{d1}\x02\x02\u{3f9}\u{401}\x05\
	\x14\x0b\x02\u{3fa}\u{3fe}\x07\u{106}\x02\x02\u{3fb}\u{3fd}\x0b\x02\x02\
	\x02\u{3fc}\u{3fb}\x03\x02\x02\x02\u{3fd}\u{400}\x03\x02\x02\x02\u{3fe}\
	\u{3ff}\x03\x02\x02\x02\u{3fe}\u{3fc}\x03\x02\x02\x02\u{3ff}\u{402}\x03\
	\x02\x02\x02\u{400}\u{3fe}\x03\x02\x02\x02\u{401}\u{3fa}\x03\x02\x02\x02\
	\u{401}\u{402}\x03\x02\x02\x02\u{402}\u{41b}\x03\x02\x02\x02\u{403}\u{407}\
	\x07\u{d1}\x02\x02\u{404}\u{406}\x0b\x02\x02\x02\u{405}\u{404}\x03\x02\x02\
	\x02\u{406}\u{409}\x03\x02\x02\x02\u{407}\u{408}\x03\x02\x02\x02\u{407}\
	\u{405}\x03\x02\x02\x02\u{408}\u{41b}\x03\x02\x02\x02\u{409}\u{407}\x03\
	\x02\x02\x02\u{40a}\u{40b}\x07\u{bf}\x02\x02\u{40b}\u{41b}\x05\x14\x0b\x02\
	\u{40c}\u{410}\x07\u{bf}\x02\x02\u{40d}\u{40f}\x0b\x02\x02\x02\u{40e}\u{40d}\
	\x03\x02\x02\x02\u{40f}\u{412}\x03\x02\x02\x02\u{410}\u{411}\x03\x02\x02\
	\x02\u{410}\u{40e}\x03\x02\x02\x02\u{411}\u{41b}\x03\x02\x02\x02\u{412}\
	\u{410}\x03\x02\x02\x02\u{413}\u{417}\x05\x16\x0c\x02\u{414}\u{416}\x0b\
	\x02\x02\x02\u{415}\u{414}\x03\x02\x02\x02\u{416}\u{419}\x03\x02\x02\x02\
	\u{417}\u{418}\x03\x02\x02\x02\u{417}\u{415}\x03\x02\x02\x02\u{418}\u{41b}\
	\x03\x02\x02\x02\u{419}\u{417}\x03\x02\x02\x02\u{41a}\u{132}\x03\x02\x02\
	\x02\u{41a}\u{134}\x03\x02\x02\x02\u{41a}\u{137}\x03\x02\x02\x02\u{41a}\
	\u{13c}\x03\x02\x02\x02\u{41a}\u{14e}\x03\x02\x02\x02\u{41a}\u{155}\x03\
	\x02\x02\x02\u{41a}\u{15b}\x03\x02\x02\x02\u{41a}\u{165}\x03\x02\x02\x02\
	\u{41a}\u{171}\x03\x02\x02\x02\u{41a}\u{180}\x03\x02\x02\x02\u{41a}\u{1a5}\
	\x03\x02\x02\x02\u{41a}\u{1ba}\x03\x02\x02\x02\u{41a}\u{1c9}\x03\x02\x02\
	\x02\u{41a}\u{1da}\x03\x02\x02\x02\u{41a}\u{1e1}\x03\x02\x02\x02\u{41a}\
	\u{1ea}\x03\x02\x02\x02\u{41a}\u{1f3}\x03\x02\x02\x02\u{41a}\u{1fc}\x03\
	\x02\x02\x02\u{41a}\u{203}\x03\x02\x02\x02\u{41a}\u{20a}\x03\x02\x02\x02\
	\u{41a}\u{211}\x03\x02\x02\x02\u{41a}\u{21c}\x03\x02\x02\x02\u{41a}\u{227}\
	\x03\x02\x02\x02\u{41a}\u{236}\x03\x02\x02\x02\u{41a}\u{242}\x03\x02\x02\
	\x02\u{41a}\u{250}\x03\x02\x02\x02\u{41a}\u{25a}\x03\x02\x02\x02\u{41a}\
	\u{268}\x03\x02\x02\x02\u{41a}\u{270}\x03\x02\x02\x02\u{41a}\u{283}\x03\
	\x02\x02\x02\u{41a}\u{28c}\x03\x02\x02\x02\u{41a}\u{292}\x03\x02\x02\x02\
	\u{41a}\u{29c}\x03\x02\x02\x02\u{41a}\u{2a3}\x03\x02\x02\x02\u{41a}\u{2c6}\
	\x03\x02\x02\x02\u{41a}\u{2dc}\x03\x02\x02\x02\u{41a}\u{2e4}\x03\x02\x02\
	\x02\u{41a}\u{300}\x03\x02\x02\x02\u{41a}\u{30a}\x03\x02\x02\x02\u{41a}\
	\u{30f}\x03\x02\x02\x02\u{41a}\u{31b}\x03\x02\x02\x02\u{41a}\u{327}\x03\
	\x02\x02\x02\u{41a}\u{330}\x03\x02\x02\x02\u{41a}\u{338}\x03\x02\x02\x02\
	\u{41a}\u{344}\x03\x02\x02\x02\u{41a}\u{34a}\x03\x02\x02\x02\u{41a}\u{358}\
	\x03\x02\x02\x02\u{41a}\u{360}\x03\x02\x02\x02\u{41a}\u{363}\x03\x02\x02\
	\x02\u{41a}\u{369}\x03\x02\x02\x02\u{41a}\u{370}\x03\x02\x02\x02\u{41a}\
	\u{37e}\x03\x02\x02\x02\u{41a}\u{383}\x03\x02\x02\x02\u{41a}\u{38a}\x03\
	\x02\x02\x02\u{41a}\u{391}\x03\x02\x02\x02\u{41a}\u{394}\x03\x02\x02\x02\
	\u{41a}\u{397}\x03\x02\x02\x02\u{41a}\u{3a1}\x03\x02\x02\x02\u{41a}\u{3b1}\
	\x03\x02\x02\x02\u{41a}\u{3b8}\x03\x02\x02\x02\u{41a}\u{3ba}\x03\x02\x02\
	\x02\u{41a}\u{3ca}\x03\x02\x02\x02\u{41a}\u{3d0}\x03\x02\x02\x02\u{41a}\
	\u{3d4}\x03\x02\x02\x02\u{41a}\u{3df}\x03\x02\x02\x02\u{41a}\u{3e7}\x03\
	\x02\x02\x02\u{41a}\u{3eb}\x03\x02\x02\x02\u{41a}\u{3ef}\x03\x02\x02\x02\
	\u{41a}\u{3f8}\x03\x02\x02\x02\u{41a}\u{403}\x03\x02\x02\x02\u{41a}\u{40a}\
	\x03\x02\x02\x02\u{41a}\u{40c}\x03\x02\x02\x02\u{41a}\u{413}\x03\x02\x02\
	\x02\u{41b}\x13\x03\x02\x02\x02\u{41c}\u{41d}\x05\u{108}\u{85}\x02\u{41d}\
	\x15\x03\x02\x02\x02\u{41e}\u{41f}\x07\x36\x02\x02\u{41f}\u{4c7}\x07\u{c4}\
	\x02\x02\u{420}\u{421}\x07\x4d\x02\x02\u{421}\u{4c7}\x07\u{c4}\x02\x02\u{422}\
	\u{424}\x07\x6a\x02\x02\u{423}\u{425}\x07\u{c4}\x02\x02\u{424}\u{423}\x03\
	\x02\x02\x02\u{424}\u{425}\x03\x02\x02\x02\u{425}\u{4c7}\x03\x02\x02\x02\
	\u{426}\u{428}\x07\u{c1}\x02\x02\u{427}\u{429}\x07\u{c4}\x02\x02\u{428}\
	\u{427}\x03\x02\x02\x02\u{428}\u{429}\x03\x02\x02\x02\u{429}\u{4c7}\x03\
	\x02\x02\x02\u{42a}\u{42b}\x07\u{d4}\x02\x02\u{42b}\u{4c7}\x07\x6a\x02\x02\
	\u{42c}\u{42d}\x07\u{d4}\x02\x02\u{42d}\u{42f}\x07\u{c4}\x02\x02\u{42e}\
	\u{430}\x07\x6a\x02\x02\u{42f}\u{42e}\x03\x02\x02\x02\u{42f}\u{430}\x03\
	\x02\x02\x02\u{430}\u{4c7}\x03\x02\x02\x02\u{431}\u{432}\x07\u{d4}\x02\x02\
	\u{432}\u{4c7}\x07\u{b1}\x02\x02\u{433}\u{434}\x07\u{d4}\x02\x02\u{434}\
	\u{4c7}\x07\u{c5}\x02\x02\u{435}\u{436}\x07\u{d4}\x02\x02\u{436}\u{437}\
	\x07\x39\x02\x02\u{437}\u{4c7}\x07\u{c5}\x02\x02\u{438}\u{439}\x07\x56\x02\
	\x02\u{439}\u{4c7}\x07\u{e0}\x02\x02\u{43a}\u{43b}\x07\x70\x02\x02\u{43b}\
	\u{4c7}\x07\u{e0}\x02\x02\u{43c}\u{43d}\x07\u{d4}\x02\x02\u{43d}\u{4c7}\
	\x07\x31\x02\x02\u{43e}\u{43f}\x07\u{d4}\x02\x02\u{43f}\u{440}\x07\x36\x02\
	\x02\u{440}\u{4c7}\x07\u{e0}\x02\x02\u{441}\u{442}\x07\u{d4}\x02\x02\u{442}\
	\u{4c7}\x07\u{ec}\x02\x02\u{443}\u{444}\x07\u{d4}\x02\x02\u{444}\u{4c7}\
	\x07\x73\x02\x02\u{445}\u{446}\x07\u{d4}\x02\x02\u{446}\u{4c7}\x07\u{8c}\
	\x02\x02\u{447}\u{448}\x07\x36\x02\x02\u{448}\u{4c7}\x07\x72\x02\x02\u{449}\
	\u{44a}\x07\x4d\x02\x02\u{44a}\u{4c7}\x07\x72\x02\x02\u{44b}\u{44c}\x07\
	\x10\x02\x02\u{44c}\u{4c7}\x07\x72\x02\x02\u{44d}\u{44e}\x07\u{8b}\x02\x02\
	\u{44e}\u{4c7}\x07\u{e0}\x02\x02\u{44f}\u{450}\x07\u{8b}\x02\x02\u{450}\
	\u{4c7}\x07\x3f\x02\x02\u{451}\u{452}\x07\u{f8}\x02\x02\u{452}\u{4c7}\x07\
	\u{e0}\x02\x02\u{453}\u{454}\x07\u{f8}\x02\x02\u{454}\u{4c7}\x07\x3f\x02\
	\x02\u{455}\u{456}\x07\x36\x02\x02\u{456}\u{457}\x07\u{e4}\x02\x02\u{457}\
	\u{4c7}\x07\u{8e}\x02\x02\u{458}\u{459}\x07\x4d\x02\x02\u{459}\u{45a}\x07\
	\u{e4}\x02\x02\u{45a}\u{4c7}\x07\u{8e}\x02\x02\u{45b}\u{45c}\x07\x10\x02\
	\x02\u{45c}\u{45d}\x07\u{e0}\x02\x02\u{45d}\u{45e}\x05\u{b2}\x5a\x02\u{45e}\
	\u{45f}\x07\u{97}\x02\x02\u{45f}\u{460}\x07\x28\x02\x02\u{460}\u{4c7}\x03\
	\x02\x02\x02\u{461}\u{462}\x07\x10\x02\x02\u{462}\u{463}\x07\u{e0}\x02\x02\
	\u{463}\u{464}\x05\u{b2}\x5a\x02\u{464}\u{465}\x07\x28\x02\x02\u{465}\u{466}\
	\x07\x1f\x02\x02\u{466}\u{4c7}\x03\x02\x02\x02\u{467}\u{468}\x07\x10\x02\
	\x02\u{468}\u{469}\x07\u{e0}\x02\x02\u{469}\u{46a}\x05\u{b2}\x5a\x02\u{46a}\
	\u{46b}\x07\u{97}\x02\x02\u{46b}\u{46c}\x07\u{d8}\x02\x02\u{46c}\u{4c7}\
	\x03\x02\x02\x02\u{46d}\u{46e}\x07\x10\x02\x02\u{46e}\u{46f}\x07\u{e0}\x02\
	\x02\u{46f}\u{470}\x05\u{b2}\x5a\x02\u{470}\u{471}\x07\u{d5}\x02\x02\u{471}\
	\u{472}\x07\x1f\x02\x02\u{472}\u{4c7}\x03\x02\x02\x02\u{473}\u{474}\x07\
	\x10\x02\x02\u{474}\u{475}\x07\u{e0}\x02\x02\u{475}\u{476}\x05\u{b2}\x5a\
	\x02\u{476}\u{477}\x07\u{97}\x02\x02\u{477}\u{478}\x07\u{d5}\x02\x02\u{478}\
	\u{4c7}\x03\x02\x02\x02\u{479}\u{47a}\x07\x10\x02\x02\u{47a}\u{47b}\x07\
	\u{e0}\x02\x02\u{47b}\u{47c}\x05\u{b2}\x5a\x02\u{47c}\u{47d}\x07\u{97}\x02\
	\x02\u{47d}\u{47e}\x07\u{db}\x02\x02\u{47e}\u{47f}\x07\x17\x02\x02\u{47f}\
	\u{480}\x07\x48\x02\x02\u{480}\u{4c7}\x03\x02\x02\x02\u{481}\u{482}\x07\
	\x10\x02\x02\u{482}\u{483}\x07\u{e0}\x02\x02\u{483}\u{484}\x05\u{b2}\x5a\
	\x02\u{484}\u{485}\x07\u{d1}\x02\x02\u{485}\u{486}\x07\u{d5}\x02\x02\u{486}\
	\u{487}\x07\u{8a}\x02\x02\u{487}\u{4c7}\x03\x02\x02\x02\u{488}\u{489}\x07\
	\x10\x02\x02\u{489}\u{48a}\x07\u{e0}\x02\x02\u{48a}\u{48b}\x05\u{b2}\x5a\
	\x02\u{48b}\u{48c}\x07\x53\x02\x02\u{48c}\u{48d}\x07\u{a8}\x02\x02\u{48d}\
	\u{4c7}\x03\x02\x02\x02\u{48e}\u{48f}\x07\x10\x02\x02\u{48f}\u{490}\x07\
	\u{e0}\x02\x02\u{490}\u{491}\x05\u{b2}\x5a\x02\u{491}\u{492}\x07\x15\x02\
	\x02\u{492}\u{493}\x07\u{a8}\x02\x02\u{493}\u{4c7}\x03\x02\x02\x02\u{494}\
	\u{495}\x07\x10\x02\x02\u{495}\u{496}\x07\u{e0}\x02\x02\u{496}\u{497}\x05\
	\u{b2}\x5a\x02\u{497}\u{498}\x07\u{f2}\x02\x02\u{498}\u{499}\x07\u{a8}\x02\
	\x02\u{499}\u{4c7}\x03\x02\x02\x02\u{49a}\u{49b}\x07\x10\x02\x02\u{49b}\
	\u{49c}\x07\u{e0}\x02\x02\u{49c}\u{49d}\x05\u{b2}\x5a\x02\u{49d}\u{49e}\
	\x07\u{e9}\x02\x02\u{49e}\u{4c7}\x03\x02\x02\x02\u{49f}\u{4a0}\x07\x10\x02\
	\x02\u{4a0}\u{4a1}\x07\u{e0}\x02\x02\u{4a1}\u{4a3}\x05\u{b2}\x5a\x02\u{4a2}\
	\u{4a4}\x05\x2a\x16\x02\u{4a3}\u{4a2}\x03\x02\x02\x02\u{4a3}\u{4a4}\x03\
	\x02\x02\x02\u{4a4}\u{4a5}\x03\x02\x02\x02\u{4a5}\u{4a6}\x07\x30\x02\x02\
	\u{4a6}\u{4c7}\x03\x02\x02\x02\u{4a7}\u{4a8}\x07\x10\x02\x02\u{4a8}\u{4a9}\
	\x07\u{e0}\x02\x02\u{4a9}\u{4ab}\x05\u{b2}\x5a\x02\u{4aa}\u{4ac}\x05\x2a\
	\x16\x02\u{4ab}\u{4aa}\x03\x02\x02\x02\u{4ab}\u{4ac}\x03\x02\x02\x02\u{4ac}\
	\u{4ad}\x03\x02\x02\x02\u{4ad}\u{4ae}\x07\x33\x02\x02\u{4ae}\u{4c7}\x03\
	\x02\x02\x02\u{4af}\u{4b0}\x07\x10\x02\x02\u{4b0}\u{4b1}\x07\u{e0}\x02\x02\
	\u{4b1}\u{4b3}\x05\u{b2}\x5a\x02\u{4b2}\u{4b4}\x05\x2a\x16\x02\u{4b3}\u{4b2}\
	\x03\x02\x02\x02\u{4b3}\u{4b4}\x03\x02\x02\x02\u{4b4}\u{4b5}\x03\x02\x02\
	\x02\u{4b5}\u{4b6}\x07\u{d1}\x02\x02\u{4b6}\u{4b7}\x07\x5e\x02\x02\u{4b7}\
	\u{4c7}\x03\x02\x02\x02\u{4b8}\u{4b9}\x07\x10\x02\x02\u{4b9}\u{4ba}\x07\
	\u{e0}\x02\x02\u{4ba}\u{4bc}\x05\u{b2}\x5a\x02\u{4bb}\u{4bd}\x05\x2a\x16\
	\x02\u{4bc}\u{4bb}\x03\x02\x02\x02\u{4bc}\u{4bd}\x03\x02\x02\x02\u{4bd}\
	\u{4be}\x03\x02\x02\x02\u{4be}\u{4bf}\x07\u{be}\x02\x02\u{4bf}\u{4c0}\x07\
	\x2d\x02\x02\u{4c0}\u{4c7}\x03\x02\x02\x02\u{4c1}\u{4c2}\x07\u{d9}\x02\x02\
	\u{4c2}\u{4c7}\x07\u{eb}\x02\x02\u{4c3}\u{4c7}\x07\x2f\x02\x02\u{4c4}\u{4c7}\
	\x07\u{c6}\x02\x02\u{4c5}\u{4c7}\x07\x47\x02\x02\u{4c6}\u{41e}\x03\x02\x02\
	\x02\u{4c6}\u{420}\x03\x02\x02\x02\u{4c6}\u{422}\x03\x02\x02\x02\u{4c6}\
	\u{426}\x03\x02\x02\x02\u{4c6}\u{42a}\x03\x02\x02\x02\u{4c6}\u{42c}\x03\
	\x02\x02\x02\u{4c6}\u{431}\x03\x02\x02\x02\u{4c6}\u{433}\x03\x02\x02\x02\
	\u{4c6}\u{435}\x03\x02\x02\x02\u{4c6}\u{438}\x03\x02\x02\x02\u{4c6}\u{43a}\
	\x03\x02\x02\x02\u{4c6}\u{43c}\x03\x02\x02\x02\u{4c6}\u{43e}\x03\x02\x02\
	\x02\u{4c6}\u{441}\x03\x02\x02\x02\u{4c6}\u{443}\x03\x02\x02\x02\u{4c6}\
	\u{445}\x03\x02\x02\x02\u{4c6}\u{447}\x03\x02\x02\x02\u{4c6}\u{449}\x03\
	\x02\x02\x02\u{4c6}\u{44b}\x03\x02\x02\x02\u{4c6}\u{44d}\x03\x02\x02\x02\
	\u{4c6}\u{44f}\x03\x02\x02\x02\u{4c6}\u{451}\x03\x02\x02\x02\u{4c6}\u{453}\
	\x03\x02\x02\x02\u{4c6}\u{455}\x03\x02\x02\x02\u{4c6}\u{458}\x03\x02\x02\
	\x02\u{4c6}\u{45b}\x03\x02\x02\x02\u{4c6}\u{461}\x03\x02\x02\x02\u{4c6}\
	\u{467}\x03\x02\x02\x02\u{4c6}\u{46d}\x03\x02\x02\x02\u{4c6}\u{473}\x03\
	\x02\x02\x02\u{4c6}\u{479}\x03\x02\x02\x02\u{4c6}\u{481}\x03\x02\x02\x02\
	\u{4c6}\u{488}\x03\x02\x02\x02\u{4c6}\u{48e}\x03\x02\x02\x02\u{4c6}\u{494}\
	\x03\x02\x02\x02\u{4c6}\u{49a}\x03\x02\x02\x02\u{4c6}\u{49f}\x03\x02\x02\
	\x02\u{4c6}\u{4a7}\x03\x02\x02\x02\u{4c6}\u{4af}\x03\x02\x02\x02\u{4c6}\
	\u{4b8}\x03\x02\x02\x02\u{4c6}\u{4c1}\x03\x02\x02\x02\u{4c6}\u{4c3}\x03\
	\x02\x02\x02\u{4c6}\u{4c4}\x03\x02\x02\x02\u{4c6}\u{4c5}\x03\x02\x02\x02\
	\u{4c7}\x17\x03\x02\x02\x02\u{4c8}\u{4ca}\x07\x36\x02\x02\u{4c9}\u{4cb}\
	\x07\u{e4}\x02\x02\u{4ca}\u{4c9}\x03\x02\x02\x02\u{4ca}\u{4cb}\x03\x02\x02\
	\x02\u{4cb}\u{4cd}\x03\x02\x02\x02\u{4cc}\u{4ce}\x07\x58\x02\x02\u{4cd}\
	\u{4cc}\x03\x02\x02\x02\u{4cd}\u{4ce}\x03\x02\x02\x02\u{4ce}\u{4cf}\x03\
	\x02\x02\x02\u{4cf}\u{4d3}\x07\u{e0}\x02\x02\u{4d0}\u{4d1}\x07\x6e\x02\x02\
	\u{4d1}\u{4d2}\x07\u{97}\x02\x02\u{4d2}\u{4d4}\x07\x54\x02\x02\u{4d3}\u{4d0}\
	\x03\x02\x02\x02\u{4d3}\u{4d4}\x03\x02\x02\x02\u{4d4}\u{4d5}\x03\x02\x02\
	\x02\u{4d5}\u{4d6}\x05\u{b0}\x59\x02\u{4d6}\x19\x03\x02\x02\x02\u{4d7}\u{4d8}\
	\x07\x36\x02\x02\u{4d8}\u{4da}\x07\u{9f}\x02\x02\u{4d9}\u{4d7}\x03\x02\x02\
	\x02\u{4d9}\u{4da}\x03\x02\x02\x02\u{4da}\u{4db}\x03\x02\x02\x02\u{4db}\
	\u{4dc}\x07\u{be}\x02\x02\u{4dc}\u{4dd}\x07\u{e0}\x02\x02\u{4dd}\u{4de}\
	\x05\u{b0}\x59\x02\u{4de}\x1b\x03\x02\x02\x02\u{4df}\u{4e0}\x07\x28\x02\
	\x02\u{4e0}\u{4e1}\x07\x1f\x02\x02\u{4e1}\u{4e5}\x05\u{98}\x4d\x02\u{4e2}\
	\u{4e3}\x07\u{d8}\x02\x02\u{4e3}\u{4e4}\x07\x1f\x02\x02\u{4e4}\u{4e6}\x05\
	\u{9c}\x4f\x02\u{4e5}\u{4e2}\x03\x02\x02\x02\u{4e5}\u{4e6}\x03\x02\x02\x02\
	\u{4e6}\u{4e7}\x03\x02\x02\x02\u{4e7}\u{4e8}\x07\x7a\x02\x02\u{4e8}\u{4e9}\
	\x07\u{11d}\x02\x02\u{4e9}\u{4ea}\x07\x1e\x02\x02\u{4ea}\x1d\x03\x02\x02\
	\x02\u{4eb}\u{4ec}\x07\u{d5}\x02\x02\u{4ec}\u{4ed}\x07\x1f\x02\x02\u{4ed}\
	\u{4ee}\x05\u{98}\x4d\x02\u{4ee}\u{4f1}\x07\u{9b}\x02\x02\u{4ef}\u{4f2}\
	\x05\x44\x23\x02\u{4f0}\u{4f2}\x05\x46\x24\x02\u{4f1}\u{4ef}\x03\x02\x02\
	\x02\u{4f1}\u{4f0}\x03\x02\x02\x02\u{4f2}\u{4f6}\x03\x02\x02\x02\u{4f3}\
	\u{4f4}\x07\u{db}\x02\x02\u{4f4}\u{4f5}\x07\x17\x02\x02\u{4f5}\u{4f7}\x07\
	\x48\x02\x02\u{4f6}\u{4f3}\x03\x02\x02\x02\u{4f6}\u{4f7}\x03\x02\x02\x02\
	\u{4f7}\x1f\x03\x02\x02\x02\u{4f8}\u{4f9}\x07\u{8a}\x02\x02\u{4f9}\u{4fa}\
	\x07\u{119}\x02\x02\u{4fa}\x21\x03\x02\x02\x02\u{4fb}\u{4fc}\x07\x2e\x02\
	\x02\u{4fc}\u{4fd}\x07\u{119}\x02\x02\u{4fd}\x23\x03\x02\x02\x02\u{4fe}\
	\u{500}\x05\x34\x1b\x02\u{4ff}\u{4fe}\x03\x02\x02\x02\u{4ff}\u{500}\x03\
	\x02\x02\x02\u{500}\u{501}\x03\x02\x02\x02\u{501}\u{502}\x05\x56\x2c\x02\
	\u{502}\u{503}\x05\x52\x2a\x02\u{503}\x25\x03\x02\x02\x02\u{504}\u{505}\
	\x07\x77\x02\x02\u{505}\u{507}\x07\u{a7}\x02\x02\u{506}\u{508}\x07\u{e0}\
	\x02\x02\u{507}\u{506}\x03\x02\x02\x02\u{507}\u{508}\x03\x02\x02\x02\u{508}\
	\u{509}\x03\x02\x02\x02\u{509}\u{510}\x05\u{b0}\x59\x02\u{50a}\u{50e}\x05\
	\x2a\x16\x02\u{50b}\u{50c}\x07\x6e\x02\x02\u{50c}\u{50d}\x07\u{97}\x02\x02\
	\u{50d}\u{50f}\x07\x54\x02\x02\u{50e}\u{50b}\x03\x02\x02\x02\u{50e}\u{50f}\
	\x03\x02\x02\x02\u{50f}\u{511}\x03\x02\x02\x02\u{510}\u{50a}\x03\x02\x02\
	\x02\u{510}\u{511}\x03\x02\x02\x02\u{511}\u{53c}\x03\x02\x02\x02\u{512}\
	\u{513}\x07\x77\x02\x02\u{513}\u{515}\x07\x7a\x02\x02\u{514}\u{516}\x07\
	\u{e0}\x02\x02\u{515}\u{514}\x03\x02\x02\x02\u{515}\u{516}\x03\x02\x02\x02\
	\u{516}\u{517}\x03\x02\x02\x02\u{517}\u{519}\x05\u{b0}\x59\x02\u{518}\u{51a}\
	\x05\x2a\x16\x02\u{519}\u{518}\x03\x02\x02\x02\u{519}\u{51a}\x03\x02\x02\
	\x02\u{51a}\u{51e}\x03\x02\x02\x02\u{51b}\u{51c}\x07\x6e\x02\x02\u{51c}\
	\u{51d}\x07\u{97}\x02\x02\u{51d}\u{51f}\x07\x54\x02\x02\u{51e}\u{51b}\x03\
	\x02\x02\x02\u{51e}\u{51f}\x03\x02\x02\x02\u{51f}\u{53c}\x03\x02\x02\x02\
	\u{520}\u{521}\x07\x77\x02\x02\u{521}\u{523}\x07\u{a7}\x02\x02\u{522}\u{524}\
	\x07\u{89}\x02\x02\u{523}\u{522}\x03\x02\x02\x02\u{523}\u{524}\x03\x02\x02\
	\x02\u{524}\u{525}\x03\x02\x02\x02\u{525}\u{526}\x07\x49\x02\x02\u{526}\
	\u{528}\x07\u{119}\x02\x02\u{527}\u{529}\x05\u{ac}\x57\x02\u{528}\u{527}\
	\x03\x02\x02\x02\u{528}\u{529}\x03\x02\x02\x02\u{529}\u{52b}\x03\x02\x02\
	\x02\u{52a}\u{52c}\x05\x48\x25\x02\u{52b}\u{52a}\x03\x02\x02\x02\u{52b}\
	\u{52c}\x03\x02\x02\x02\u{52c}\u{53c}\x03\x02\x02\x02\u{52d}\u{52e}\x07\
	\x77\x02\x02\u{52e}\u{530}\x07\u{a7}\x02\x02\u{52f}\u{531}\x07\u{89}\x02\
	\x02\u{530}\u{52f}\x03\x02\x02\x02\u{530}\u{531}\x03\x02\x02\x02\u{531}\
	\u{532}\x03\x02\x02\x02\u{532}\u{534}\x07\x49\x02\x02\u{533}\u{535}\x07\
	\u{119}\x02\x02\u{534}\u{533}\x03\x02\x02\x02\u{534}\u{535}\x03\x02\x02\
	\x02\u{535}\u{536}\x03\x02\x02\x02\u{536}\u{539}\x05\x38\x1d\x02\u{537}\
	\u{538}\x07\u{9e}\x02\x02\u{538}\u{53a}\x05\x3c\x1f\x02\u{539}\u{537}\x03\
	\x02\x02\x02\u{539}\u{53a}\x03\x02\x02\x02\u{53a}\u{53c}\x03\x02\x02\x02\
	\u{53b}\u{504}\x03\x02\x02\x02\u{53b}\u{512}\x03\x02\x02\x02\u{53b}\u{520}\
	\x03\x02\x02\x02\u{53b}\u{52d}\x03\x02\x02\x02\u{53c}\x27\x03\x02\x02\x02\
	\u{53d}\u{53f}\x05\x2a\x16\x02\u{53e}\u{540}\x05\x20\x11\x02\u{53f}\u{53e}\
	\x03\x02\x02\x02\u{53f}\u{540}\x03\x02\x02\x02\u{540}\x29\x03\x02\x02\x02\
	\u{541}\u{542}\x07\u{a8}\x02\x02\u{542}\u{543}\x07\x03\x02\x02\u{543}\u{548}\
	\x05\x2c\x17\x02\u{544}\u{545}\x07\x05\x02\x02\u{545}\u{547}\x05\x2c\x17\
	\x02\u{546}\u{544}\x03\x02\x02\x02\u{547}\u{54a}\x03\x02\x02\x02\u{548}\
	\u{546}\x03\x02\x02\x02\u{548}\u{549}\x03\x02\x02\x02\u{549}\u{54b}\x03\
	\x02\x02\x02\u{54a}\u{548}\x03\x02\x02\x02\u{54b}\u{54c}\x07\x04\x02\x02\
	\u{54c}\x2b\x03\x02\x02\x02\u{54d}\u{550}\x05\u{104}\u{83}\x02\u{54e}\u{54f}\
	\x07\u{106}\x02\x02\u{54f}\u{551}\x05\u{c8}\x65\x02\u{550}\u{54e}\x03\x02\
	\x02\x02\u{550}\u{551}\x03\x02\x02\x02\u{551}\x2d\x03\x02\x02\x02\u{552}\
	\u{553}\x09\x0f\x02\x02\u{553}\x2f\x03\x02\x02\x02\u{554}\u{55a}\x05\u{fe}\
	\u{80}\x02\u{555}\u{55a}\x07\u{119}\x02\x02\u{556}\u{55a}\x05\u{ca}\x66\
	\x02\u{557}\u{55a}\x05\u{cc}\x67\x02\u{558}\u{55a}\x05\u{ce}\x68\x02\u{559}\
	\u{554}\x03\x02\x02\x02\u{559}\u{555}\x03\x02\x02\x02\u{559}\u{556}\x03\
	\x02\x02\x02\u{559}\u{557}\x03\x02\x02\x02\u{559}\u{558}\x03\x02\x02\x02\
	\u{55a}\x31\x03\x02\x02\x02\u{55b}\u{560}\x05\u{104}\u{83}\x02\u{55c}\u{55d}\
	\x07\x06\x02\x02\u{55d}\u{55f}\x05\u{104}\u{83}\x02\u{55e}\u{55c}\x03\x02\
	\x02\x02\u{55f}\u{562}\x03\x02\x02\x02\u{560}\u{55e}\x03\x02\x02\x02\u{560}\
	\u{561}\x03\x02\x02\x02\u{561}\x33\x03\x02\x02\x02\u{562}\u{560}\x03\x02\
	\x02\x02\u{563}\u{564}\x07\u{104}\x02\x02\u{564}\u{569}\x05\x36\x1c\x02\
	\u{565}\u{566}\x07\x05\x02\x02\u{566}\u{568}\x05\x36\x1c\x02\u{567}\u{565}\
	\x03\x02\x02\x02\u{568}\u{56b}\x03\x02\x02\x02\u{569}\u{567}\x03\x02\x02\
	\x02\u{569}\u{56a}\x03\x02\x02\x02\u{56a}\x35\x03\x02\x02\x02\u{56b}\u{569}\
	\x03\x02\x02\x02\u{56c}\u{56e}\x05\u{100}\u{81}\x02\u{56d}\u{56f}\x05\u{98}\
	\x4d\x02\u{56e}\u{56d}\x03\x02\x02\x02\u{56e}\u{56f}\x03\x02\x02\x02\u{56f}\
	\u{571}\x03\x02\x02\x02\u{570}\u{572}\x07\x17\x02\x02\u{571}\u{570}\x03\
	\x02\x02\x02\u{571}\u{572}\x03\x02\x02\x02\u{572}\u{573}\x03\x02\x02\x02\
	\u{573}\u{574}\x07\x03\x02\x02\u{574}\u{575}\x05\x24\x13\x02\u{575}\u{576}\
	\x07\x04\x02\x02\u{576}\x37\x03\x02\x02\x02\u{577}\u{578}\x07\u{fd}\x02\
	\x02\u{578}\u{579}\x05\u{b0}\x59\x02\u{579}\x39\x03\x02\x02\x02\u{57a}\u{57b}\
	\x07\u{9e}\x02\x02\u{57b}\u{585}\x05\x3c\x1f\x02\u{57c}\u{57d}\x07\u{a9}\
	\x02\x02\u{57d}\u{57e}\x07\x1f\x02\x02\u{57e}\u{585}\x05\u{b8}\x5d\x02\u{57f}\
	\u{585}\x05\x1c\x0f\x02\u{580}\u{585}\x05\x20\x11\x02\u{581}\u{585}\x05\
	\x22\x12\x02\u{582}\u{583}\x07\u{e3}\x02\x02\u{583}\u{585}\x05\x3c\x1f\x02\
	\u{584}\u{57a}\x03\x02\x02\x02\u{584}\u{57c}\x03\x02\x02\x02\u{584}\u{57f}\
	\x03\x02\x02\x02\u{584}\u{580}\x03\x02\x02\x02\u{584}\u{581}\x03\x02\x02\
	\x02\u{584}\u{582}\x03\x02\x02\x02\u{585}\u{588}\x03\x02\x02\x02\u{586}\
	\u{584}\x03\x02\x02\x02\u{586}\u{587}\x03\x02\x02\x02\u{587}\x3b\x03\x02\
	\x02\x02\u{588}\u{586}\x03\x02\x02\x02\u{589}\u{58a}\x07\x03\x02\x02\u{58a}\
	\u{58f}\x05\x3e\x20\x02\u{58b}\u{58c}\x07\x05\x02\x02\u{58c}\u{58e}\x05\
	\x3e\x20\x02\u{58d}\u{58b}\x03\x02\x02\x02\u{58e}\u{591}\x03\x02\x02\x02\
	\u{58f}\u{58d}\x03\x02\x02\x02\u{58f}\u{590}\x03\x02\x02\x02\u{590}\u{592}\
	\x03\x02\x02\x02\u{591}\u{58f}\x03\x02\x02\x02\u{592}\u{593}\x07\x04\x02\
	\x02\u{593}\x3d\x03\x02\x02\x02\u{594}\u{599}\x05\x40\x21\x02\u{595}\u{597}\
	\x07\u{106}\x02\x02\u{596}\u{595}\x03\x02\x02\x02\u{596}\u{597}\x03\x02\
	\x02\x02\u{597}\u{598}\x03\x02\x02\x02\u{598}\u{59a}\x05\x42\x22\x02\u{599}\
	\u{596}\x03\x02\x02\x02\u{599}\u{59a}\x03\x02\x02\x02\u{59a}\x3f\x03\x02\
	\x02\x02\u{59b}\u{5a0}\x05\u{104}\u{83}\x02\u{59c}\u{59d}\x07\x06\x02\x02\
	\u{59d}\u{59f}\x05\u{104}\u{83}\x02\u{59e}\u{59c}\x03\x02\x02\x02\u{59f}\
	\u{5a2}\x03\x02\x02\x02\u{5a0}\u{59e}\x03\x02\x02\x02\u{5a0}\u{5a1}\x03\
	\x02\x02\x02\u{5a1}\u{5a5}\x03\x02\x02\x02\u{5a2}\u{5a0}\x03\x02\x02\x02\
	\u{5a3}\u{5a5}\x07\u{119}\x02\x02\u{5a4}\u{59b}\x03\x02\x02\x02\u{5a4}\u{5a3}\
	\x03\x02\x02\x02\u{5a5}\x41\x03\x02\x02\x02\u{5a6}\u{5ab}\x07\u{11d}\x02\
	\x02\u{5a7}\u{5ab}\x07\u{11f}\x02\x02\u{5a8}\u{5ab}\x05\u{d0}\x69\x02\u{5a9}\
	\u{5ab}\x07\u{119}\x02\x02\u{5aa}\u{5a6}\x03\x02\x02\x02\u{5aa}\u{5a7}\x03\
	\x02\x02\x02\u{5aa}\u{5a8}\x03\x02\x02\x02\u{5aa}\u{5a9}\x03\x02\x02\x02\
	\u{5ab}\x43\x03\x02\x02\x02\u{5ac}\u{5ad}\x07\x03\x02\x02\u{5ad}\u{5b2}\
	\x05\u{c8}\x65\x02\u{5ae}\u{5af}\x07\x05\x02\x02\u{5af}\u{5b1}\x05\u{c8}\
	\x65\x02\u{5b0}\u{5ae}\x03\x02\x02\x02\u{5b1}\u{5b4}\x03\x02\x02\x02\u{5b2}\
	\u{5b0}\x03\x02\x02\x02\u{5b2}\u{5b3}\x03\x02\x02\x02\u{5b3}\u{5b5}\x03\
	\x02\x02\x02\u{5b4}\u{5b2}\x03\x02\x02\x02\u{5b5}\u{5b6}\x07\x04\x02\x02\
	\u{5b6}\x45\x03\x02\x02\x02\u{5b7}\u{5b8}\x07\x03\x02\x02\u{5b8}\u{5bd}\
	\x05\x44\x23\x02\u{5b9}\u{5ba}\x07\x05\x02\x02\u{5ba}\u{5bc}\x05\x44\x23\
	\x02\u{5bb}\u{5b9}\x03\x02\x02\x02\u{5bc}\u{5bf}\x03\x02\x02\x02\u{5bd}\
	\u{5bb}\x03\x02\x02\x02\u{5bd}\u{5be}\x03\x02\x02\x02\u{5be}\u{5c0}\x03\
	\x02\x02\x02\u{5bf}\u{5bd}\x03\x02\x02\x02\u{5c0}\u{5c1}\x07\x04\x02\x02\
	\u{5c1}\x47\x03\x02\x02\x02\u{5c2}\u{5c3}\x07\u{db}\x02\x02\u{5c3}\u{5c4}\
	\x07\x17\x02\x02\u{5c4}\u{5c9}\x05\x4a\x26\x02\u{5c5}\u{5c6}\x07\u{db}\x02\
	\x02\u{5c6}\u{5c7}\x07\x1f\x02\x02\u{5c7}\u{5c9}\x05\x4c\x27\x02\u{5c8}\
	\u{5c2}\x03\x02\x02\x02\u{5c8}\u{5c5}\x03\x02\x02\x02\u{5c9}\x49\x03\x02\
	\x02\x02\u{5ca}\u{5cb}\x07\x76\x02\x02\u{5cb}\u{5cc}\x07\u{119}\x02\x02\
	\u{5cc}\u{5cd}\x07\u{a3}\x02\x02\u{5cd}\u{5d0}\x07\u{119}\x02\x02\u{5ce}\
	\u{5d0}\x05\u{104}\u{83}\x02\u{5cf}\u{5ca}\x03\x02\x02\x02\u{5cf}\u{5ce}\
	\x03\x02\x02\x02\u{5d0}\x4b\x03\x02\x02\x02\u{5d1}\u{5d5}\x07\u{119}\x02\
	\x02\u{5d2}\u{5d3}\x07\u{104}\x02\x02\u{5d3}\u{5d4}\x07\u{cf}\x02\x02\u{5d4}\
	\u{5d6}\x05\x3c\x1f\x02\u{5d5}\u{5d2}\x03\x02\x02\x02\u{5d5}\u{5d6}\x03\
	\x02\x02\x02\u{5d6}\x4d\x03\x02\x02\x02\u{5d7}\u{5d8}\x05\u{104}\u{83}\x02\
	\u{5d8}\u{5d9}\x07\u{119}\x02\x02\u{5d9}\x4f\x03\x02\x02\x02\u{5da}\u{5db}\
	\x05\x26\x14\x02\u{5db}\u{5dc}\x05\x56\x2c\x02\u{5dc}\u{5dd}\x05\x52\x2a\
	\x02\u{5dd}\u{60e}\x03\x02\x02\x02\u{5de}\u{5e0}\x05\x7c\x3f\x02\u{5df}\
	\u{5e1}\x05\x54\x2b\x02\u{5e0}\u{5df}\x03\x02\x02\x02\u{5e1}\u{5e2}\x03\
	\x02\x02\x02\u{5e2}\u{5e0}\x03\x02\x02\x02\u{5e2}\u{5e3}\x03\x02\x02\x02\
	\u{5e3}\u{60e}\x03\x02\x02\x02\u{5e4}\u{5e5}\x07\x43\x02\x02\u{5e5}\u{5e6}\
	\x07\x65\x02\x02\u{5e6}\u{5e7}\x05\u{b0}\x59\x02\u{5e7}\u{5e9}\x05\u{aa}\
	\x56\x02\u{5e8}\u{5ea}\x05\x74\x3b\x02\u{5e9}\u{5e8}\x03\x02\x02\x02\u{5e9}\
	\u{5ea}\x03\x02\x02\x02\u{5ea}\u{60e}\x03\x02\x02\x02\u{5eb}\u{5ec}\x07\
	\u{fa}\x02\x02\u{5ec}\u{5ed}\x05\u{b0}\x59\x02\u{5ed}\u{5ee}\x05\u{aa}\x56\
	\x02\u{5ee}\u{5f0}\x05\x66\x34\x02\u{5ef}\u{5f1}\x05\x74\x3b\x02\u{5f0}\
	\u{5ef}\x03\x02\x02\x02\u{5f0}\u{5f1}\x03\x02\x02\x02\u{5f1}\u{60e}\x03\
	\x02\x02\x02\u{5f2}\u{5f3}\x07\u{91}\x02\x02\u{5f3}\u{5f4}\x07\x7a\x02\x02\
	\u{5f4}\u{5f5}\x05\u{b0}\x59\x02\u{5f5}\u{5f6}\x05\u{aa}\x56\x02\u{5f6}\
	\u{5fc}\x07\u{fd}\x02\x02\u{5f7}\u{5fd}\x05\u{b0}\x59\x02\u{5f8}\u{5f9}\
	\x07\x03\x02\x02\u{5f9}\u{5fa}\x05\x24\x13\x02\u{5fa}\u{5fb}\x07\x04\x02\
	\x02\u{5fb}\u{5fd}\x03\x02\x02\x02\u{5fc}\u{5f7}\x03\x02\x02\x02\u{5fc}\
	\u{5f8}\x03\x02\x02\x02\u{5fd}\u{5fe}\x03\x02\x02\x02\u{5fe}\u{5ff}\x05\
	\u{aa}\x56\x02\u{5ff}\u{600}\x07\u{9b}\x02\x02\u{600}\u{604}\x05\u{c0}\x61\
	\x02\u{601}\u{603}\x05\x68\x35\x02\u{602}\u{601}\x03\x02\x02\x02\u{603}\
	\u{606}\x03\x02\x02\x02\u{604}\u{602}\x03\x02\x02\x02\u{604}\u{605}\x03\
	\x02\x02\x02\u{605}\u{60a}\x03\x02\x02\x02\u{606}\u{604}\x03\x02\x02\x02\
	\u{607}\u{609}\x05\x6a\x36\x02\u{608}\u{607}\x03\x02\x02\x02\u{609}\u{60c}\
	\x03\x02\x02\x02\u{60a}\u{608}\x03\x02\x02\x02\u{60a}\u{60b}\x03\x02\x02\
	\x02\u{60b}\u{60e}\x03\x02\x02\x02\u{60c}\u{60a}\x03\x02\x02\x02\u{60d}\
	\u{5da}\x03\x02\x02\x02\u{60d}\u{5de}\x03\x02\x02\x02\u{60d}\u{5e4}\x03\
	\x02\x02\x02\u{60d}\u{5eb}\x03\x02\x02\x02\u{60d}\u{5f2}\x03\x02\x02\x02\
	\u{60e}\x51\x03\x02\x02\x02\u{60f}\u{610}\x07\u{a0}\x02\x02\u{610}\u{611}\
	\x07\x1f\x02\x02\u{611}\u{616}\x05\x5a\x2e\x02\u{612}\u{613}\x07\x05\x02\
	\x02\u{613}\u{615}\x05\x5a\x2e\x02\u{614}\u{612}\x03\x02\x02\x02\u{615}\
	\u{618}\x03\x02\x02\x02\u{616}\u{614}\x03\x02\x02\x02\u{616}\u{617}\x03\
	\x02\x02\x02\u{617}\u{61a}\x03\x02\x02\x02\u{618}\u{616}\x03\x02\x02\x02\
	\u{619}\u{60f}\x03\x02\x02\x02\u{619}\u{61a}\x03\x02\x02\x02\u{61a}\u{625}\
	\x03\x02\x02\x02\u{61b}\u{61c}\x07\x27\x02\x02\u{61c}\u{61d}\x07\x1f\x02\
	\x02\u{61d}\u{622}\x05\u{be}\x60\x02\u{61e}\u{61f}\x07\x05\x02\x02\u{61f}\
	\u{621}\x05\u{be}\x60\x02\u{620}\u{61e}\x03\x02\x02\x02\u{621}\u{624}\x03\
	\x02\x02\x02\u{622}\u{620}\x03\x02\x02\x02\u{622}\u{623}\x03\x02\x02\x02\
	\u{623}\u{626}\x03\x02\x02\x02\u{624}\u{622}\x03\x02\x02\x02\u{625}\u{61b}\
	\x03\x02\x02\x02\u{625}\u{626}\x03\x02\x02\x02\u{626}\u{631}\x03\x02\x02\
	\x02\u{627}\u{628}\x07\x4b\x02\x02\u{628}\u{629}\x07\x1f\x02\x02\u{629}\
	\u{62e}\x05\u{be}\x60\x02\u{62a}\u{62b}\x07\x05\x02\x02\u{62b}\u{62d}\x05\
	\u{be}\x60\x02\u{62c}\u{62a}\x03\x02\x02\x02\u{62d}\u{630}\x03\x02\x02\x02\
	\u{62e}\u{62c}\x03\x02\x02\x02\u{62e}\u{62f}\x03\x02\x02\x02\u{62f}\u{632}\
	\x03\x02\x02\x02\u{630}\u{62e}\x03\x02\x02\x02\u{631}\u{627}\x03\x02\x02\
	\x02\u{631}\u{632}\x03\x02\x02\x02\u{632}\u{63d}\x03\x02\x02\x02\u{633}\
	\u{634}\x07\u{d7}\x02\x02\u{634}\u{635}\x07\x1f\x02\x02\u{635}\u{63a}\x05\
	\x5a\x2e\x02\u{636}\u{637}\x07\x05\x02\x02\u{637}\u{639}\x05\x5a\x2e\x02\
	\u{638}\u{636}\x03\x02\x02\x02\u{639}\u{63c}\x03\x02\x02\x02\u{63a}\u{638}\
	\x03\x02\x02\x02\u{63a}\u{63b}\x03\x02\x02\x02\u{63b}\u{63e}\x03\x02\x02\
	\x02\u{63c}\u{63a}\x03\x02\x02\x02\u{63d}\u{633}\x03\x02\x02\x02\u{63d}\
	\u{63e}\x03\x02\x02\x02\u{63e}\u{640}\x03\x02\x02\x02\u{63f}\u{641}\x05\
	\u{f0}\x79\x02\u{640}\u{63f}\x03\x02\x02\x02\u{640}\u{641}\x03\x02\x02\x02\
	\u{641}\u{647}\x03\x02\x02\x02\u{642}\u{645}\x07\u{85}\x02\x02\u{643}\u{646}\
	\x07\x0f\x02\x02\u{644}\u{646}\x05\u{be}\x60\x02\u{645}\u{643}\x03\x02\x02\
	\x02\u{645}\u{644}\x03\x02\x02\x02\u{646}\u{648}\x03\x02\x02\x02\u{647}\
	\u{642}\x03\x02\x02\x02\u{647}\u{648}\x03\x02\x02\x02\u{648}\x53\x03\x02\
	\x02\x02\u{649}\u{64a}\x05\x26\x14\x02\u{64a}\u{64b}\x05\x5e\x30\x02\u{64b}\
	\x55\x03\x02\x02\x02\u{64c}\u{64d}\x08\x2c\x01\x02\u{64d}\u{64e}\x05\x58\
	\x2d\x02\u{64e}\u{666}\x03\x02\x02\x02\u{64f}\u{650}\x0c\x05\x02\x02\u{650}\
	\u{651}\x06\x2c\x03\x02\u{651}\u{653}\x09\x10\x02\x02\u{652}\u{654}\x05\
	\u{8a}\x46\x02\u{653}\u{652}\x03\x02\x02\x02\u{653}\u{654}\x03\x02\x02\x02\
	\u{654}\u{655}\x03\x02\x02\x02\u{655}\u{665}\x05\x56\x2c\x06\u{656}\u{657}\
	\x0c\x04\x02\x02\u{657}\u{658}\x06\x2c\x05\x02\u{658}\u{65a}\x07\x78\x02\
	\x02\u{659}\u{65b}\x05\u{8a}\x46\x02\u{65a}\u{659}\x03\x02\x02\x02\u{65a}\
	\u{65b}\x03\x02\x02\x02\u{65b}\u{65c}\x03\x02\x02\x02\u{65c}\u{665}\x05\
	\x56\x2c\x05\u{65d}\u{65e}\x0c\x03\x02\x02\u{65e}\u{65f}\x06\x2c\x07\x02\
	\u{65f}\u{661}\x09\x11\x02\x02\u{660}\u{662}\x05\u{8a}\x46\x02\u{661}\u{660}\
	\x03\x02\x02\x02\u{661}\u{662}\x03\x02\x02\x02\u{662}\u{663}\x03\x02\x02\
	\x02\u{663}\u{665}\x05\x56\x2c\x04\u{664}\u{64f}\x03\x02\x02\x02\u{664}\
	\u{656}\x03\x02\x02\x02\u{664}\u{65d}\x03\x02\x02\x02\u{665}\u{668}\x03\
	\x02\x02\x02\u{666}\u{664}\x03\x02\x02\x02\u{666}\u{667}\x03\x02\x02\x02\
	\u{667}\x57\x03\x02\x02\x02\u{668}\u{666}\x03\x02\x02\x02\u{669}\u{673}\
	\x05\x60\x31\x02\u{66a}\u{673}\x05\x5c\x2f\x02\u{66b}\u{66c}\x07\u{e0}\x02\
	\x02\u{66c}\u{673}\x05\u{b0}\x59\x02\u{66d}\u{673}\x05\u{a6}\x54\x02\u{66e}\
	\u{66f}\x07\x03\x02\x02\u{66f}\u{670}\x05\x24\x13\x02\u{670}\u{671}\x07\
	\x04\x02\x02\u{671}\u{673}\x03\x02\x02\x02\u{672}\u{669}\x03\x02\x02\x02\
	\u{672}\u{66a}\x03\x02\x02\x02\u{672}\u{66b}\x03\x02\x02\x02\u{672}\u{66d}\
	\x03\x02\x02\x02\u{672}\u{66e}\x03\x02\x02\x02\u{673}\x59\x03\x02\x02\x02\
	\u{674}\u{676}\x05\u{be}\x60\x02\u{675}\u{677}\x09\x12\x02\x02\u{676}\u{675}\
	\x03\x02\x02\x02\u{676}\u{677}\x03\x02\x02\x02\u{677}\u{67a}\x03\x02\x02\
	\x02\u{678}\u{679}\x07\u{99}\x02\x02\u{679}\u{67b}\x09\x13\x02\x02\u{67a}\
	\u{678}\x03\x02\x02\x02\u{67a}\u{67b}\x03\x02\x02\x02\u{67b}\x5b\x03\x02\
	\x02\x02\u{67c}\u{67e}\x05\x7c\x3f\x02\u{67d}\u{67f}\x05\x5e\x30\x02\u{67e}\
	\u{67d}\x03\x02\x02\x02\u{67f}\u{680}\x03\x02\x02\x02\u{680}\u{67e}\x03\
	\x02\x02\x02\u{680}\u{681}\x03\x02\x02\x02\u{681}\x5d\x03\x02\x02\x02\u{682}\
	\u{684}\x05\x62\x32\x02\u{683}\u{685}\x05\x74\x3b\x02\u{684}\u{683}\x03\
	\x02\x02\x02\u{684}\u{685}\x03\x02\x02\x02\u{685}\u{686}\x03\x02\x02\x02\
	\u{686}\u{687}\x05\x52\x2a\x02\u{687}\u{69e}\x03\x02\x02\x02\u{688}\u{68c}\
	\x05\x64\x33\x02\u{689}\u{68b}\x05\u{88}\x45\x02\u{68a}\u{689}\x03\x02\x02\
	\x02\u{68b}\u{68e}\x03\x02\x02\x02\u{68c}\u{68a}\x03\x02\x02\x02\u{68c}\
	\u{68d}\x03\x02\x02\x02\u{68d}\u{690}\x03\x02\x02\x02\u{68e}\u{68c}\x03\
	\x02\x02\x02\u{68f}\u{691}\x05\x74\x3b\x02\u{690}\u{68f}\x03\x02\x02\x02\
	\u{690}\u{691}\x03\x02\x02\x02\u{691}\u{693}\x03\x02\x02\x02\u{692}\u{694}\
	\x05\x7e\x40\x02\u{693}\u{692}\x03\x02\x02\x02\u{693}\u{694}\x03\x02\x02\
	\x02\u{694}\u{696}\x03\x02\x02\x02\u{695}\u{697}\x05\x76\x3c\x02\u{696}\
	\u{695}\x03\x02\x02\x02\u{696}\u{697}\x03\x02\x02\x02\u{697}\u{699}\x03\
	\x02\x02\x02\u{698}\u{69a}\x05\u{f0}\x79\x02\u{699}\u{698}\x03\x02\x02\x02\
	\u{699}\u{69a}\x03\x02\x02\x02\u{69a}\u{69b}\x03\x02\x02\x02\u{69b}\u{69c}\
	\x05\x52\x2a\x02\u{69c}\u{69e}\x03\x02\x02\x02\u{69d}\u{682}\x03\x02\x02\
	\x02\u{69d}\u{688}\x03\x02\x02\x02\u{69e}\x5f\x03\x02\x02\x02\u{69f}\u{6a1}\
	\x05\x62\x32\x02\u{6a0}\u{6a2}\x05\x7c\x3f\x02\u{6a1}\u{6a0}\x03\x02\x02\
	\x02\u{6a1}\u{6a2}\x03\x02\x02\x02\u{6a2}\u{6a4}\x03\x02\x02\x02\u{6a3}\
	\u{6a5}\x05\x74\x3b\x02\u{6a4}\u{6a3}\x03\x02\x02\x02\u{6a4}\u{6a5}\x03\
	\x02\x02\x02\u{6a5}\u{6bd}\x03\x02\x02\x02\u{6a6}\u{6a8}\x05\x64\x33\x02\
	\u{6a7}\u{6a9}\x05\x7c\x3f\x02\u{6a8}\u{6a7}\x03\x02\x02\x02\u{6a8}\u{6a9}\
	\x03\x02\x02\x02\u{6a9}\u{6ad}\x03\x02\x02\x02\u{6aa}\u{6ac}\x05\u{88}\x45\
	\x02\u{6ab}\u{6aa}\x03\x02\x02\x02\u{6ac}\u{6af}\x03\x02\x02\x02\u{6ad}\
	\u{6ab}\x03\x02\x02\x02\u{6ad}\u{6ae}\x03\x02\x02\x02\u{6ae}\u{6b1}\x03\
	\x02\x02\x02\u{6af}\u{6ad}\x03\x02\x02\x02\u{6b0}\u{6b2}\x05\x74\x3b\x02\
	\u{6b1}\u{6b0}\x03\x02\x02\x02\u{6b1}\u{6b2}\x03\x02\x02\x02\u{6b2}\u{6b4}\
	\x03\x02\x02\x02\u{6b3}\u{6b5}\x05\x7e\x40\x02\u{6b4}\u{6b3}\x03\x02\x02\
	\x02\u{6b4}\u{6b5}\x03\x02\x02\x02\u{6b5}\u{6b7}\x03\x02\x02\x02\u{6b6}\
	\u{6b8}\x05\x76\x3c\x02\u{6b7}\u{6b6}\x03\x02\x02\x02\u{6b7}\u{6b8}\x03\
	\x02\x02\x02\u{6b8}\u{6ba}\x03\x02\x02\x02\u{6b9}\u{6bb}\x05\u{f0}\x79\x02\
	\u{6ba}\u{6b9}\x03\x02\x02\x02\u{6ba}\u{6bb}\x03\x02\x02\x02\u{6bb}\u{6bd}\
	\x03\x02\x02\x02\u{6bc}\u{69f}\x03\x02\x02\x02\u{6bc}\u{6a6}\x03\x02\x02\
	\x02\u{6bd}\x61\x03\x02\x02\x02\u{6be}\u{6bf}\x07\u{cb}\x02\x02\u{6bf}\u{6c0}\
	\x07\u{ed}\x02\x02\u{6c0}\u{6c1}\x07\x03\x02\x02\u{6c1}\u{6c2}\x05\u{b6}\
	\x5c\x02\u{6c2}\u{6c3}\x07\x04\x02\x02\u{6c3}\u{6c9}\x03\x02\x02\x02\u{6c4}\
	\u{6c5}\x07\u{8f}\x02\x02\u{6c5}\u{6c9}\x05\u{b6}\x5c\x02\u{6c6}\u{6c7}\
	\x07\u{b9}\x02\x02\u{6c7}\u{6c9}\x05\u{b6}\x5c\x02\u{6c8}\u{6be}\x03\x02\
	\x02\x02\u{6c8}\u{6c4}\x03\x02\x02\x02\u{6c8}\u{6c6}\x03\x02\x02\x02\u{6c9}\
	\u{6cb}\x03\x02\x02\x02\u{6ca}\u{6cc}\x05\u{ac}\x57\x02\u{6cb}\u{6ca}\x03\
	\x02\x02\x02\u{6cb}\u{6cc}\x03\x02\x02\x02\u{6cc}\u{6cf}\x03\x02\x02\x02\
	\u{6cd}\u{6ce}\x07\u{b7}\x02\x02\u{6ce}\u{6d0}\x07\u{119}\x02\x02\u{6cf}\
	\u{6cd}\x03\x02\x02\x02\u{6cf}\u{6d0}\x03\x02\x02\x02\u{6d0}\u{6d1}\x03\
	\x02\x02\x02\u{6d1}\u{6d2}\x07\u{fd}\x02\x02\u{6d2}\u{6df}\x07\u{119}\x02\
	\x02\u{6d3}\u{6dd}\x07\x17\x02\x02\u{6d4}\u{6de}\x05\u{9a}\x4e\x02\u{6d5}\
	\u{6de}\x05\u{e6}\x74\x02\u{6d6}\u{6d9}\x07\x03\x02\x02\u{6d7}\u{6da}\x05\
	\u{9a}\x4e\x02\u{6d8}\u{6da}\x05\u{e6}\x74\x02\u{6d9}\u{6d7}\x03\x02\x02\
	\x02\u{6d9}\u{6d8}\x03\x02\x02\x02\u{6da}\u{6db}\x03\x02\x02\x02\u{6db}\
	\u{6dc}\x07\x04\x02\x02\u{6dc}\u{6de}\x03\x02\x02\x02\u{6dd}\u{6d4}\x03\
	\x02\x02\x02\u{6dd}\u{6d5}\x03\x02\x02\x02\u{6dd}\u{6d6}\x03\x02\x02\x02\
	\u{6de}\u{6e0}\x03\x02\x02\x02\u{6df}\u{6d3}\x03\x02\x02\x02\u{6df}\u{6e0}\
	\x03\x02\x02\x02\u{6e0}\u{6e2}\x03\x02\x02\x02\u{6e1}\u{6e3}\x05\u{ac}\x57\
	\x02\u{6e2}\u{6e1}\x03\x02\x02\x02\u{6e2}\u{6e3}\x03\x02\x02\x02\u{6e3}\
	\u{6e6}\x03\x02\x02\x02\u{6e4}\u{6e5}\x07\u{b6}\x02\x02\u{6e5}\u{6e7}\x07\
	\u{119}\x02\x02\u{6e6}\u{6e4}\x03\x02\x02\x02\u{6e6}\u{6e7}\x03\x02\x02\
	\x02\u{6e7}\x63\x03\x02\x02\x02\u{6e8}\u{6ec}\x07\u{cb}\x02\x02\u{6e9}\u{6eb}\
	\x05\x78\x3d\x02\u{6ea}\u{6e9}\x03\x02\x02\x02\u{6eb}\u{6ee}\x03\x02\x02\
	\x02\u{6ec}\u{6ea}\x03\x02\x02\x02\u{6ec}\u{6ed}\x03\x02\x02\x02\u{6ed}\
	\u{6f0}\x03\x02\x02\x02\u{6ee}\u{6ec}\x03\x02\x02\x02\u{6ef}\u{6f1}\x05\
	\u{8a}\x46\x02\u{6f0}\u{6ef}\x03\x02\x02\x02\u{6f0}\u{6f1}\x03\x02\x02\x02\
	\u{6f1}\u{6f2}\x03\x02\x02\x02\u{6f2}\u{6f3}\x05\u{b6}\x5c\x02\u{6f3}\x65\
	\x03\x02\x02\x02\u{6f4}\u{6f5}\x07\u{d1}\x02\x02\u{6f5}\u{6f6}\x05\x70\x39\
	\x02\u{6f6}\x67\x03\x02\x02\x02\u{6f7}\u{6f8}\x07\u{101}\x02\x02\u{6f8}\
	\u{6fb}\x07\u{90}\x02\x02\u{6f9}\u{6fa}\x07\x12\x02\x02\u{6fa}\u{6fc}\x05\
	\u{c0}\x61\x02\u{6fb}\u{6f9}\x03\x02\x02\x02\u{6fb}\u{6fc}\x03\x02\x02\x02\
	\u{6fc}\u{6fd}\x03\x02\x02\x02\u{6fd}\u{6fe}\x07\u{e6}\x02\x02\u{6fe}\u{6ff}\
	\x05\x6c\x37\x02\u{6ff}\x69\x03\x02\x02\x02\u{700}\u{701}\x07\u{101}\x02\
	\x02\u{701}\u{702}\x07\u{97}\x02\x02\u{702}\u{705}\x07\u{90}\x02\x02\u{703}\
	\u{704}\x07\x12\x02\x02\u{704}\u{706}\x05\u{c0}\x61\x02\u{705}\u{703}\x03\
	\x02\x02\x02\u{705}\u{706}\x03\x02\x02\x02\u{706}\u{707}\x03\x02\x02\x02\
	\u{707}\u{708}\x07\u{e6}\x02\x02\u{708}\u{709}\x05\x6e\x38\x02\u{709}\x6b\
	\x03\x02\x02\x02\u{70a}\u{712}\x07\x43\x02\x02\u{70b}\u{70c}\x07\u{fa}\x02\
	\x02\u{70c}\u{70d}\x07\u{d1}\x02\x02\u{70d}\u{712}\x07\u{110}\x02\x02\u{70e}\
	\u{70f}\x07\u{fa}\x02\x02\u{70f}\u{710}\x07\u{d1}\x02\x02\u{710}\u{712}\
	\x05\x70\x39\x02\u{711}\u{70a}\x03\x02\x02\x02\u{711}\u{70b}\x03\x02\x02\
	\x02\u{711}\u{70e}\x03\x02\x02\x02\u{712}\x6d\x03\x02\x02\x02\u{713}\u{714}\
	\x07\x77\x02\x02\u{714}\u{726}\x07\u{110}\x02\x02\u{715}\u{716}\x07\x77\
	\x02\x02\u{716}\u{717}\x07\x03\x02\x02\u{717}\u{718}\x05\u{ae}\x58\x02\u{718}\
	\u{719}\x07\x04\x02\x02\u{719}\u{71a}\x07\u{fe}\x02\x02\u{71a}\u{71b}\x07\
	\x03\x02\x02\u{71b}\u{720}\x05\u{be}\x60\x02\u{71c}\u{71d}\x07\x05\x02\x02\
	\u{71d}\u{71f}\x05\u{be}\x60\x02\u{71e}\u{71c}\x03\x02\x02\x02\u{71f}\u{722}\
	\x03\x02\x02\x02\u{720}\u{71e}\x03\x02\x02\x02\u{720}\u{721}\x03\x02\x02\
	\x02\u{721}\u{723}\x03\x02\x02\x02\u{722}\u{720}\x03\x02\x02\x02\u{723}\
	\u{724}\x07\x04\x02\x02\u{724}\u{726}\x03\x02\x02\x02\u{725}\u{713}\x03\
	\x02\x02\x02\u{725}\u{715}\x03\x02\x02\x02\u{726}\x6f\x03\x02\x02\x02\u{727}\
	\u{72c}\x05\x72\x3a\x02\u{728}\u{729}\x07\x05\x02\x02\u{729}\u{72b}\x05\
	\x72\x3a\x02\u{72a}\u{728}\x03\x02\x02\x02\u{72b}\u{72e}\x03\x02\x02\x02\
	\u{72c}\u{72a}\x03\x02\x02\x02\u{72c}\u{72d}\x03\x02\x02\x02\u{72d}\x71\
	\x03\x02\x02\x02\u{72e}\u{72c}\x03\x02\x02\x02\u{72f}\u{730}\x05\u{b0}\x59\
	\x02\u{730}\u{731}\x07\u{106}\x02\x02\u{731}\u{732}\x05\u{be}\x60\x02\u{732}\
	\x73\x03\x02\x02\x02\u{733}\u{734}\x07\u{102}\x02\x02\u{734}\u{735}\x05\
	\u{c0}\x61\x02\u{735}\x75\x03\x02\x02\x02\u{736}\u{737}\x07\x6d\x02\x02\
	\u{737}\u{738}\x05\u{c0}\x61\x02\u{738}\x77\x03\x02\x02\x02\u{739}\u{73a}\
	\x07\x07\x02\x02\u{73a}\u{741}\x05\x7a\x3e\x02\u{73b}\u{73d}\x07\x05\x02\
	\x02\u{73c}\u{73b}\x03\x02\x02\x02\u{73c}\u{73d}\x03\x02\x02\x02\u{73d}\
	\u{73e}\x03\x02\x02\x02\u{73e}\u{740}\x05\x7a\x3e\x02\u{73f}\u{73c}\x03\
	\x02\x02\x02\u{740}\u{743}\x03\x02\x02\x02\u{741}\u{73f}\x03\x02\x02\x02\
	\u{741}\u{742}\x03\x02\x02\x02\u{742}\u{744}\x03\x02\x02\x02\u{743}\u{741}\
	\x03\x02\x02\x02\u{744}\u{745}\x07\x08\x02\x02\u{745}\x79\x03\x02\x02\x02\
	\u{746}\u{754}\x05\u{104}\u{83}\x02\u{747}\u{748}\x05\u{104}\u{83}\x02\u{748}\
	\u{749}\x07\x03\x02\x02\u{749}\u{74e}\x05\u{c6}\x64\x02\u{74a}\u{74b}\x07\
	\x05\x02\x02\u{74b}\u{74d}\x05\u{c6}\x64\x02\u{74c}\u{74a}\x03\x02\x02\x02\
	\u{74d}\u{750}\x03\x02\x02\x02\u{74e}\u{74c}\x03\x02\x02\x02\u{74e}\u{74f}\
	\x03\x02\x02\x02\u{74f}\u{751}\x03\x02\x02\x02\u{750}\u{74e}\x03\x02\x02\
	\x02\u{751}\u{752}\x07\x04\x02\x02\u{752}\u{754}\x03\x02\x02\x02\u{753}\
	\u{746}\x03\x02\x02\x02\u{753}\u{747}\x03\x02\x02\x02\u{754}\x7b\x03\x02\
	\x02\x02\u{755}\u{756}\x07\x65\x02\x02\u{756}\u{75b}\x05\u{8c}\x47\x02\u{757}\
	\u{758}\x07\x05\x02\x02\u{758}\u{75a}\x05\u{8c}\x47\x02\u{759}\u{757}\x03\
	\x02\x02\x02\u{75a}\u{75d}\x03\x02\x02\x02\u{75b}\u{759}\x03\x02\x02\x02\
	\u{75b}\u{75c}\x03\x02\x02\x02\u{75c}\u{761}\x03\x02\x02\x02\u{75d}\u{75b}\
	\x03\x02\x02\x02\u{75e}\u{760}\x05\u{88}\x45\x02\u{75f}\u{75e}\x03\x02\x02\
	\x02\u{760}\u{763}\x03\x02\x02\x02\u{761}\u{75f}\x03\x02\x02\x02\u{761}\
	\u{762}\x03\x02\x02\x02\u{762}\u{765}\x03\x02\x02\x02\u{763}\u{761}\x03\
	\x02\x02\x02\u{764}\u{766}\x05\u{82}\x42\x02\u{765}\u{764}\x03\x02\x02\x02\
	\u{765}\u{766}\x03\x02\x02\x02\u{766}\x7d\x03\x02\x02\x02\u{767}\u{768}\
	\x07\x6b\x02\x02\u{768}\u{769}\x07\x1f\x02\x02\u{769}\u{76e}\x05\u{be}\x60\
	\x02\u{76a}\u{76b}\x07\x05\x02\x02\u{76b}\u{76d}\x05\u{be}\x60\x02\u{76c}\
	\u{76a}\x03\x02\x02\x02\u{76d}\u{770}\x03\x02\x02\x02\u{76e}\u{76c}\x03\
	\x02\x02\x02\u{76e}\u{76f}\x03\x02\x02\x02\u{76f}\u{782}\x03\x02\x02\x02\
	\u{770}\u{76e}\x03\x02\x02\x02\u{771}\u{772}\x07\u{104}\x02\x02\u{772}\u{783}\
	\x07\u{c7}\x02\x02\u{773}\u{774}\x07\u{104}\x02\x02\u{774}\u{783}\x07\x38\
	\x02\x02\u{775}\u{776}\x07\x6c\x02\x02\u{776}\u{777}\x07\u{d3}\x02\x02\u{777}\
	\u{778}\x07\x03\x02\x02\u{778}\u{77d}\x05\u{80}\x41\x02\u{779}\u{77a}\x07\
	\x05\x02\x02\u{77a}\u{77c}\x05\u{80}\x41\x02\u{77b}\u{779}\x03\x02\x02\x02\
	\u{77c}\u{77f}\x03\x02\x02\x02\u{77d}\u{77b}\x03\x02\x02\x02\u{77d}\u{77e}\
	\x03\x02\x02\x02\u{77e}\u{780}\x03\x02\x02\x02\u{77f}\u{77d}\x03\x02\x02\
	\x02\u{780}\u{781}\x07\x04\x02\x02\u{781}\u{783}\x03\x02\x02\x02\u{782}\
	\u{771}\x03\x02\x02\x02\u{782}\u{773}\x03\x02\x02\x02\u{782}\u{775}\x03\
	\x02\x02\x02\u{782}\u{783}\x03\x02\x02\x02\u{783}\u{794}\x03\x02\x02\x02\
	\u{784}\u{785}\x07\x6b\x02\x02\u{785}\u{786}\x07\x1f\x02\x02\u{786}\u{787}\
	\x07\x6c\x02\x02\u{787}\u{788}\x07\u{d3}\x02\x02\u{788}\u{789}\x07\x03\x02\
	\x02\u{789}\u{78e}\x05\u{80}\x41\x02\u{78a}\u{78b}\x07\x05\x02\x02\u{78b}\
	\u{78d}\x05\u{80}\x41\x02\u{78c}\u{78a}\x03\x02\x02\x02\u{78d}\u{790}\x03\
	\x02\x02\x02\u{78e}\u{78c}\x03\x02\x02\x02\u{78e}\u{78f}\x03\x02\x02\x02\
	\u{78f}\u{791}\x03\x02\x02\x02\u{790}\u{78e}\x03\x02\x02\x02\u{791}\u{792}\
	\x07\x04\x02\x02\u{792}\u{794}\x03\x02\x02\x02\u{793}\u{767}\x03\x02\x02\
	\x02\u{793}\u{784}\x03\x02\x02\x02\u{794}\x7f\x03\x02\x02\x02\u{795}\u{79e}\
	\x07\x03\x02\x02\u{796}\u{79b}\x05\u{be}\x60\x02\u{797}\u{798}\x07\x05\x02\
	\x02\u{798}\u{79a}\x05\u{be}\x60\x02\u{799}\u{797}\x03\x02\x02\x02\u{79a}\
	\u{79d}\x03\x02\x02\x02\u{79b}\u{799}\x03\x02\x02\x02\u{79b}\u{79c}\x03\
	\x02\x02\x02\u{79c}\u{79f}\x03\x02\x02\x02\u{79d}\u{79b}\x03\x02\x02\x02\
	\u{79e}\u{796}\x03\x02\x02\x02\u{79e}\u{79f}\x03\x02\x02\x02\u{79f}\u{7a0}\
	\x03\x02\x02\x02\u{7a0}\u{7a3}\x07\x04\x02\x02\u{7a1}\u{7a3}\x05\u{be}\x60\
	\x02\u{7a2}\u{795}\x03\x02\x02\x02\u{7a2}\u{7a1}\x03\x02\x02\x02\u{7a3}\
	\u{81}\x03\x02\x02\x02\u{7a4}\u{7a5}\x07\u{ac}\x02\x02\u{7a5}\u{7a6}\x07\
	\x03\x02\x02\u{7a6}\u{7a7}\x05\u{b6}\x5c\x02\u{7a7}\u{7a8}\x07\x61\x02\x02\
	\u{7a8}\u{7a9}\x05\u{84}\x43\x02\u{7a9}\u{7aa}\x07\x71\x02\x02\u{7aa}\u{7ab}\
	\x07\x03\x02\x02\u{7ab}\u{7b0}\x05\u{86}\x44\x02\u{7ac}\u{7ad}\x07\x05\x02\
	\x02\u{7ad}\u{7af}\x05\u{86}\x44\x02\u{7ae}\u{7ac}\x03\x02\x02\x02\u{7af}\
	\u{7b2}\x03\x02\x02\x02\u{7b0}\u{7ae}\x03\x02\x02\x02\u{7b0}\u{7b1}\x03\
	\x02\x02\x02\u{7b1}\u{7b3}\x03\x02\x02\x02\u{7b2}\u{7b0}\x03\x02\x02\x02\
	\u{7b3}\u{7b4}\x07\x04\x02\x02\u{7b4}\u{7b5}\x07\x04\x02\x02\u{7b5}\u{83}\
	\x03\x02\x02\x02\u{7b6}\u{7c3}\x05\u{104}\u{83}\x02\u{7b7}\u{7b8}\x07\x03\
	\x02\x02\u{7b8}\u{7bd}\x05\u{104}\u{83}\x02\u{7b9}\u{7ba}\x07\x05\x02\x02\
	\u{7ba}\u{7bc}\x05\u{104}\u{83}\x02\u{7bb}\u{7b9}\x03\x02\x02\x02\u{7bc}\
	\u{7bf}\x03\x02\x02\x02\u{7bd}\u{7bb}\x03\x02\x02\x02\u{7bd}\u{7be}\x03\
	\x02\x02\x02\u{7be}\u{7c0}\x03\x02\x02\x02\u{7bf}\u{7bd}\x03\x02\x02\x02\
	\u{7c0}\u{7c1}\x07\x04\x02\x02\u{7c1}\u{7c3}\x03\x02\x02\x02\u{7c2}\u{7b6}\
	\x03\x02\x02\x02\u{7c2}\u{7b7}\x03\x02\x02\x02\u{7c3}\u{85}\x03\x02\x02\
	\x02\u{7c4}\u{7c9}\x05\u{be}\x60\x02\u{7c5}\u{7c7}\x07\x17\x02\x02\u{7c6}\
	\u{7c5}\x03\x02\x02\x02\u{7c6}\u{7c7}\x03\x02\x02\x02\u{7c7}\u{7c8}\x03\
	\x02\x02\x02\u{7c8}\u{7ca}\x05\u{104}\u{83}\x02\u{7c9}\u{7c6}\x03\x02\x02\
	\x02\u{7c9}\u{7ca}\x03\x02\x02\x02\u{7ca}\u{87}\x03\x02\x02\x02\u{7cb}\u{7cc}\
	\x07\u{80}\x02\x02\u{7cc}\u{7ce}\x07\u{ff}\x02\x02\u{7cd}\u{7cf}\x07\u{a2}\
	\x02\x02\u{7ce}\u{7cd}\x03\x02\x02\x02\u{7ce}\u{7cf}\x03\x02\x02\x02\u{7cf}\
	\u{7d0}\x03\x02\x02\x02\u{7d0}\u{7d1}\x05\u{fe}\u{80}\x02\u{7d1}\u{7da}\
	\x07\x03\x02\x02\u{7d2}\u{7d7}\x05\u{be}\x60\x02\u{7d3}\u{7d4}\x07\x05\x02\
	\x02\u{7d4}\u{7d6}\x05\u{be}\x60\x02\u{7d5}\u{7d3}\x03\x02\x02\x02\u{7d6}\
	\u{7d9}\x03\x02\x02\x02\u{7d7}\u{7d5}\x03\x02\x02\x02\u{7d7}\u{7d8}\x03\
	\x02\x02\x02\u{7d8}\u{7db}\x03\x02\x02\x02\u{7d9}\u{7d7}\x03\x02\x02\x02\
	\u{7da}\u{7d2}\x03\x02\x02\x02\u{7da}\u{7db}\x03\x02\x02\x02\u{7db}\u{7dc}\
	\x03\x02\x02\x02\u{7dc}\u{7dd}\x07\x04\x02\x02\u{7dd}\u{7e9}\x05\u{104}\
	\u{83}\x02\u{7de}\u{7e0}\x07\x17\x02\x02\u{7df}\u{7de}\x03\x02\x02\x02\u{7df}\
	\u{7e0}\x03\x02\x02\x02\u{7e0}\u{7e1}\x03\x02\x02\x02\u{7e1}\u{7e6}\x05\
	\u{104}\u{83}\x02\u{7e2}\u{7e3}\x07\x05\x02\x02\u{7e3}\u{7e5}\x05\u{104}\
	\u{83}\x02\u{7e4}\u{7e2}\x03\x02\x02\x02\u{7e5}\u{7e8}\x03\x02\x02\x02\u{7e6}\
	\u{7e4}\x03\x02\x02\x02\u{7e6}\u{7e7}\x03\x02\x02\x02\u{7e7}\u{7ea}\x03\
	\x02\x02\x02\u{7e8}\u{7e6}\x03\x02\x02\x02\u{7e9}\u{7df}\x03\x02\x02\x02\
	\u{7e9}\u{7ea}\x03\x02\x02\x02\u{7ea}\u{89}\x03\x02\x02\x02\u{7eb}\u{7ec}\
	\x09\x14\x02\x02\u{7ec}\u{8b}\x03\x02\x02\x02\u{7ed}\u{7f1}\x05\u{a4}\x53\
	\x02\u{7ee}\u{7f0}\x05\u{8e}\x48\x02\u{7ef}\u{7ee}\x03\x02\x02\x02\u{7f0}\
	\u{7f3}\x03\x02\x02\x02\u{7f1}\u{7ef}\x03\x02\x02\x02\u{7f1}\u{7f2}\x03\
	\x02\x02\x02\u{7f2}\u{8d}\x03\x02\x02\x02\u{7f3}\u{7f1}\x03\x02\x02\x02\
	\u{7f4}\u{7f5}\x05\u{90}\x49\x02\u{7f5}\u{7f6}\x07\x7d\x02\x02\u{7f6}\u{7f8}\
	\x05\u{a4}\x53\x02\u{7f7}\u{7f9}\x05\u{92}\x4a\x02\u{7f8}\u{7f7}\x03\x02\
	\x02\x02\u{7f8}\u{7f9}\x03\x02\x02\x02\u{7f9}\u{800}\x03\x02\x02\x02\u{7fa}\
	\u{7fb}\x07\u{95}\x02\x02\u{7fb}\u{7fc}\x05\u{90}\x49\x02\u{7fc}\u{7fd}\
	\x07\x7d\x02\x02\u{7fd}\u{7fe}\x05\u{a4}\x53\x02\u{7fe}\u{800}\x03\x02\x02\
	\x02\u{7ff}\u{7f4}\x03\x02\x02\x02\u{7ff}\u{7fa}\x03\x02\x02\x02\u{800}\
	\u{8f}\x03\x02\x02\x02\u{801}\u{803}\x07\x74\x02\x02\u{802}\u{801}\x03\x02\
	\x02\x02\u{802}\u{803}\x03\x02\x02\x02\u{803}\u{81a}\x03\x02\x02\x02\u{804}\
	\u{81a}\x07\x37\x02\x02\u{805}\u{807}\x07\u{83}\x02\x02\u{806}\u{808}\x07\
	\u{a2}\x02\x02\u{807}\u{806}\x03\x02\x02\x02\u{807}\u{808}\x03\x02\x02\x02\
	\u{808}\u{81a}\x03\x02\x02\x02\u{809}\u{80b}\x07\u{83}\x02\x02\u{80a}\u{809}\
	\x03\x02\x02\x02\u{80a}\u{80b}\x03\x02\x02\x02\u{80b}\u{80c}\x03\x02\x02\
	\x02\u{80c}\u{81a}\x07\u{cc}\x02\x02\u{80d}\u{80f}\x07\u{c2}\x02\x02\u{80e}\
	\u{810}\x07\u{a2}\x02\x02\u{80f}\u{80e}\x03\x02\x02\x02\u{80f}\u{810}\x03\
	\x02\x02\x02\u{810}\u{81a}\x03\x02\x02\x02\u{811}\u{813}\x07\x66\x02\x02\
	\u{812}\u{814}\x07\u{a2}\x02\x02\u{813}\u{812}\x03\x02\x02\x02\u{813}\u{814}\
	\x03\x02\x02\x02\u{814}\u{81a}\x03\x02\x02\x02\u{815}\u{817}\x07\u{83}\x02\
	\x02\u{816}\u{815}\x03\x02\x02\x02\u{816}\u{817}\x03\x02\x02\x02\u{817}\
	\u{818}\x03\x02\x02\x02\u{818}\u{81a}\x07\x13\x02\x02\u{819}\u{802}\x03\
	\x02\x02\x02\u{819}\u{804}\x03\x02\x02\x02\u{819}\u{805}\x03\x02\x02\x02\
	\u{819}\u{80a}\x03\x02\x02\x02\u{819}\u{80d}\x03\x02\x02\x02\u{819}\u{811}\
	\x03\x02\x02\x02\u{819}\u{816}\x03\x02\x02\x02\u{81a}\u{91}\x03\x02\x02\
	\x02\u{81b}\u{81c}\x07\u{9b}\x02\x02\u{81c}\u{820}\x05\u{c0}\x61\x02\u{81d}\
	\u{81e}\x07\u{fd}\x02\x02\u{81e}\u{820}\x05\u{98}\x4d\x02\u{81f}\u{81b}\
	\x03\x02\x02\x02\u{81f}\u{81d}\x03\x02\x02\x02\u{820}\u{93}\x03\x02\x02\
	\x02\u{821}\u{822}\x07\u{e2}\x02\x02\u{822}\u{824}\x07\x03\x02\x02\u{823}\
	\u{825}\x05\u{96}\x4c\x02\u{824}\u{823}\x03\x02\x02\x02\u{824}\u{825}\x03\
	\x02\x02\x02\u{825}\u{826}\x03\x02\x02\x02\u{826}\u{827}\x07\x04\x02\x02\
	\u{827}\u{95}\x03\x02\x02\x02\u{828}\u{82a}\x07\u{10f}\x02\x02\u{829}\u{828}\
	\x03\x02\x02\x02\u{829}\u{82a}\x03\x02\x02\x02\u{82a}\u{82b}\x03\x02\x02\
	\x02\u{82b}\u{82c}\x09\x15\x02\x02\u{82c}\u{841}\x07\u{ab}\x02\x02\u{82d}\
	\u{82e}\x05\u{be}\x60\x02\u{82e}\u{82f}\x07\u{c9}\x02\x02\u{82f}\u{841}\
	\x03\x02\x02\x02\u{830}\u{831}\x07\x1d\x02\x02\u{831}\u{832}\x07\u{11d}\
	\x02\x02\u{832}\u{833}\x07\u{a1}\x02\x02\u{833}\u{834}\x07\u{9a}\x02\x02\
	\u{834}\u{83d}\x07\u{11d}\x02\x02\u{835}\u{83b}\x07\u{9b}\x02\x02\u{836}\
	\u{83c}\x05\u{104}\u{83}\x02\u{837}\u{838}\x05\u{fe}\u{80}\x02\u{838}\u{839}\
	\x07\x03\x02\x02\u{839}\u{83a}\x07\x04\x02\x02\u{83a}\u{83c}\x03\x02\x02\
	\x02\u{83b}\u{836}\x03\x02\x02\x02\u{83b}\u{837}\x03\x02\x02\x02\u{83c}\
	\u{83e}\x03\x02\x02\x02\u{83d}\u{835}\x03\x02\x02\x02\u{83d}\u{83e}\x03\
	\x02\x02\x02\u{83e}\u{841}\x03\x02\x02\x02\u{83f}\u{841}\x05\u{be}\x60\x02\
	\u{840}\u{829}\x03\x02\x02\x02\u{840}\u{82d}\x03\x02\x02\x02\u{840}\u{830}\
	\x03\x02\x02\x02\u{840}\u{83f}\x03\x02\x02\x02\u{841}\u{97}\x03\x02\x02\
	\x02\u{842}\u{843}\x07\x03\x02\x02\u{843}\u{844}\x05\u{9a}\x4e\x02\u{844}\
	\u{845}\x07\x04\x02\x02\u{845}\u{99}\x03\x02\x02\x02\u{846}\u{84b}\x05\u{100}\
	\u{81}\x02\u{847}\u{848}\x07\x05\x02\x02\u{848}\u{84a}\x05\u{100}\u{81}\
	\x02\u{849}\u{847}\x03\x02\x02\x02\u{84a}\u{84d}\x03\x02\x02\x02\u{84b}\
	\u{849}\x03\x02\x02\x02\u{84b}\u{84c}\x03\x02\x02\x02\u{84c}\u{9b}\x03\x02\
	\x02\x02\u{84d}\u{84b}\x03\x02\x02\x02\u{84e}\u{84f}\x07\x03\x02\x02\u{84f}\
	\u{854}\x05\u{9e}\x50\x02\u{850}\u{851}\x07\x05\x02\x02\u{851}\u{853}\x05\
	\u{9e}\x50\x02\u{852}\u{850}\x03\x02\x02\x02\u{853}\u{856}\x03\x02\x02\x02\
	\u{854}\u{852}\x03\x02\x02\x02\u{854}\u{855}\x03\x02\x02\x02\u{855}\u{857}\
	\x03\x02\x02\x02\u{856}\u{854}\x03\x02\x02\x02\u{857}\u{858}\x07\x04\x02\
	\x02\u{858}\u{9d}\x03\x02\x02\x02\u{859}\u{85b}\x05\u{100}\u{81}\x02\u{85a}\
	\u{85c}\x09\x12\x02\x02\u{85b}\u{85a}\x03\x02\x02\x02\u{85b}\u{85c}\x03\
	\x02\x02\x02\u{85c}\u{9f}\x03\x02\x02\x02\u{85d}\u{85e}\x07\x03\x02\x02\
	\u{85e}\u{863}\x05\u{a2}\x52\x02\u{85f}\u{860}\x07\x05\x02\x02\u{860}\u{862}\
	\x05\u{a2}\x52\x02\u{861}\u{85f}\x03\x02\x02\x02\u{862}\u{865}\x03\x02\x02\
	\x02\u{863}\u{861}\x03\x02\x02\x02\u{863}\u{864}\x03\x02\x02\x02\u{864}\
	\u{866}\x03\x02\x02\x02\u{865}\u{863}\x03\x02\x02\x02\u{866}\u{867}\x07\
	\x04\x02\x02\u{867}\u{a1}\x03\x02\x02\x02\u{868}\u{86a}\x05\u{104}\u{83}\
	\x02\u{869}\u{86b}\x05\x22\x12\x02\u{86a}\u{869}\x03\x02\x02\x02\u{86a}\
	\u{86b}\x03\x02\x02\x02\u{86b}\u{a3}\x03\x02\x02\x02\u{86c}\u{86e}\x05\u{b0}\
	\x59\x02\u{86d}\u{86f}\x05\u{94}\x4b\x02\u{86e}\u{86d}\x03\x02\x02\x02\u{86e}\
	\u{86f}\x03\x02\x02\x02\u{86f}\u{870}\x03\x02\x02\x02\u{870}\u{871}\x05\
	\u{aa}\x56\x02\u{871}\u{885}\x03\x02\x02\x02\u{872}\u{873}\x07\x03\x02\x02\
	\u{873}\u{874}\x05\x24\x13\x02\u{874}\u{876}\x07\x04\x02\x02\u{875}\u{877}\
	\x05\u{94}\x4b\x02\u{876}\u{875}\x03\x02\x02\x02\u{876}\u{877}\x03\x02\x02\
	\x02\u{877}\u{878}\x03\x02\x02\x02\u{878}\u{879}\x05\u{aa}\x56\x02\u{879}\
	\u{885}\x03\x02\x02\x02\u{87a}\u{87b}\x07\x03\x02\x02\u{87b}\u{87c}\x05\
	\u{8c}\x47\x02\u{87c}\u{87e}\x07\x04\x02\x02\u{87d}\u{87f}\x05\u{94}\x4b\
	\x02\u{87e}\u{87d}\x03\x02\x02\x02\u{87e}\u{87f}\x03\x02\x02\x02\u{87f}\
	\u{880}\x03\x02\x02\x02\u{880}\u{881}\x05\u{aa}\x56\x02\u{881}\u{885}\x03\
	\x02\x02\x02\u{882}\u{885}\x05\u{a6}\x54\x02\u{883}\u{885}\x05\u{a8}\x55\
	\x02\u{884}\u{86c}\x03\x02\x02\x02\u{884}\u{872}\x03\x02\x02\x02\u{884}\
	\u{87a}\x03\x02\x02\x02\u{884}\u{882}\x03\x02\x02\x02\u{884}\u{883}\x03\
	\x02\x02\x02\u{885}\u{a5}\x03\x02\x02\x02\u{886}\u{887}\x07\u{fe}\x02\x02\
	\u{887}\u{88c}\x05\u{be}\x60\x02\u{888}\u{889}\x07\x05\x02\x02\u{889}\u{88b}\
	\x05\u{be}\x60\x02\u{88a}\u{888}\x03\x02\x02\x02\u{88b}\u{88e}\x03\x02\x02\
	\x02\u{88c}\u{88a}\x03\x02\x02\x02\u{88c}\u{88d}\x03\x02\x02\x02\u{88d}\
	\u{88f}\x03\x02\x02\x02\u{88e}\u{88c}\x03\x02\x02\x02\u{88f}\u{890}\x05\
	\u{aa}\x56\x02\u{890}\u{a7}\x03\x02\x02\x02\u{891}\u{892}\x05\u{100}\u{81}\
	\x02\u{892}\u{89b}\x07\x03\x02\x02\u{893}\u{898}\x05\u{be}\x60\x02\u{894}\
	\u{895}\x07\x05\x02\x02\u{895}\u{897}\x05\u{be}\x60\x02\u{896}\u{894}\x03\
	\x02\x02\x02\u{897}\u{89a}\x03\x02\x02\x02\u{898}\u{896}\x03\x02\x02\x02\
	\u{898}\u{899}\x03\x02\x02\x02\u{899}\u{89c}\x03\x02\x02\x02\u{89a}\u{898}\
	\x03\x02\x02\x02\u{89b}\u{893}\x03\x02\x02\x02\u{89b}\u{89c}\x03\x02\x02\
	\x02\u{89c}\u{89d}\x03\x02\x02\x02\u{89d}\u{89e}\x07\x04\x02\x02\u{89e}\
	\u{89f}\x05\u{aa}\x56\x02\u{89f}\u{a9}\x03\x02\x02\x02\u{8a0}\u{8a2}\x07\
	\x17\x02\x02\u{8a1}\u{8a0}\x03\x02\x02\x02\u{8a1}\u{8a2}\x03\x02\x02\x02\
	\u{8a2}\u{8a3}\x03\x02\x02\x02\u{8a3}\u{8a5}\x05\u{106}\u{84}\x02\u{8a4}\
	\u{8a6}\x05\u{98}\x4d\x02\u{8a5}\u{8a4}\x03\x02\x02\x02\u{8a5}\u{8a6}\x03\
	\x02\x02\x02\u{8a6}\u{8a8}\x03\x02\x02\x02\u{8a7}\u{8a1}\x03\x02\x02\x02\
	\u{8a7}\u{8a8}\x03\x02\x02\x02\u{8a8}\u{ab}\x03\x02\x02\x02\u{8a9}\u{8aa}\
	\x07\u{c8}\x02\x02\u{8aa}\u{8ab}\x07\x63\x02\x02\u{8ab}\u{8ac}\x07\u{ce}\
	\x02\x02\u{8ac}\u{8b0}\x07\u{119}\x02\x02\u{8ad}\u{8ae}\x07\u{104}\x02\x02\
	\u{8ae}\u{8af}\x07\u{cf}\x02\x02\u{8af}\u{8b1}\x05\x3c\x1f\x02\u{8b0}\u{8ad}\
	\x03\x02\x02\x02\u{8b0}\u{8b1}\x03\x02\x02\x02\u{8b1}\u{8db}\x03\x02\x02\
	\x02\u{8b2}\u{8b3}\x07\u{c8}\x02\x02\u{8b3}\u{8b4}\x07\x63\x02\x02\u{8b4}\
	\u{8be}\x07\x44\x02\x02\u{8b5}\u{8b6}\x07\x5c\x02\x02\u{8b6}\u{8b7}\x07\
	\u{e5}\x02\x02\u{8b7}\u{8b8}\x07\x1f\x02\x02\u{8b8}\u{8bc}\x07\u{119}\x02\
	\x02\u{8b9}\u{8ba}\x07\x51\x02\x02\u{8ba}\u{8bb}\x07\x1f\x02\x02\u{8bb}\
	\u{8bd}\x07\u{119}\x02\x02\u{8bc}\u{8b9}\x03\x02\x02\x02\u{8bc}\u{8bd}\x03\
	\x02\x02\x02\u{8bd}\u{8bf}\x03\x02\x02\x02\u{8be}\u{8b5}\x03\x02\x02\x02\
	\u{8be}\u{8bf}\x03\x02\x02\x02\u{8bf}\u{8c5}\x03\x02\x02\x02\u{8c0}\u{8c1}\
	\x07\x2b\x02\x02\u{8c1}\u{8c2}\x07\x7c\x02\x02\u{8c2}\u{8c3}\x07\u{e5}\x02\
	\x02\u{8c3}\u{8c4}\x07\x1f\x02\x02\u{8c4}\u{8c6}\x07\u{119}\x02\x02\u{8c5}\
	\u{8c0}\x03\x02\x02\x02\u{8c5}\u{8c6}\x03\x02\x02\x02\u{8c6}\u{8cc}\x03\
	\x02\x02\x02\u{8c7}\u{8c8}\x07\u{8f}\x02\x02\u{8c8}\u{8c9}\x07\x7e\x02\x02\
	\u{8c9}\u{8ca}\x07\u{e5}\x02\x02\u{8ca}\u{8cb}\x07\x1f\x02\x02\u{8cb}\u{8cd}\
	\x07\u{119}\x02\x02\u{8cc}\u{8c7}\x03\x02\x02\x02\u{8cc}\u{8cd}\x03\x02\
	\x02\x02\u{8cd}\u{8d2}\x03\x02\x02\x02\u{8ce}\u{8cf}\x07\u{86}\x02\x02\u{8cf}\
	\u{8d0}\x07\u{e5}\x02\x02\u{8d0}\u{8d1}\x07\x1f\x02\x02\u{8d1}\u{8d3}\x07\
	\u{119}\x02\x02\u{8d2}\u{8ce}\x03\x02\x02\x02\u{8d2}\u{8d3}\x03\x02\x02\
	\x02\u{8d3}\u{8d8}\x03\x02\x02\x02\u{8d4}\u{8d5}\x07\u{98}\x02\x02\u{8d5}\
	\u{8d6}\x07\x42\x02\x02\u{8d6}\u{8d7}\x07\x17\x02\x02\u{8d7}\u{8d9}\x07\
	\u{119}\x02\x02\u{8d8}\u{8d4}\x03\x02\x02\x02\u{8d8}\u{8d9}\x03\x02\x02\
	\x02\u{8d9}\u{8db}\x03\x02\x02\x02\u{8da}\u{8a9}\x03\x02\x02\x02\u{8da}\
	\u{8b2}\x03\x02\x02\x02\u{8db}\u{ad}\x03\x02\x02\x02\u{8dc}\u{8e1}\x05\u{b0}\
	\x59\x02\u{8dd}\u{8de}\x07\x05\x02\x02\u{8de}\u{8e0}\x05\u{b0}\x59\x02\u{8df}\
	\u{8dd}\x03\x02\x02\x02\u{8e0}\u{8e3}\x03\x02\x02\x02\u{8e1}\u{8df}\x03\
	\x02\x02\x02\u{8e1}\u{8e2}\x03\x02\x02\x02\u{8e2}\u{af}\x03\x02\x02\x02\
	\u{8e3}\u{8e1}\x03\x02\x02\x02\u{8e4}\u{8e9}\x05\u{100}\u{81}\x02\u{8e5}\
	\u{8e6}\x07\x06\x02\x02\u{8e6}\u{8e8}\x05\u{100}\u{81}\x02\u{8e7}\u{8e5}\
	\x03\x02\x02\x02\u{8e8}\u{8eb}\x03\x02\x02\x02\u{8e9}\u{8e7}\x03\x02\x02\
	\x02\u{8e9}\u{8ea}\x03\x02\x02\x02\u{8ea}\u{b1}\x03\x02\x02\x02\u{8eb}\u{8e9}\
	\x03\x02\x02\x02\u{8ec}\u{8ed}\x05\u{100}\u{81}\x02\u{8ed}\u{8ee}\x07\x06\
	\x02\x02\u{8ee}\u{8f0}\x03\x02\x02\x02\u{8ef}\u{8ec}\x03\x02\x02\x02\u{8ef}\
	\u{8f0}\x03\x02\x02\x02\u{8f0}\u{8f1}\x03\x02\x02\x02\u{8f1}\u{8f2}\x05\
	\u{100}\u{81}\x02\u{8f2}\u{b3}\x03\x02\x02\x02\u{8f3}\u{8fb}\x05\u{be}\x60\
	\x02\u{8f4}\u{8f6}\x07\x17\x02\x02\u{8f5}\u{8f4}\x03\x02\x02\x02\u{8f5}\
	\u{8f6}\x03\x02\x02\x02\u{8f6}\u{8f9}\x03\x02\x02\x02\u{8f7}\u{8fa}\x05\
	\u{100}\u{81}\x02\u{8f8}\u{8fa}\x05\u{98}\x4d\x02\u{8f9}\u{8f7}\x03\x02\
	\x02\x02\u{8f9}\u{8f8}\x03\x02\x02\x02\u{8fa}\u{8fc}\x03\x02\x02\x02\u{8fb}\
	\u{8f5}\x03\x02\x02\x02\u{8fb}\u{8fc}\x03\x02\x02\x02\u{8fc}\u{b5}\x03\x02\
	\x02\x02\u{8fd}\u{902}\x05\u{b4}\x5b\x02\u{8fe}\u{8ff}\x07\x05\x02\x02\u{8ff}\
	\u{901}\x05\u{b4}\x5b\x02\u{900}\u{8fe}\x03\x02\x02\x02\u{901}\u{904}\x03\
	\x02\x02\x02\u{902}\u{900}\x03\x02\x02\x02\u{902}\u{903}\x03\x02\x02\x02\
	\u{903}\u{b7}\x03\x02\x02\x02\u{904}\u{902}\x03\x02\x02\x02\u{905}\u{906}\
	\x07\x03\x02\x02\u{906}\u{90b}\x05\u{ba}\x5e\x02\u{907}\u{908}\x07\x05\x02\
	\x02\u{908}\u{90a}\x05\u{ba}\x5e\x02\u{909}\u{907}\x03\x02\x02\x02\u{90a}\
	\u{90d}\x03\x02\x02\x02\u{90b}\u{909}\x03\x02\x02\x02\u{90b}\u{90c}\x03\
	\x02\x02\x02\u{90c}\u{90e}\x03\x02\x02\x02\u{90d}\u{90b}\x03\x02\x02\x02\
	\u{90e}\u{90f}\x07\x04\x02\x02\u{90f}\u{b9}\x03\x02\x02\x02\u{910}\u{91e}\
	\x05\u{fe}\u{80}\x02\u{911}\u{912}\x05\u{104}\u{83}\x02\u{912}\u{913}\x07\
	\x03\x02\x02\u{913}\u{918}\x05\u{bc}\x5f\x02\u{914}\u{915}\x07\x05\x02\x02\
	\u{915}\u{917}\x05\u{bc}\x5f\x02\u{916}\u{914}\x03\x02\x02\x02\u{917}\u{91a}\
	\x03\x02\x02\x02\u{918}\u{916}\x03\x02\x02\x02\u{918}\u{919}\x03\x02\x02\
	\x02\u{919}\u{91b}\x03\x02\x02\x02\u{91a}\u{918}\x03\x02\x02\x02\u{91b}\
	\u{91c}\x07\x04\x02\x02\u{91c}\u{91e}\x03\x02\x02\x02\u{91d}\u{910}\x03\
	\x02\x02\x02\u{91d}\u{911}\x03\x02\x02\x02\u{91e}\u{bb}\x03\x02\x02\x02\
	\u{91f}\u{922}\x05\u{fe}\u{80}\x02\u{920}\u{922}\x05\u{c8}\x65\x02\u{921}\
	\u{91f}\x03\x02\x02\x02\u{921}\u{920}\x03\x02\x02\x02\u{922}\u{bd}\x03\x02\
	\x02\x02\u{923}\u{924}\x05\u{c0}\x61\x02\u{924}\u{bf}\x03\x02\x02\x02\u{925}\
	\u{926}\x08\x61\x01\x02\u{926}\u{927}\x07\u{97}\x02\x02\u{927}\u{932}\x05\
	\u{c0}\x61\x07\u{928}\u{929}\x07\x54\x02\x02\u{929}\u{92a}\x07\x03\x02\x02\
	\u{92a}\u{92b}\x05\x24\x13\x02\u{92b}\u{92c}\x07\x04\x02\x02\u{92c}\u{932}\
	\x03\x02\x02\x02\u{92d}\u{92f}\x05\u{c4}\x63\x02\u{92e}\u{930}\x05\u{c2}\
	\x62\x02\u{92f}\u{92e}\x03\x02\x02\x02\u{92f}\u{930}\x03\x02\x02\x02\u{930}\
	\u{932}\x03\x02\x02\x02\u{931}\u{925}\x03\x02\x02\x02\u{931}\u{928}\x03\
	\x02\x02\x02\u{931}\u{92d}\x03\x02\x02\x02\u{932}\u{93b}\x03\x02\x02\x02\
	\u{933}\u{934}\x0c\x04\x02\x02\u{934}\u{935}\x07\x12\x02\x02\u{935}\u{93a}\
	\x05\u{c0}\x61\x05\u{936}\u{937}\x0c\x03\x02\x02\u{937}\u{938}\x07\u{9f}\
	\x02\x02\u{938}\u{93a}\x05\u{c0}\x61\x04\u{939}\u{933}\x03\x02\x02\x02\u{939}\
	\u{936}\x03\x02\x02\x02\u{93a}\u{93d}\x03\x02\x02\x02\u{93b}\u{939}\x03\
	\x02\x02\x02\u{93b}\u{93c}\x03\x02\x02\x02\u{93c}\u{c1}\x03\x02\x02\x02\
	\u{93d}\u{93b}\x03\x02\x02\x02\u{93e}\u{940}\x07\u{97}\x02\x02\u{93f}\u{93e}\
	\x03\x02\x02\x02\u{93f}\u{940}\x03\x02\x02\x02\u{940}\u{941}\x03\x02\x02\
	\x02\u{941}\u{942}\x07\x1b\x02\x02\u{942}\u{943}\x05\u{c4}\x63\x02\u{943}\
	\u{944}\x07\x12\x02\x02\u{944}\u{945}\x05\u{c4}\x63\x02\u{945}\u{991}\x03\
	\x02\x02\x02\u{946}\u{948}\x07\u{97}\x02\x02\u{947}\u{946}\x03\x02\x02\x02\
	\u{947}\u{948}\x03\x02\x02\x02\u{948}\u{949}\x03\x02\x02\x02\u{949}\u{94a}\
	\x07\x71\x02\x02\u{94a}\u{94b}\x07\x03\x02\x02\u{94b}\u{950}\x05\u{be}\x60\
	\x02\u{94c}\u{94d}\x07\x05\x02\x02\u{94d}\u{94f}\x05\u{be}\x60\x02\u{94e}\
	\u{94c}\x03\x02\x02\x02\u{94f}\u{952}\x03\x02\x02\x02\u{950}\u{94e}\x03\
	\x02\x02\x02\u{950}\u{951}\x03\x02\x02\x02\u{951}\u{953}\x03\x02\x02\x02\
	\u{952}\u{950}\x03\x02\x02\x02\u{953}\u{954}\x07\x04\x02\x02\u{954}\u{991}\
	\x03\x02\x02\x02\u{955}\u{957}\x07\u{97}\x02\x02\u{956}\u{955}\x03\x02\x02\
	\x02\u{956}\u{957}\x03\x02\x02\x02\u{957}\u{958}\x03\x02\x02\x02\u{958}\
	\u{959}\x07\x71\x02\x02\u{959}\u{95a}\x07\x03\x02\x02\u{95a}\u{95b}\x05\
	\x24\x13\x02\u{95b}\u{95c}\x07\x04\x02\x02\u{95c}\u{991}\x03\x02\x02\x02\
	\u{95d}\u{95f}\x07\u{97}\x02\x02\u{95e}\u{95d}\x03\x02\x02\x02\u{95e}\u{95f}\
	\x03\x02\x02\x02\u{95f}\u{960}\x03\x02\x02\x02\u{960}\u{961}\x07\u{c3}\x02\
	\x02\u{961}\u{991}\x05\u{c4}\x63\x02\u{962}\u{964}\x07\u{97}\x02\x02\u{963}\
	\u{962}\x03\x02\x02\x02\u{963}\u{964}\x03\x02\x02\x02\u{964}\u{965}\x03\
	\x02\x02\x02\u{965}\u{966}\x07\u{84}\x02\x02\u{966}\u{974}\x09\x16\x02\x02\
	\u{967}\u{968}\x07\x03\x02\x02\u{968}\u{975}\x07\x04\x02\x02\u{969}\u{96a}\
	\x07\x03\x02\x02\u{96a}\u{96f}\x05\u{be}\x60\x02\u{96b}\u{96c}\x07\x05\x02\
	\x02\u{96c}\u{96e}\x05\u{be}\x60\x02\u{96d}\u{96b}\x03\x02\x02\x02\u{96e}\
	\u{971}\x03\x02\x02\x02\u{96f}\u{96d}\x03\x02\x02\x02\u{96f}\u{970}\x03\
	\x02\x02\x02\u{970}\u{972}\x03\x02\x02\x02\u{971}\u{96f}\x03\x02\x02\x02\
	\u{972}\u{973}\x07\x04\x02\x02\u{973}\u{975}\x03\x02\x02\x02\u{974}\u{967}\
	\x03\x02\x02\x02\u{974}\u{969}\x03\x02\x02\x02\u{975}\u{991}\x03\x02\x02\
	\x02\u{976}\u{978}\x07\u{97}\x02\x02\u{977}\u{976}\x03\x02\x02\x02\u{977}\
	\u{978}\x03\x02\x02\x02\u{978}\u{979}\x03\x02\x02\x02\u{979}\u{97a}\x07\
	\u{84}\x02\x02\u{97a}\u{97d}\x05\u{c4}\x63\x02\u{97b}\u{97c}\x07\x50\x02\
	\x02\u{97c}\u{97e}\x07\u{119}\x02\x02\u{97d}\u{97b}\x03\x02\x02\x02\u{97d}\
	\u{97e}\x03\x02\x02\x02\u{97e}\u{991}\x03\x02\x02\x02\u{97f}\u{981}\x07\
	\x7b\x02\x02\u{980}\u{982}\x07\u{97}\x02\x02\u{981}\u{980}\x03\x02\x02\x02\
	\u{981}\u{982}\x03\x02\x02\x02\u{982}\u{983}\x03\x02\x02\x02\u{983}\u{991}\
	\x07\u{98}\x02\x02\u{984}\u{986}\x07\x7b\x02\x02\u{985}\u{987}\x07\u{97}\
	\x02\x02\u{986}\u{985}\x03\x02\x02\x02\u{986}\u{987}\x03\x02\x02\x02\u{987}\
	\u{988}\x03\x02\x02\x02\u{988}\u{991}\x09\x17\x02\x02\u{989}\u{98b}\x07\
	\x7b\x02\x02\u{98a}\u{98c}\x07\u{97}\x02\x02\u{98b}\u{98a}\x03\x02\x02\x02\
	\u{98b}\u{98c}\x03\x02\x02\x02\u{98c}\u{98d}\x03\x02\x02\x02\u{98d}\u{98e}\
	\x07\x4a\x02\x02\u{98e}\u{98f}\x07\x65\x02\x02\u{98f}\u{991}\x05\u{c4}\x63\
	\x02\u{990}\u{93f}\x03\x02\x02\x02\u{990}\u{947}\x03\x02\x02\x02\u{990}\
	\u{956}\x03\x02\x02\x02\u{990}\u{95e}\x03\x02\x02\x02\u{990}\u{963}\x03\
	\x02\x02\x02\u{990}\u{977}\x03\x02\x02\x02\u{990}\u{97f}\x03\x02\x02\x02\
	\u{990}\u{984}\x03\x02\x02\x02\u{990}\u{989}\x03\x02\x02\x02\u{991}\u{c3}\
	\x03\x02\x02\x02\u{992}\u{993}\x08\x63\x01\x02\u{993}\u{997}\x05\u{c6}\x64\
	\x02\u{994}\u{995}\x09\x18\x02\x02\u{995}\u{997}\x05\u{c4}\x63\x09\u{996}\
	\u{992}\x03\x02\x02\x02\u{996}\u{994}\x03\x02\x02\x02\u{997}\u{9ad}\x03\
	\x02\x02\x02\u{998}\u{999}\x0c\x08\x02\x02\u{999}\u{99a}\x09\x19\x02\x02\
	\u{99a}\u{9ac}\x05\u{c4}\x63\x09\u{99b}\u{99c}\x0c\x07\x02\x02\u{99c}\u{99d}\
	\x09\x1a\x02\x02\u{99d}\u{9ac}\x05\u{c4}\x63\x08\u{99e}\u{99f}\x0c\x06\x02\
	\x02\u{99f}\u{9a0}\x07\u{114}\x02\x02\u{9a0}\u{9ac}\x05\u{c4}\x63\x07\u{9a1}\
	\u{9a2}\x0c\x05\x02\x02\u{9a2}\u{9a3}\x07\u{117}\x02\x02\u{9a3}\u{9ac}\x05\
	\u{c4}\x63\x06\u{9a4}\u{9a5}\x0c\x04\x02\x02\u{9a5}\u{9a6}\x07\u{115}\x02\
	\x02\u{9a6}\u{9ac}\x05\u{c4}\x63\x05\u{9a7}\u{9a8}\x0c\x03\x02\x02\u{9a8}\
	\u{9a9}\x05\u{ca}\x66\x02\u{9a9}\u{9aa}\x05\u{c4}\x63\x04\u{9aa}\u{9ac}\
	\x03\x02\x02\x02\u{9ab}\u{998}\x03\x02\x02\x02\u{9ab}\u{99b}\x03\x02\x02\
	\x02\u{9ab}\u{99e}\x03\x02\x02\x02\u{9ab}\u{9a1}\x03\x02\x02\x02\u{9ab}\
	\u{9a4}\x03\x02\x02\x02\u{9ab}\u{9a7}\x03\x02\x02\x02\u{9ac}\u{9af}\x03\
	\x02\x02\x02\u{9ad}\u{9ab}\x03\x02\x02\x02\u{9ad}\u{9ae}\x03\x02\x02\x02\
	\u{9ae}\u{c5}\x03\x02\x02\x02\u{9af}\u{9ad}\x03\x02\x02\x02\u{9b0}\u{9b1}\
	\x08\x64\x01\x02\u{9b1}\u{a69}\x09\x1b\x02\x02\u{9b2}\u{9b4}\x07\x22\x02\
	\x02\u{9b3}\u{9b5}\x05\u{ee}\x78\x02\u{9b4}\u{9b3}\x03\x02\x02\x02\u{9b5}\
	\u{9b6}\x03\x02\x02\x02\u{9b6}\u{9b4}\x03\x02\x02\x02\u{9b6}\u{9b7}\x03\
	\x02\x02\x02\u{9b7}\u{9ba}\x03\x02\x02\x02\u{9b8}\u{9b9}\x07\x4e\x02\x02\
	\u{9b9}\u{9bb}\x05\u{be}\x60\x02\u{9ba}\u{9b8}\x03\x02\x02\x02\u{9ba}\u{9bb}\
	\x03\x02\x02\x02\u{9bb}\u{9bc}\x03\x02\x02\x02\u{9bc}\u{9bd}\x07\x4f\x02\
	\x02\u{9bd}\u{a69}\x03\x02\x02\x02\u{9be}\u{9bf}\x07\x22\x02\x02\u{9bf}\
	\u{9c1}\x05\u{be}\x60\x02\u{9c0}\u{9c2}\x05\u{ee}\x78\x02\u{9c1}\u{9c0}\
	\x03\x02\x02\x02\u{9c2}\u{9c3}\x03\x02\x02\x02\u{9c3}\u{9c1}\x03\x02\x02\
	\x02\u{9c3}\u{9c4}\x03\x02\x02\x02\u{9c4}\u{9c7}\x03\x02\x02\x02\u{9c5}\
	\u{9c6}\x07\x4e\x02\x02\u{9c6}\u{9c8}\x05\u{be}\x60\x02\u{9c7}\u{9c5}\x03\
	\x02\x02\x02\u{9c7}\u{9c8}\x03\x02\x02\x02\u{9c8}\u{9c9}\x03\x02\x02\x02\
	\u{9c9}\u{9ca}\x07\x4f\x02\x02\u{9ca}\u{a69}\x03\x02\x02\x02\u{9cb}\u{9cc}\
	\x07\x23\x02\x02\u{9cc}\u{9cd}\x07\x03\x02\x02\u{9cd}\u{9ce}\x05\u{be}\x60\
	\x02\u{9ce}\u{9cf}\x07\x17\x02\x02\u{9cf}\u{9d0}\x05\u{e0}\x71\x02\u{9d0}\
	\u{9d1}\x07\x04\x02\x02\u{9d1}\u{a69}\x03\x02\x02\x02\u{9d2}\u{9d3}\x07\
	\u{dd}\x02\x02\u{9d3}\u{9dc}\x07\x03\x02\x02\u{9d4}\u{9d9}\x05\u{b4}\x5b\
	\x02\u{9d5}\u{9d6}\x07\x05\x02\x02\u{9d6}\u{9d8}\x05\u{b4}\x5b\x02\u{9d7}\
	\u{9d5}\x03\x02\x02\x02\u{9d8}\u{9db}\x03\x02\x02\x02\u{9d9}\u{9d7}\x03\
	\x02\x02\x02\u{9d9}\u{9da}\x03\x02\x02\x02\u{9da}\u{9dd}\x03\x02\x02\x02\
	\u{9db}\u{9d9}\x03\x02\x02\x02\u{9dc}\u{9d4}\x03\x02\x02\x02\u{9dc}\u{9dd}\
	\x03\x02\x02\x02\u{9dd}\u{9de}\x03\x02\x02\x02\u{9de}\u{a69}\x07\x04\x02\
	\x02\u{9df}\u{9e0}\x07\x5f\x02\x02\u{9e0}\u{9e1}\x07\x03\x02\x02\u{9e1}\
	\u{9e4}\x05\u{be}\x60\x02\u{9e2}\u{9e3}\x07\x6f\x02\x02\u{9e3}\u{9e5}\x07\
	\u{99}\x02\x02\u{9e4}\u{9e2}\x03\x02\x02\x02\u{9e4}\u{9e5}\x03\x02\x02\x02\
	\u{9e5}\u{9e6}\x03\x02\x02\x02\u{9e6}\u{9e7}\x07\x04\x02\x02\u{9e7}\u{a69}\
	\x03\x02\x02\x02\u{9e8}\u{9e9}\x07\x7f\x02\x02\u{9e9}\u{9ea}\x07\x03\x02\
	\x02\u{9ea}\u{9ed}\x05\u{be}\x60\x02\u{9eb}\u{9ec}\x07\x6f\x02\x02\u{9ec}\
	\u{9ee}\x07\u{99}\x02\x02\u{9ed}\u{9eb}\x03\x02\x02\x02\u{9ed}\u{9ee}\x03\
	\x02\x02\x02\u{9ee}\u{9ef}\x03\x02\x02\x02\u{9ef}\u{9f0}\x07\x04\x02\x02\
	\u{9f0}\u{a69}\x03\x02\x02\x02\u{9f1}\u{9f2}\x07\u{ae}\x02\x02\u{9f2}\u{9f3}\
	\x07\x03\x02\x02\u{9f3}\u{9f4}\x05\u{c4}\x63\x02\u{9f4}\u{9f5}\x07\x71\x02\
	\x02\u{9f5}\u{9f6}\x05\u{c4}\x63\x02\u{9f6}\u{9f7}\x07\x04\x02\x02\u{9f7}\
	\u{a69}\x03\x02\x02\x02\u{9f8}\u{a69}\x05\u{c8}\x65\x02\u{9f9}\u{a69}\x07\
	\u{110}\x02\x02\u{9fa}\u{9fb}\x05\u{fe}\u{80}\x02\u{9fb}\u{9fc}\x07\x06\
	\x02\x02\u{9fc}\u{9fd}\x07\u{110}\x02\x02\u{9fd}\u{a69}\x03\x02\x02\x02\
	\u{9fe}\u{9ff}\x07\x03\x02\x02\u{9ff}\u{a02}\x05\u{b4}\x5b\x02\u{a00}\u{a01}\
	\x07\x05\x02\x02\u{a01}\u{a03}\x05\u{b4}\x5b\x02\u{a02}\u{a00}\x03\x02\x02\
	\x02\u{a03}\u{a04}\x03\x02\x02\x02\u{a04}\u{a02}\x03\x02\x02\x02\u{a04}\
	\u{a05}\x03\x02\x02\x02\u{a05}\u{a06}\x03\x02\x02\x02\u{a06}\u{a07}\x07\
	\x04\x02\x02\u{a07}\u{a69}\x03\x02\x02\x02\u{a08}\u{a09}\x07\x03\x02\x02\
	\u{a09}\u{a0a}\x05\x24\x13\x02\u{a0a}\u{a0b}\x07\x04\x02\x02\u{a0b}\u{a69}\
	\x03\x02\x02\x02\u{a0c}\u{a0d}\x05\u{fc}\x7f\x02\u{a0d}\u{a19}\x07\x03\x02\
	\x02\u{a0e}\u{a10}\x05\u{8a}\x46\x02\u{a0f}\u{a0e}\x03\x02\x02\x02\u{a0f}\
	\u{a10}\x03\x02\x02\x02\u{a10}\u{a11}\x03\x02\x02\x02\u{a11}\u{a16}\x05\
	\u{be}\x60\x02\u{a12}\u{a13}\x07\x05\x02\x02\u{a13}\u{a15}\x05\u{be}\x60\
	\x02\u{a14}\u{a12}\x03\x02\x02\x02\u{a15}\u{a18}\x03\x02\x02\x02\u{a16}\
	\u{a14}\x03\x02\x02\x02\u{a16}\u{a17}\x03\x02\x02\x02\u{a17}\u{a1a}\x03\
	\x02\x02\x02\u{a18}\u{a16}\x03\x02\x02\x02\u{a19}\u{a0f}\x03\x02\x02\x02\
	\u{a19}\u{a1a}\x03\x02\x02\x02\u{a1a}\u{a1b}\x03\x02\x02\x02\u{a1b}\u{a22}\
	\x07\x04\x02\x02\u{a1c}\u{a1d}\x07\x5d\x02\x02\u{a1d}\u{a1e}\x07\x03\x02\
	\x02\u{a1e}\u{a1f}\x07\u{102}\x02\x02\u{a1f}\u{a20}\x05\u{c0}\x61\x02\u{a20}\
	\u{a21}\x07\x04\x02\x02\u{a21}\u{a23}\x03\x02\x02\x02\u{a22}\u{a1c}\x03\
	\x02\x02\x02\u{a22}\u{a23}\x03\x02\x02\x02\u{a23}\u{a26}\x03\x02\x02\x02\
	\u{a24}\u{a25}\x07\u{a4}\x02\x02\u{a25}\u{a27}\x05\u{f4}\x7b\x02\u{a26}\
	\u{a24}\x03\x02\x02\x02\u{a26}\u{a27}\x03\x02\x02\x02\u{a27}\u{a69}\x03\
	\x02\x02\x02\u{a28}\u{a29}\x05\u{104}\u{83}\x02\u{a29}\u{a2a}\x07\x09\x02\
	\x02\u{a2a}\u{a2b}\x05\u{be}\x60\x02\u{a2b}\u{a69}\x03\x02\x02\x02\u{a2c}\
	\u{a2d}\x07\x03\x02\x02\u{a2d}\u{a30}\x05\u{104}\u{83}\x02\u{a2e}\u{a2f}\
	\x07\x05\x02\x02\u{a2f}\u{a31}\x05\u{104}\u{83}\x02\u{a30}\u{a2e}\x03\x02\
	\x02\x02\u{a31}\u{a32}\x03\x02\x02\x02\u{a32}\u{a30}\x03\x02\x02\x02\u{a32}\
	\u{a33}\x03\x02\x02\x02\u{a33}\u{a34}\x03\x02\x02\x02\u{a34}\u{a35}\x07\
	\x04\x02\x02\u{a35}\u{a36}\x07\x09\x02\x02\u{a36}\u{a37}\x05\u{be}\x60\x02\
	\u{a37}\u{a69}\x03\x02\x02\x02\u{a38}\u{a69}\x05\u{104}\u{83}\x02\u{a39}\
	\u{a3a}\x07\x03\x02\x02\u{a3a}\u{a3b}\x05\u{be}\x60\x02\u{a3b}\u{a3c}\x07\
	\x04\x02\x02\u{a3c}\u{a69}\x03\x02\x02\x02\u{a3d}\u{a3e}\x07\x59\x02\x02\
	\u{a3e}\u{a3f}\x07\x03\x02\x02\u{a3f}\u{a40}\x05\u{104}\u{83}\x02\u{a40}\
	\u{a41}\x07\x65\x02\x02\u{a41}\u{a42}\x05\u{c4}\x63\x02\u{a42}\u{a43}\x07\
	\x04\x02\x02\u{a43}\u{a69}\x03\x02\x02\x02\u{a44}\u{a45}\x09\x1c\x02\x02\
	\u{a45}\u{a46}\x07\x03\x02\x02\u{a46}\u{a47}\x05\u{c4}\x63\x02\u{a47}\u{a48}\
	\x09\x1d\x02\x02\u{a48}\u{a4b}\x05\u{c4}\x63\x02\u{a49}\u{a4a}\x09\x1e\x02\
	\x02\u{a4a}\u{a4c}\x05\u{c4}\x63\x02\u{a4b}\u{a49}\x03\x02\x02\x02\u{a4b}\
	\u{a4c}\x03\x02\x02\x02\u{a4c}\u{a4d}\x03\x02\x02\x02\u{a4d}\u{a4e}\x07\
	\x04\x02\x02\u{a4e}\u{a69}\x03\x02\x02\x02\u{a4f}\u{a50}\x07\u{ee}\x02\x02\
	\u{a50}\u{a52}\x07\x03\x02\x02\u{a51}\u{a53}\x09\x1f\x02\x02\u{a52}\u{a51}\
	\x03\x02\x02\x02\u{a52}\u{a53}\x03\x02\x02\x02\u{a53}\u{a55}\x03\x02\x02\
	\x02\u{a54}\u{a56}\x05\u{c4}\x63\x02\u{a55}\u{a54}\x03\x02\x02\x02\u{a55}\
	\u{a56}\x03\x02\x02\x02\u{a56}\u{a57}\x03\x02\x02\x02\u{a57}\u{a58}\x07\
	\x65\x02\x02\u{a58}\u{a59}\x05\u{c4}\x63\x02\u{a59}\u{a5a}\x07\x04\x02\x02\
	\u{a5a}\u{a69}\x03\x02\x02\x02\u{a5b}\u{a5c}\x07\u{a6}\x02\x02\u{a5c}\u{a5d}\
	\x07\x03\x02\x02\u{a5d}\u{a5e}\x05\u{c4}\x63\x02\u{a5e}\u{a5f}\x07\u{ad}\
	\x02\x02\u{a5f}\u{a60}\x05\u{c4}\x63\x02\u{a60}\u{a61}\x07\x65\x02\x02\u{a61}\
	\u{a64}\x05\u{c4}\x63\x02\u{a62}\u{a63}\x07\x61\x02\x02\u{a63}\u{a65}\x05\
	\u{c4}\x63\x02\u{a64}\u{a62}\x03\x02\x02\x02\u{a64}\u{a65}\x03\x02\x02\x02\
	\u{a65}\u{a66}\x03\x02\x02\x02\u{a66}\u{a67}\x07\x04\x02\x02\u{a67}\u{a69}\
	\x03\x02\x02\x02\u{a68}\u{9b0}\x03\x02\x02\x02\u{a68}\u{9b2}\x03\x02\x02\
	\x02\u{a68}\u{9be}\x03\x02\x02\x02\u{a68}\u{9cb}\x03\x02\x02\x02\u{a68}\
	\u{9d2}\x03\x02\x02\x02\u{a68}\u{9df}\x03\x02\x02\x02\u{a68}\u{9e8}\x03\
	\x02\x02\x02\u{a68}\u{9f1}\x03\x02\x02\x02\u{a68}\u{9f8}\x03\x02\x02\x02\
	\u{a68}\u{9f9}\x03\x02\x02\x02\u{a68}\u{9fa}\x03\x02\x02\x02\u{a68}\u{9fe}\
	\x03\x02\x02\x02\u{a68}\u{a08}\x03\x02\x02\x02\u{a68}\u{a0c}\x03\x02\x02\
	\x02\u{a68}\u{a28}\x03\x02\x02\x02\u{a68}\u{a2c}\x03\x02\x02\x02\u{a68}\
	\u{a38}\x03\x02\x02\x02\u{a68}\u{a39}\x03\x02\x02\x02\u{a68}\u{a3d}\x03\
	\x02\x02\x02\u{a68}\u{a44}\x03\x02\x02\x02\u{a68}\u{a4f}\x03\x02\x02\x02\
	\u{a68}\u{a5b}\x03\x02\x02\x02\u{a69}\u{a74}\x03\x02\x02\x02\u{a6a}\u{a6b}\
	\x0c\x0a\x02\x02\u{a6b}\u{a6c}\x07\x0a\x02\x02\u{a6c}\u{a6d}\x05\u{c4}\x63\
	\x02\u{a6d}\u{a6e}\x07\x0b\x02\x02\u{a6e}\u{a73}\x03\x02\x02\x02\u{a6f}\
	\u{a70}\x0c\x08\x02\x02\u{a70}\u{a71}\x07\x06\x02\x02\u{a71}\u{a73}\x05\
	\u{104}\u{83}\x02\u{a72}\u{a6a}\x03\x02\x02\x02\u{a72}\u{a6f}\x03\x02\x02\
	\x02\u{a73}\u{a76}\x03\x02\x02\x02\u{a74}\u{a72}\x03\x02\x02\x02\u{a74}\
	\u{a75}\x03\x02\x02\x02\u{a75}\u{c7}\x03\x02\x02\x02\u{a76}\u{a74}\x03\x02\
	\x02\x02\u{a77}\u{a84}\x07\u{98}\x02\x02\u{a78}\u{a84}\x05\u{d2}\x6a\x02\
	\u{a79}\u{a7a}\x05\u{104}\u{83}\x02\u{a7a}\u{a7b}\x07\u{119}\x02\x02\u{a7b}\
	\u{a84}\x03\x02\x02\x02\u{a7c}\u{a84}\x05\u{10a}\u{86}\x02\u{a7d}\u{a84}\
	\x05\u{d0}\x69\x02\u{a7e}\u{a80}\x07\u{119}\x02\x02\u{a7f}\u{a7e}\x03\x02\
	\x02\x02\u{a80}\u{a81}\x03\x02\x02\x02\u{a81}\u{a7f}\x03\x02\x02\x02\u{a81}\
	\u{a82}\x03\x02\x02\x02\u{a82}\u{a84}\x03\x02\x02\x02\u{a83}\u{a77}\x03\
	\x02\x02\x02\u{a83}\u{a78}\x03\x02\x02\x02\u{a83}\u{a79}\x03\x02\x02\x02\
	\u{a83}\u{a7c}\x03\x02\x02\x02\u{a83}\u{a7d}\x03\x02\x02\x02\u{a83}\u{a7f}\
	\x03\x02\x02\x02\u{a84}\u{c9}\x03\x02\x02\x02\u{a85}\u{a86}\x09\x20\x02\
	\x02\u{a86}\u{cb}\x03\x02\x02\x02\u{a87}\u{a88}\x09\x21\x02\x02\u{a88}\u{cd}\
	\x03\x02\x02\x02\u{a89}\u{a8a}\x09\x22\x02\x02\u{a8a}\u{cf}\x03\x02\x02\
	\x02\u{a8b}\u{a8c}\x09\x23\x02\x02\u{a8c}\u{d1}\x03\x02\x02\x02\u{a8d}\u{a90}\
	\x07\x79\x02\x02\u{a8e}\u{a91}\x05\u{d4}\x6b\x02\u{a8f}\u{a91}\x05\u{d8}\
	\x6d\x02\u{a90}\u{a8e}\x03\x02\x02\x02\u{a90}\u{a8f}\x03\x02\x02\x02\u{a90}\
	\u{a91}\x03\x02\x02\x02\u{a91}\u{d3}\x03\x02\x02\x02\u{a92}\u{a94}\x05\u{d6}\
	\x6c\x02\u{a93}\u{a95}\x05\u{da}\x6e\x02\u{a94}\u{a93}\x03\x02\x02\x02\u{a94}\
	\u{a95}\x03\x02\x02\x02\u{a95}\u{d5}\x03\x02\x02\x02\u{a96}\u{a97}\x05\u{dc}\
	\x6f\x02\u{a97}\u{a98}\x05\u{104}\u{83}\x02\u{a98}\u{a9a}\x03\x02\x02\x02\
	\u{a99}\u{a96}\x03\x02\x02\x02\u{a9a}\u{a9b}\x03\x02\x02\x02\u{a9b}\u{a99}\
	\x03\x02\x02\x02\u{a9b}\u{a9c}\x03\x02\x02\x02\u{a9c}\u{d7}\x03\x02\x02\
	\x02\u{a9d}\u{aa0}\x05\u{da}\x6e\x02\u{a9e}\u{aa1}\x05\u{d6}\x6c\x02\u{a9f}\
	\u{aa1}\x05\u{da}\x6e\x02\u{aa0}\u{a9e}\x03\x02\x02\x02\u{aa0}\u{a9f}\x03\
	\x02\x02\x02\u{aa0}\u{aa1}\x03\x02\x02\x02\u{aa1}\u{d9}\x03\x02\x02\x02\
	\u{aa2}\u{aa3}\x05\u{dc}\x6f\x02\u{aa3}\u{aa4}\x05\u{104}\u{83}\x02\u{aa4}\
	\u{aa5}\x07\u{e8}\x02\x02\u{aa5}\u{aa6}\x05\u{104}\u{83}\x02\u{aa6}\u{db}\
	\x03\x02\x02\x02\u{aa7}\u{aa9}\x09\x24\x02\x02\u{aa8}\u{aa7}\x03\x02\x02\
	\x02\u{aa8}\u{aa9}\x03\x02\x02\x02\u{aa9}\u{aaa}\x03\x02\x02\x02\u{aaa}\
	\u{aad}\x09\x15\x02\x02\u{aab}\u{aad}\x07\u{119}\x02\x02\u{aac}\u{aa8}\x03\
	\x02\x02\x02\u{aac}\u{aab}\x03\x02\x02\x02\u{aad}\u{dd}\x03\x02\x02\x02\
	\u{aae}\u{ab2}\x07\x5f\x02\x02\u{aaf}\u{ab0}\x07\x0e\x02\x02\u{ab0}\u{ab2}\
	\x05\u{100}\u{81}\x02\u{ab1}\u{aae}\x03\x02\x02\x02\u{ab1}\u{aaf}\x03\x02\
	\x02\x02\u{ab2}\u{df}\x03\x02\x02\x02\u{ab3}\u{ab4}\x07\x16\x02\x02\u{ab4}\
	\u{ab5}\x07\u{10a}\x02\x02\u{ab5}\u{ab6}\x05\u{e0}\x71\x02\u{ab6}\u{ab7}\
	\x07\u{10c}\x02\x02\u{ab7}\u{ad6}\x03\x02\x02\x02\u{ab8}\u{ab9}\x07\u{8f}\
	\x02\x02\u{ab9}\u{aba}\x07\u{10a}\x02\x02\u{aba}\u{abb}\x05\u{e0}\x71\x02\
	\u{abb}\u{abc}\x07\x05\x02\x02\u{abc}\u{abd}\x05\u{e0}\x71\x02\u{abd}\u{abe}\
	\x07\u{10c}\x02\x02\u{abe}\u{ad6}\x03\x02\x02\x02\u{abf}\u{ac6}\x07\u{dd}\
	\x02\x02\u{ac0}\u{ac2}\x07\u{10a}\x02\x02\u{ac1}\u{ac3}\x05\u{ea}\x76\x02\
	\u{ac2}\u{ac1}\x03\x02\x02\x02\u{ac2}\u{ac3}\x03\x02\x02\x02\u{ac3}\u{ac4}\
	\x03\x02\x02\x02\u{ac4}\u{ac7}\x07\u{10c}\x02\x02\u{ac5}\u{ac7}\x07\u{108}\
	\x02\x02\u{ac6}\u{ac0}\x03\x02\x02\x02\u{ac6}\u{ac5}\x03\x02\x02\x02\u{ac7}\
	\u{ad6}\x03\x02\x02\x02\u{ac8}\u{ad3}\x05\u{104}\u{83}\x02\u{ac9}\u{aca}\
	\x07\x03\x02\x02\u{aca}\u{acf}\x07\u{11d}\x02\x02\u{acb}\u{acc}\x07\x05\
	\x02\x02\u{acc}\u{ace}\x07\u{11d}\x02\x02\u{acd}\u{acb}\x03\x02\x02\x02\
	\u{ace}\u{ad1}\x03\x02\x02\x02\u{acf}\u{acd}\x03\x02\x02\x02\u{acf}\u{ad0}\
	\x03\x02\x02\x02\u{ad0}\u{ad2}\x03\x02\x02\x02\u{ad1}\u{acf}\x03\x02\x02\
	\x02\u{ad2}\u{ad4}\x07\x04\x02\x02\u{ad3}\u{ac9}\x03\x02\x02\x02\u{ad3}\
	\u{ad4}\x03\x02\x02\x02\u{ad4}\u{ad6}\x03\x02\x02\x02\u{ad5}\u{ab3}\x03\
	\x02\x02\x02\u{ad5}\u{ab8}\x03\x02\x02\x02\u{ad5}\u{abf}\x03\x02\x02\x02\
	\u{ad5}\u{ac8}\x03\x02\x02\x02\u{ad6}\u{e1}\x03\x02\x02\x02\u{ad7}\u{adc}\
	\x05\u{e4}\x73\x02\u{ad8}\u{ad9}\x07\x05\x02\x02\u{ad9}\u{adb}\x05\u{e4}\
	\x73\x02\u{ada}\u{ad8}\x03\x02\x02\x02\u{adb}\u{ade}\x03\x02\x02\x02\u{adc}\
	\u{ada}\x03\x02\x02\x02\u{adc}\u{add}\x03\x02\x02\x02\u{add}\u{e3}\x03\x02\
	\x02\x02\u{ade}\u{adc}\x03\x02\x02\x02\u{adf}\u{ae0}\x05\u{b0}\x59\x02\u{ae0}\
	\u{ae3}\x05\u{e0}\x71\x02\u{ae1}\u{ae2}\x07\u{97}\x02\x02\u{ae2}\u{ae4}\
	\x07\u{98}\x02\x02\u{ae3}\u{ae1}\x03\x02\x02\x02\u{ae3}\u{ae4}\x03\x02\x02\
	\x02\u{ae4}\u{ae6}\x03\x02\x02\x02\u{ae5}\u{ae7}\x05\x22\x12\x02\u{ae6}\
	\u{ae5}\x03\x02\x02\x02\u{ae6}\u{ae7}\x03\x02\x02\x02\u{ae7}\u{ae9}\x03\
	\x02\x02\x02\u{ae8}\u{aea}\x05\u{de}\x70\x02\u{ae9}\u{ae8}\x03\x02\x02\x02\
	\u{ae9}\u{aea}\x03\x02\x02\x02\u{aea}\u{e5}\x03\x02\x02\x02\u{aeb}\u{af0}\
	\x05\u{e8}\x75\x02\u{aec}\u{aed}\x07\x05\x02\x02\u{aed}\u{aef}\x05\u{e8}\
	\x75\x02\u{aee}\u{aec}\x03\x02\x02\x02\u{aef}\u{af2}\x03\x02\x02\x02\u{af0}\
	\u{aee}\x03\x02\x02\x02\u{af0}\u{af1}\x03\x02\x02\x02\u{af1}\u{e7}\x03\x02\
	\x02\x02\u{af2}\u{af0}\x03\x02\x02\x02\u{af3}\u{af4}\x05\u{100}\u{81}\x02\
	\u{af4}\u{af7}\x05\u{e0}\x71\x02\u{af5}\u{af6}\x07\u{97}\x02\x02\u{af6}\
	\u{af8}\x07\u{98}\x02\x02\u{af7}\u{af5}\x03\x02\x02\x02\u{af7}\u{af8}\x03\
	\x02\x02\x02\u{af8}\u{afa}\x03\x02\x02\x02\u{af9}\u{afb}\x05\x22\x12\x02\
	\u{afa}\u{af9}\x03\x02\x02\x02\u{afa}\u{afb}\x03\x02\x02\x02\u{afb}\u{e9}\
	\x03\x02\x02\x02\u{afc}\u{b01}\x05\u{ec}\x77\x02\u{afd}\u{afe}\x07\x05\x02\
	\x02\u{afe}\u{b00}\x05\u{ec}\x77\x02\u{aff}\u{afd}\x03\x02\x02\x02\u{b00}\
	\u{b03}\x03\x02\x02\x02\u{b01}\u{aff}\x03\x02\x02\x02\u{b01}\u{b02}\x03\
	\x02\x02\x02\u{b02}\u{eb}\x03\x02\x02\x02\u{b03}\u{b01}\x03\x02\x02\x02\
	\u{b04}\u{b05}\x05\u{104}\u{83}\x02\u{b05}\u{b06}\x07\x0c\x02\x02\u{b06}\
	\u{b09}\x05\u{e0}\x71\x02\u{b07}\u{b08}\x07\u{97}\x02\x02\u{b08}\u{b0a}\
	\x07\u{98}\x02\x02\u{b09}\u{b07}\x03\x02\x02\x02\u{b09}\u{b0a}\x03\x02\x02\
	\x02\u{b0a}\u{b0c}\x03\x02\x02\x02\u{b0b}\u{b0d}\x05\x22\x12\x02\u{b0c}\
	\u{b0b}\x03\x02\x02\x02\u{b0c}\u{b0d}\x03\x02\x02\x02\u{b0d}\u{ed}\x03\x02\
	\x02\x02\u{b0e}\u{b0f}\x07\u{101}\x02\x02\u{b0f}\u{b10}\x05\u{be}\x60\x02\
	\u{b10}\u{b11}\x07\u{e6}\x02\x02\u{b11}\u{b12}\x05\u{be}\x60\x02\u{b12}\
	\u{ef}\x03\x02\x02\x02\u{b13}\u{b14}\x07\u{103}\x02\x02\u{b14}\u{b19}\x05\
	\u{f2}\x7a\x02\u{b15}\u{b16}\x07\x05\x02\x02\u{b16}\u{b18}\x05\u{f2}\x7a\
	\x02\u{b17}\u{b15}\x03\x02\x02\x02\u{b18}\u{b1b}\x03\x02\x02\x02\u{b19}\
	\u{b17}\x03\x02\x02\x02\u{b19}\u{b1a}\x03\x02\x02\x02\u{b1a}\u{f1}\x03\x02\
	\x02\x02\u{b1b}\u{b19}\x03\x02\x02\x02\u{b1c}\u{b1d}\x05\u{100}\u{81}\x02\
	\u{b1d}\u{b1e}\x07\x17\x02\x02\u{b1e}\u{b1f}\x05\u{f4}\x7b\x02\u{b1f}\u{f3}\
	\x03\x02\x02\x02\u{b20}\u{b4f}\x05\u{100}\u{81}\x02\u{b21}\u{b22}\x07\x03\
	\x02\x02\u{b22}\u{b23}\x05\u{100}\u{81}\x02\u{b23}\u{b24}\x07\x04\x02\x02\
	\u{b24}\u{b4f}\x03\x02\x02\x02\u{b25}\u{b48}\x07\x03\x02\x02\u{b26}\u{b27}\
	\x07\x27\x02\x02\u{b27}\u{b28}\x07\x1f\x02\x02\u{b28}\u{b2d}\x05\u{be}\x60\
	\x02\u{b29}\u{b2a}\x07\x05\x02\x02\u{b2a}\u{b2c}\x05\u{be}\x60\x02\u{b2b}\
	\u{b29}\x03\x02\x02\x02\u{b2c}\u{b2f}\x03\x02\x02\x02\u{b2d}\u{b2b}\x03\
	\x02\x02\x02\u{b2d}\u{b2e}\x03\x02\x02\x02\u{b2e}\u{b49}\x03\x02\x02\x02\
	\u{b2f}\u{b2d}\x03\x02\x02\x02\u{b30}\u{b31}\x09\x25\x02\x02\u{b31}\u{b32}\
	\x07\x1f\x02\x02\u{b32}\u{b37}\x05\u{be}\x60\x02\u{b33}\u{b34}\x07\x05\x02\
	\x02\u{b34}\u{b36}\x05\u{be}\x60\x02\u{b35}\u{b33}\x03\x02\x02\x02\u{b36}\
	\u{b39}\x03\x02\x02\x02\u{b37}\u{b35}\x03\x02\x02\x02\u{b37}\u{b38}\x03\
	\x02\x02\x02\u{b38}\u{b3b}\x03\x02\x02\x02\u{b39}\u{b37}\x03\x02\x02\x02\
	\u{b3a}\u{b30}\x03\x02\x02\x02\u{b3a}\u{b3b}\x03\x02\x02\x02\u{b3b}\u{b46}\
	\x03\x02\x02\x02\u{b3c}\u{b3d}\x09\x26\x02\x02\u{b3d}\u{b3e}\x07\x1f\x02\
	\x02\u{b3e}\u{b43}\x05\x5a\x2e\x02\u{b3f}\u{b40}\x07\x05\x02\x02\u{b40}\
	\u{b42}\x05\x5a\x2e\x02\u{b41}\u{b3f}\x03\x02\x02\x02\u{b42}\u{b45}\x03\
	\x02\x02\x02\u{b43}\u{b41}\x03\x02\x02\x02\u{b43}\u{b44}\x03\x02\x02\x02\
	\u{b44}\u{b47}\x03\x02\x02\x02\u{b45}\u{b43}\x03\x02\x02\x02\u{b46}\u{b3c}\
	\x03\x02\x02\x02\u{b46}\u{b47}\x03\x02\x02\x02\u{b47}\u{b49}\x03\x02\x02\
	\x02\u{b48}\u{b26}\x03\x02\x02\x02\u{b48}\u{b3a}\x03\x02\x02\x02\u{b49}\
	\u{b4b}\x03\x02\x02\x02\u{b4a}\u{b4c}\x05\u{f6}\x7c\x02\u{b4b}\u{b4a}\x03\
	\x02\x02\x02\u{b4b}\u{b4c}\x03\x02\x02\x02\u{b4c}\u{b4d}\x03\x02\x02\x02\
	\u{b4d}\u{b4f}\x07\x04\x02\x02\u{b4e}\u{b20}\x03\x02\x02\x02\u{b4e}\u{b21}\
	\x03\x02\x02\x02\u{b4e}\u{b25}\x03\x02\x02\x02\u{b4f}\u{f5}\x03\x02\x02\
	\x02\u{b50}\u{b51}\x07\u{b5}\x02\x02\u{b51}\u{b61}\x05\u{f8}\x7d\x02\u{b52}\
	\u{b53}\x07\u{c9}\x02\x02\u{b53}\u{b61}\x05\u{f8}\x7d\x02\u{b54}\u{b55}\
	\x07\u{b5}\x02\x02\u{b55}\u{b56}\x07\x1b\x02\x02\u{b56}\u{b57}\x05\u{f8}\
	\x7d\x02\u{b57}\u{b58}\x07\x12\x02\x02\u{b58}\u{b59}\x05\u{f8}\x7d\x02\u{b59}\
	\u{b61}\x03\x02\x02\x02\u{b5a}\u{b5b}\x07\u{c9}\x02\x02\u{b5b}\u{b5c}\x07\
	\x1b\x02\x02\u{b5c}\u{b5d}\x05\u{f8}\x7d\x02\u{b5d}\u{b5e}\x07\x12\x02\x02\
	\u{b5e}\u{b5f}\x05\u{f8}\x7d\x02\u{b5f}\u{b61}\x03\x02\x02\x02\u{b60}\u{b50}\
	\x03\x02\x02\x02\u{b60}\u{b52}\x03\x02\x02\x02\u{b60}\u{b54}\x03\x02\x02\
	\x02\u{b60}\u{b5a}\x03\x02\x02\x02\u{b61}\u{f7}\x03\x02\x02\x02\u{b62}\u{b63}\
	\x07\u{f3}\x02\x02\u{b63}\u{b6a}\x09\x27\x02\x02\u{b64}\u{b65}\x07\x39\x02\
	\x02\u{b65}\u{b6a}\x07\u{c8}\x02\x02\u{b66}\u{b67}\x05\u{be}\x60\x02\u{b67}\
	\u{b68}\x09\x27\x02\x02\u{b68}\u{b6a}\x03\x02\x02\x02\u{b69}\u{b62}\x03\
	\x02\x02\x02\u{b69}\u{b64}\x03\x02\x02\x02\u{b69}\u{b66}\x03\x02\x02\x02\
	\u{b6a}\u{f9}\x03\x02\x02\x02\u{b6b}\u{b70}\x05\u{fe}\u{80}\x02\u{b6c}\u{b6d}\
	\x07\x05\x02\x02\u{b6d}\u{b6f}\x05\u{fe}\u{80}\x02\u{b6e}\u{b6c}\x03\x02\
	\x02\x02\u{b6f}\u{b72}\x03\x02\x02\x02\u{b70}\u{b6e}\x03\x02\x02\x02\u{b70}\
	\u{b71}\x03\x02\x02\x02\u{b71}\u{fb}\x03\x02\x02\x02\u{b72}\u{b70}\x03\x02\
	\x02\x02\u{b73}\u{b78}\x05\u{fe}\u{80}\x02\u{b74}\u{b78}\x07\x5d\x02\x02\
	\u{b75}\u{b78}\x07\u{83}\x02\x02\u{b76}\u{b78}\x07\u{c2}\x02\x02\u{b77}\
	\u{b73}\x03\x02\x02\x02\u{b77}\u{b74}\x03\x02\x02\x02\u{b77}\u{b75}\x03\
	\x02\x02\x02\u{b77}\u{b76}\x03\x02\x02\x02\u{b78}\u{fd}\x03\x02\x02\x02\
	\u{b79}\u{b7e}\x05\u{104}\u{83}\x02\u{b7a}\u{b7b}\x07\x06\x02\x02\u{b7b}\
	\u{b7d}\x05\u{104}\u{83}\x02\u{b7c}\u{b7a}\x03\x02\x02\x02\u{b7d}\u{b80}\
	\x03\x02\x02\x02\u{b7e}\u{b7c}\x03\x02\x02\x02\u{b7e}\u{b7f}\x03\x02\x02\
	\x02\u{b7f}\u{ff}\x03\x02\x02\x02\u{b80}\u{b7e}\x03\x02\x02\x02\u{b81}\u{b82}\
	\x05\u{104}\u{83}\x02\u{b82}\u{b83}\x05\u{102}\u{82}\x02\u{b83}\u{101}\x03\
	\x02\x02\x02\u{b84}\u{b85}\x07\u{10f}\x02\x02\u{b85}\u{b87}\x05\u{104}\u{83}\
	\x02\u{b86}\u{b84}\x03\x02\x02\x02\u{b87}\u{b88}\x03\x02\x02\x02\u{b88}\
	\u{b86}\x03\x02\x02\x02\u{b88}\u{b89}\x03\x02\x02\x02\u{b89}\u{b8c}\x03\
	\x02\x02\x02\u{b8a}\u{b8c}\x03\x02\x02\x02\u{b8b}\u{b86}\x03\x02\x02\x02\
	\u{b8b}\u{b8a}\x03\x02\x02\x02\u{b8c}\u{103}\x03\x02\x02\x02\u{b8d}\u{b91}\
	\x05\u{106}\u{84}\x02\u{b8e}\u{b8f}\x06\u{83}\x12\x02\u{b8f}\u{b91}\x05\
	\u{110}\u{89}\x02\u{b90}\u{b8d}\x03\x02\x02\x02\u{b90}\u{b8e}\x03\x02\x02\
	\x02\u{b91}\u{105}\x03\x02\x02\x02\u{b92}\u{b99}\x07\u{123}\x02\x02\u{b93}\
	\u{b99}\x05\u{108}\u{85}\x02\u{b94}\u{b95}\x06\u{84}\x13\x02\u{b95}\u{b99}\
	\x05\u{10e}\u{88}\x02\u{b96}\u{b97}\x06\u{84}\x14\x02\u{b97}\u{b99}\x05\
	\u{112}\u{8a}\x02\u{b98}\u{b92}\x03\x02\x02\x02\u{b98}\u{b93}\x03\x02\x02\
	\x02\u{b98}\u{b94}\x03\x02\x02\x02\u{b98}\u{b96}\x03\x02\x02\x02\u{b99}\
	\u{107}\x03\x02\x02\x02\u{b9a}\u{b9b}\x07\u{124}\x02\x02\u{b9b}\u{109}\x03\
	\x02\x02\x02\u{b9c}\u{b9e}\x06\u{86}\x15\x02\u{b9d}\u{b9f}\x07\u{10f}\x02\
	\x02\u{b9e}\u{b9d}\x03\x02\x02\x02\u{b9e}\u{b9f}\x03\x02\x02\x02\u{b9f}\
	\u{ba0}\x03\x02\x02\x02\u{ba0}\u{bc8}\x07\u{11e}\x02\x02\u{ba1}\u{ba3}\x06\
	\u{86}\x16\x02\u{ba2}\u{ba4}\x07\u{10f}\x02\x02\u{ba3}\u{ba2}\x03\x02\x02\
	\x02\u{ba3}\u{ba4}\x03\x02\x02\x02\u{ba4}\u{ba5}\x03\x02\x02\x02\u{ba5}\
	\u{bc8}\x07\u{11f}\x02\x02\u{ba6}\u{ba8}\x06\u{86}\x17\x02\u{ba7}\u{ba9}\
	\x07\u{10f}\x02\x02\u{ba8}\u{ba7}\x03\x02\x02\x02\u{ba8}\u{ba9}\x03\x02\
	\x02\x02\u{ba9}\u{baa}\x03\x02\x02\x02\u{baa}\u{bc8}\x09\x28\x02\x02\u{bab}\
	\u{bad}\x07\u{10f}\x02\x02\u{bac}\u{bab}\x03\x02\x02\x02\u{bac}\u{bad}\x03\
	\x02\x02\x02\u{bad}\u{bae}\x03\x02\x02\x02\u{bae}\u{bc8}\x07\u{11d}\x02\
	\x02\u{baf}\u{bb1}\x07\u{10f}\x02\x02\u{bb0}\u{baf}\x03\x02\x02\x02\u{bb0}\
	\u{bb1}\x03\x02\x02\x02\u{bb1}\u{bb2}\x03\x02\x02\x02\u{bb2}\u{bc8}\x07\
	\u{11a}\x02\x02\u{bb3}\u{bb5}\x07\u{10f}\x02\x02\u{bb4}\u{bb3}\x03\x02\x02\
	\x02\u{bb4}\u{bb5}\x03\x02\x02\x02\u{bb5}\u{bb6}\x03\x02\x02\x02\u{bb6}\
	\u{bc8}\x07\u{11b}\x02\x02\u{bb7}\u{bb9}\x07\u{10f}\x02\x02\u{bb8}\u{bb7}\
	\x03\x02\x02\x02\u{bb8}\u{bb9}\x03\x02\x02\x02\u{bb9}\u{bba}\x03\x02\x02\
	\x02\u{bba}\u{bc8}\x07\u{11c}\x02\x02\u{bbb}\u{bbd}\x07\u{10f}\x02\x02\u{bbc}\
	\u{bbb}\x03\x02\x02\x02\u{bbc}\u{bbd}\x03\x02\x02\x02\u{bbd}\u{bbe}\x03\
	\x02\x02\x02\u{bbe}\u{bc8}\x07\u{121}\x02\x02\u{bbf}\u{bc1}\x07\u{10f}\x02\
	\x02\u{bc0}\u{bbf}\x03\x02\x02\x02\u{bc0}\u{bc1}\x03\x02\x02\x02\u{bc1}\
	\u{bc2}\x03\x02\x02\x02\u{bc2}\u{bc8}\x07\u{120}\x02\x02\u{bc3}\u{bc5}\x07\
	\u{10f}\x02\x02\u{bc4}\u{bc3}\x03\x02\x02\x02\u{bc4}\u{bc5}\x03\x02\x02\
	\x02\u{bc5}\u{bc6}\x03\x02\x02\x02\u{bc6}\u{bc8}\x07\u{122}\x02\x02\u{bc7}\
	\u{b9c}\x03\x02\x02\x02\u{bc7}\u{ba1}\x03\x02\x02\x02\u{bc7}\u{ba6}\x03\
	\x02\x02\x02\u{bc7}\u{bac}\x03\x02\x02\x02\u{bc7}\u{bb0}\x03\x02\x02\x02\
	\u{bc7}\u{bb4}\x03\x02\x02\x02\u{bc7}\u{bb8}\x03\x02\x02\x02\u{bc7}\u{bbc}\
	\x03\x02\x02\x02\u{bc7}\u{bc0}\x03\x02\x02\x02\u{bc7}\u{bc4}\x03\x02\x02\
	\x02\u{bc8}\u{10b}\x03\x02\x02\x02\u{bc9}\u{bca}\x07\u{f1}\x02\x02\u{bca}\
	\u{bd1}\x05\u{e0}\x71\x02\u{bcb}\u{bd1}\x05\x22\x12\x02\u{bcc}\u{bd1}\x05\
	\u{de}\x70\x02\u{bcd}\u{bce}\x09\x29\x02\x02\u{bce}\u{bcf}\x07\u{97}\x02\
	\x02\u{bcf}\u{bd1}\x07\u{98}\x02\x02\u{bd0}\u{bc9}\x03\x02\x02\x02\u{bd0}\
	\u{bcb}\x03\x02\x02\x02\u{bd0}\u{bcc}\x03\x02\x02\x02\u{bd0}\u{bcd}\x03\
	\x02\x02\x02\u{bd1}\u{10d}\x03\x02\x02\x02\u{bd2}\u{bd3}\x09\x2a\x02\x02\
	\u{bd3}\u{10f}\x03\x02\x02\x02\u{bd4}\u{bd5}\x09\x2b\x02\x02\u{bd5}\u{111}\
	\x03\x02\x02\x02\u{bd6}\u{bd7}\x09\x2c\x02\x02\u{bd7}\u{113}\x03\x02\x02\
	\x02\u{18d}\u{119}\u{11c}\u{11e}\u{134}\u{139}\u{141}\u{149}\u{14b}\u{15f}\
	\u{163}\u{169}\u{16c}\u{16f}\u{176}\u{17b}\u{17e}\u{185}\u{191}\u{19a}\u{19c}\
	\u{1a0}\u{1a3}\u{1aa}\u{1b5}\u{1b7}\u{1bf}\u{1c4}\u{1c7}\u{1cd}\u{1d8}\u{218}\
	\u{221}\u{225}\u{22b}\u{22f}\u{234}\u{23a}\u{246}\u{24e}\u{254}\u{261}\u{266}\
	\u{276}\u{27d}\u{281}\u{287}\u{296}\u{29a}\u{2a0}\u{2a6}\u{2a9}\u{2ac}\u{2b2}\
	\u{2b6}\u{2be}\u{2c0}\u{2c9}\u{2cc}\u{2d5}\u{2da}\u{2e0}\u{2e7}\u{2ea}\u{2f0}\
	\u{2fb}\u{2fe}\u{302}\u{307}\u{30c}\u{313}\u{316}\u{319}\u{320}\u{325}\u{32e}\
	\u{336}\u{33c}\u{33f}\u{342}\u{348}\u{34c}\u{350}\u{354}\u{356}\u{35e}\u{366}\
	\u{36c}\u{372}\u{375}\u{379}\u{37c}\u{380}\u{39c}\u{39f}\u{3a3}\u{3a9}\u{3ac}\
	\u{3af}\u{3b5}\u{3bd}\u{3c2}\u{3c8}\u{3ce}\u{3da}\u{3dd}\u{3e4}\u{3f5}\u{3fe}\
	\u{401}\u{407}\u{410}\u{417}\u{41a}\u{424}\u{428}\u{42f}\u{4a3}\u{4ab}\u{4b3}\
	\u{4bc}\u{4c6}\u{4ca}\u{4cd}\u{4d3}\u{4d9}\u{4e5}\u{4f1}\u{4f6}\u{4ff}\u{507}\
	\u{50e}\u{510}\u{515}\u{519}\u{51e}\u{523}\u{528}\u{52b}\u{530}\u{534}\u{539}\
	\u{53b}\u{53f}\u{548}\u{550}\u{559}\u{560}\u{569}\u{56e}\u{571}\u{584}\u{586}\
	\u{58f}\u{596}\u{599}\u{5a0}\u{5a4}\u{5aa}\u{5b2}\u{5bd}\u{5c8}\u{5cf}\u{5d5}\
	\u{5e2}\u{5e9}\u{5f0}\u{5fc}\u{604}\u{60a}\u{60d}\u{616}\u{619}\u{622}\u{625}\
	\u{62e}\u{631}\u{63a}\u{63d}\u{640}\u{645}\u{647}\u{653}\u{65a}\u{661}\u{664}\
	\u{666}\u{672}\u{676}\u{67a}\u{680}\u{684}\u{68c}\u{690}\u{693}\u{696}\u{699}\
	\u{69d}\u{6a1}\u{6a4}\u{6a8}\u{6ad}\u{6b1}\u{6b4}\u{6b7}\u{6ba}\u{6bc}\u{6c8}\
	\u{6cb}\u{6cf}\u{6d9}\u{6dd}\u{6df}\u{6e2}\u{6e6}\u{6ec}\u{6f0}\u{6fb}\u{705}\
	\u{711}\u{720}\u{725}\u{72c}\u{73c}\u{741}\u{74e}\u{753}\u{75b}\u{761}\u{765}\
	\u{76e}\u{77d}\u{782}\u{78e}\u{793}\u{79b}\u{79e}\u{7a2}\u{7b0}\u{7bd}\u{7c2}\
	\u{7c6}\u{7c9}\u{7ce}\u{7d7}\u{7da}\u{7df}\u{7e6}\u{7e9}\u{7f1}\u{7f8}\u{7ff}\
	\u{802}\u{807}\u{80a}\u{80f}\u{813}\u{816}\u{819}\u{81f}\u{824}\u{829}\u{83b}\
	\u{83d}\u{840}\u{84b}\u{854}\u{85b}\u{863}\u{86a}\u{86e}\u{876}\u{87e}\u{884}\
	\u{88c}\u{898}\u{89b}\u{8a1}\u{8a5}\u{8a7}\u{8b0}\u{8bc}\u{8be}\u{8c5}\u{8cc}\
	\u{8d2}\u{8d8}\u{8da}\u{8e1}\u{8e9}\u{8ef}\u{8f5}\u{8f9}\u{8fb}\u{902}\u{90b}\
	\u{918}\u{91d}\u{921}\u{92f}\u{931}\u{939}\u{93b}\u{93f}\u{947}\u{950}\u{956}\
	\u{95e}\u{963}\u{96f}\u{974}\u{977}\u{97d}\u{981}\u{986}\u{98b}\u{990}\u{996}\
	\u{9ab}\u{9ad}\u{9b6}\u{9ba}\u{9c3}\u{9c7}\u{9d9}\u{9dc}\u{9e4}\u{9ed}\u{a04}\
	\u{a0f}\u{a16}\u{a19}\u{a22}\u{a26}\u{a32}\u{a4b}\u{a52}\u{a55}\u{a64}\u{a68}\
	\u{a72}\u{a74}\u{a81}\u{a83}\u{a90}\u{a94}\u{a9b}\u{aa0}\u{aa8}\u{aac}\u{ab1}\
	\u{ac2}\u{ac6}\u{acf}\u{ad3}\u{ad5}\u{adc}\u{ae3}\u{ae6}\u{ae9}\u{af0}\u{af7}\
	\u{afa}\u{b01}\u{b09}\u{b0c}\u{b19}\u{b2d}\u{b37}\u{b3a}\u{b43}\u{b46}\u{b48}\
	\u{b4b}\u{b4e}\u{b60}\u{b69}\u{b70}\u{b77}\u{b7e}\u{b88}\u{b8b}\u{b90}\u{b98}\
	\u{b9e}\u{ba3}\u{ba8}\u{bac}\u{bb0}\u{bb4}\u{bb8}\u{bbc}\u{bc0}\u{bc4}\u{bc7}\
	\u{bd0}";

